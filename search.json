[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R Packages (2e)",
    "section": "",
    "text": "Welcome!\nWelcome to R packages by Hadley Wickham and Jennifer Bryan. Packages are the fundamental units of reproducible R code. They include reusable R functions, the documentation that describes how to use them, and sample data. In this book you’ll learn how to turn your code into packages that others can easily download and use. Writing a package can seem overwhelming at first. So start with the basics and improve it over time. It doesn’t matter if your first version isn’t perfect as long as the next version is better.\nThis is the work-in-progress 2nd edition of the book."
  },
  {
    "objectID": "preface.html#acknowledgments",
    "href": "preface.html#acknowledgments",
    "title": "Preface",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nSince the first edition of R Packages was published, the packages supporting the workflows described here have undergone extensive development. The original trio of devtools, roxygen2, and testthat has expanded to include the packages created by the “conscious uncoupling” of devtools, as described in Section 3.2. Most of these packages originate with Hadley Wickham (HW), because of their devtools roots. There are many other significant contributors, many of whom now serve as maintainers:\n\ndevtools: HW, Winston Chang, Jim Hester (maintainer, >= v1.13.5), Jennifer Bryan (maintainer >= v2.4.3)\nusethis: HW, Jennifer Bryan (maintainer >= v1.5.0), Malcolm Barrett\nroxygen2: HW (maintainer), Peter Danenburg, Manuel Eugster\n\ntestthat: HW (maintainer)\ndesc: Gábor Csárdi (maintainer), Kirill Müller, Jim Hester\n\npkgbuild: HW, Jim Hester, Gábor Csárdi (maintainer >= v1.2.1)\npkgload: HW, Jim Hester, Winston Chang, Lionel Henry (maintainer >= v1.2.4)\nrcmdcheck: Gábor Csárdi (maintainer)\nremotes: HW, Jim Hester, Gábor Csárdi (maintainer), Winston Chang, Martin Morgan, Dan Tenenbaum\n\nrevdepcheck: HW, Gábor Csárdi (maintainer)\nsessioninfo: HW, Gábor Csárdi (maintainer), Winston Chang, Robert Flight, Kirill Müller, Jim Hester\n\n\nThis book was written and revised in the open and it is truly a community effort: many people read drafts, fix typos, suggest improvements, and contribute content. Without those contributors, the book wouldn’t be nearly as good as it is, and we are deeply grateful for their help. We are indebted to our colleagues at Posit, especially the tidyverse team, for being perpetually game to discuss package development practices. The book has been greatly improved by the suggestions from our fantastic team of technical reviewers: Malcolm Barrett, Laura DeCicco, Zhian Kamvar, Tom Mock and Maëlle Salmon.\nThanks go to all contributors who submitted improvements via github (in alphabetical order): @aaronwolen, @ablejec, Adam Yormark, @adessy, Adrien Todeschini, Alan Haynes, Alexander Grüneberg, Alison Presmanes Hill, Andrea Cantieni, Andrew Bray, Andrew Craig, Andy Teucher, Andy Visser, @apomatix, Arni Magnusson, Ben Bond-Lamberty, Ben Marwick, Berry Boessenkool, @bm5tev3, Brandon Greenwell, Brett Johnson, Brett K, Brett Klamer, Brian Rice, Brooke Anderson, @btruel, @CAPN, Carl A. B. Pearson, Chao Cheng, Chester Ismay, Choyoung Yim, @chsafouane, @contravariant, Craig Citro, Crt Ahlin, Dan Yavorsky, @danhalligan, Daniel Falbel, Daniel Lee, David Robinson, David Smith, @davidkane9, Dean Attali, @deanbodenhambsse, Douglas K. G. Araujo, @dracodoc, @dryzliang, Earl Brown, Eduardo Ariño de la Rubia, @eipi10, Enrico Spinielli, @eogoodwin, Erik Erhardt, Ewan Dunbar, Federico Marini, Florian Kohrt, Floris Vanderhaeghe, Gerhard Nachtmann, Gerrit-Jan Schutten, Greg Macfarlane, Gustav W Delius, Hadley Wickham, Hannah Frick, @harrismcgehee, Hedderik van Rijn, @helix123, @henningte, Henrik Bengtsson, @heogden, Howard Baek, Hugo Gruson, Ian Gow, @iargent, Iaroslav Domin, Ibrahim Kekec, Ionut Stefan-Birdea, @jacobbien, James Keirstead, James Laird-Smith, Jee Roen, Jennifer (Jenny) Bryan, Jenny Bryan, @Jeremiah, Jim Hester, @jmarshallnz, Jo-Anne Tan, Joanna Zhao, Joe Cainey, Joe Thorley, Johan Larsson, John Baumgartner, John Blischak, @jomuller, Jon Harmon, @Jordan, @jowalski, Justin Alford, Karl Broman, Karthik Ram, @Kasper, @KatherineCox, Katrin Leinweber, Kevin Ushey, Kevin Wright, Kirill Müller, Kristopher Kapphahn, Kun Ren, @kwenzig, @kylelundstedt, @lancelote, Lech Madeyski, @Leon, @lindbrook, Lluis Ramon, Maëlle Salmon, @maiermarco, Malcolm Barrett, Manuel Reif, Mara Averick, Mark Dulhunty, @martin-mfg, Matan Hakim, Matthew Roberts, Mauro Lepore, Michael Boerman, Michael Buckley, @michaelweylandt, Michel Lang, @miguelmorin, @MikeJohnPage, @MikeLeonard, @nareal, Nathan Levett, Nathaniel Phillips, @nattalides, @Nic, Nicholas Tierney, Nick Carchedi, Nick Zeng, @NS, Oliver Keyes, Øystein Sørensen, Pablo Rodríguez-Sánchez, Patrick Kimes, Paul Blischak, Peter Meissner, @PeterDee, Philip Crain, Philip Pallmann, Po Su, @PrzeChoj, R. Mark Sharp, @ramiromagno, Richard M. Smith, Rick Tankard, @rmar073, @rmsharp, Robert Krzyzanowski, Robin Gower, @robiRagan, Ryan Peterson, @ryanatanner, Salim B, Sam Firke, Sascha Holzhauer, @scharne, Scott Rohde, Sean Wilkinson, Sébastien Rochette, Sergey Grechin, @setoyama60jp, Shannon Pileggi, Shantanu Singh, Shaun Walbridge, Shinya Uryu, @SimonPBiggs, Stefan Eng, Stefan Herzog, Stefan Jansson, Stefan Widgren, Stephen Frank, Stephen Rushe, @stevensbr, Tanner Stauss, Telmo Brugnara, Tony Breyal, Tony Fischetti, @TroyVan, @urmils, Vince Knight, Vlad Petyuk, Wenjie Wang, Will Beasley, Winston Chang, @winterschlaefer, Wouter Saelens, @wrathematics, Xiaosong Zhang, Y. Yu, Yihui Xie, @ysdgroot, @yui-knk, Zeki Akyol, @zhaoy, Zhian N. Kamvar, Zhuoer Dong."
  },
  {
    "objectID": "preface.html#conventions",
    "href": "preface.html#conventions",
    "title": "Preface",
    "section": "Conventions",
    "text": "Conventions\nThroughout this book, we write fun() to refer to functions, var to refer to variables and function arguments, and path/ for paths.\nLarger code blocks intermingle input and output. Output is commented so that if you have an electronic version of the book, e.g., https://r-pkgs.org, you can easily copy and paste examples into R. Output comments look like #> to distinguish them from regular comments."
  },
  {
    "objectID": "preface.html#colophon",
    "href": "preface.html#colophon",
    "title": "Preface",
    "section": "Colophon",
    "text": "Colophon\nThis book was authored using Quarto inside RStudio. The website is hosted with Netlify, and automatically updated after every commit by GitHub actions. The complete source is available from GitHub.\nThis version of the book was built with:\n\nlibrary(devtools)\n#> Loading required package: usethis\nlibrary(roxygen2)\nlibrary(testthat)\n#> \n#> Attaching package: 'testthat'\n#> The following object is masked from 'package:devtools':\n#> \n#>     test_file\ndevtools::session_info()\n#> ─ Session info ───────────────────────────────────────────────────\n#>  setting  value\n#>  version  R version 4.2.3 (2023-03-15)\n#>  os       Ubuntu 22.04.2 LTS\n#>  system   x86_64, linux-gnu\n#>  ui       X11\n#>  language (EN)\n#>  collate  C.UTF-8\n#>  ctype    C.UTF-8\n#>  tz       UTC\n#>  date     2023-04-15\n#>  pandoc   2.9.2.1 @ /usr/bin/ (via rmarkdown)\n#> \n#> ─ Packages ───────────────────────────────────────────────────────\n#>  package     * version    date (UTC) lib source\n#>  brio          1.1.3      2021-11-30 [1] RSPM\n#>  cachem        1.0.7      2023-02-24 [1] RSPM\n#>  callr         3.7.3      2022-11-02 [1] RSPM\n#>  cli           3.6.1      2023-03-23 [1] RSPM\n#>  crayon        1.5.2      2022-09-29 [1] RSPM\n#>  devtools    * 2.4.5      2022-10-11 [1] RSPM\n#>  digest        0.6.31     2022-12-11 [1] RSPM\n#>  ellipsis      0.3.2      2021-04-29 [1] RSPM\n#>  evaluate      0.20       2023-01-17 [1] RSPM\n#>  fastmap       1.1.1      2023-02-24 [1] RSPM\n#>  fs            1.6.1      2023-02-06 [1] RSPM\n#>  glue          1.6.2      2022-02-24 [1] RSPM\n#>  htmltools     0.5.5      2023-03-23 [1] RSPM\n#>  htmlwidgets   1.6.2      2023-03-17 [1] RSPM\n#>  httpuv        1.6.9      2023-02-14 [1] RSPM\n#>  jsonlite      1.8.4      2022-12-06 [1] RSPM\n#>  knitr         1.42       2023-01-25 [1] RSPM\n#>  later         1.3.0      2021-08-18 [1] RSPM\n#>  lifecycle     1.0.3      2022-10-07 [1] RSPM\n#>  magrittr      2.0.3      2022-03-30 [1] RSPM\n#>  memoise       2.0.1      2021-11-26 [1] RSPM\n#>  mime          0.12       2021-09-28 [1] RSPM\n#>  miniUI        0.1.1.1    2018-05-18 [1] RSPM\n#>  pkgbuild      1.4.0      2022-11-27 [1] RSPM\n#>  pkgload       1.3.2      2022-11-16 [1] RSPM\n#>  prettyunits   1.1.1      2020-01-24 [1] RSPM\n#>  processx      3.8.0      2022-10-26 [1] RSPM\n#>  profvis       0.3.7      2020-11-02 [1] RSPM\n#>  promises      1.2.0.1    2021-02-11 [1] RSPM\n#>  ps            1.7.4      2023-04-02 [1] RSPM\n#>  purrr         1.0.1      2023-01-10 [1] RSPM\n#>  R6            2.5.1      2021-08-19 [1] RSPM\n#>  Rcpp          1.0.10     2023-01-22 [1] RSPM\n#>  remotes       2.4.2      2021-11-30 [1] RSPM\n#>  rlang         1.1.0      2023-03-14 [1] RSPM\n#>  rmarkdown     2.21       2023-03-26 [1] RSPM\n#>  roxygen2    * 7.2.3      2022-12-08 [1] RSPM\n#>  sessioninfo   1.2.2      2021-12-06 [1] RSPM\n#>  shiny         1.7.4      2022-12-15 [1] RSPM\n#>  stringi       1.7.12     2023-01-11 [1] RSPM\n#>  stringr       1.5.0.9000 2023-03-16 [1] Github (tidyverse/stringr@65b218e)\n#>  testthat    * 3.1.7      2023-03-12 [1] RSPM\n#>  urlchecker    1.0.1      2021-11-30 [1] RSPM\n#>  usethis     * 2.1.6      2022-05-25 [1] RSPM\n#>  vctrs         0.6.1      2023-03-22 [1] RSPM\n#>  xfun          0.38       2023-03-24 [1] RSPM\n#>  xml2          1.3.3      2021-11-30 [1] RSPM\n#>  xtable        1.8-4      2019-04-21 [1] RSPM\n#> \n#>  [1] /home/runner/work/_temp/Library\n#>  [2] /opt/R/4.2.3/lib/R/site-library\n#>  [3] /opt/R/4.2.3/lib/R/library\n#> \n#> ──────────────────────────────────────────────────────────────────\n\n\n\n\n\nMüller, Kirill, and Lorenz Walthert. 2018. Styler: Non-Invasive Pretty Printing of R Code. http://styler.r-lib.org."
  },
  {
    "objectID": "introduction.html#sec-intro-phil",
    "href": "introduction.html#sec-intro-phil",
    "title": "1  Introduction",
    "section": "\n1.1 Philosophy",
    "text": "1.1 Philosophy\nThis book espouses our philosophy of package development: anything that can be automated, should be automated. Do as little as possible by hand. Do as much as possible with functions. The goal is to spend your time thinking about what you want your package to do rather than thinking about the minutiae of package structure.\nThis philosophy is realized primarily through the devtools package, which is the public face for a suite of R functions that automate common development tasks. The release of version 2.0.0 in October 2018 marked its internal restructuring into a set of more focused packages, with devtools becoming more of a meta-package. The usethis package is the sub-package you are most likely to interact with directly; we explain the devtools-usethis relationship in Section 3.2.\nAs always, the goal of devtools is to make package development as painless as possible. It encapsulates the best practices developed by Hadley Wickham, initially from his years as a prolific solo developer. More recently, he has assembled a team of developers at Posit (formerly known as RStudio), who collectively look after hundreds of open source R packages, including those known as the tidyverse. The reach of this team allows us to explore the space of all possible mistakes at an extraordinary scale. Fortunately, it also affords us the opportunity to reflect on both the successes and failures, in the company of expert and sympathetic colleagues. We try to develop practices that make life more enjoyable for both the maintainer and users of a package. The devtools meta-package is where these lessons are made concrete.\ndevtools works hand-in-hand with RStudio, which we believe is the best development environment for most R users. The most popular alternative to RStudio is currently Visual Studio Code (VS Code) with the R extension enabled. This can be a rewarding and powerful environment, however it does require a bit more work to set up and customize1.\n\n\n\n\n\n\nRStudio\n\n\n\nThroughout the book, we highlight specific ways that RStudio can expedite your package development workflow, in specially formatted sections like this.\n\n\nTogether, devtools and RStudio insulate you from the low-level details of how packages are built. As you start to develop more packages, we highly recommend that you learn more about those details. The best resource for the official details of package development is always the official writing R extensions manual2. However, this manual can be hard to understand if you’re not already familiar with the basics of packages. It’s also exhaustive, covering every possible package component, rather than focusing on the most common and useful components, as this book does. Writing R extensions is a useful resource once you’ve mastered the basics and want to learn what’s going on under the hood."
  },
  {
    "objectID": "introduction.html#intro-outline",
    "href": "introduction.html#intro-outline",
    "title": "1  Introduction",
    "section": "\n1.2 In this book",
    "text": "1.2 In this book\nThe first part of the book is all about giving you all the tools you need to start your package development journey and we highly recommend that you read it in order. We begin in Chapter 2 with a run through the complete development of a small package. It’s meant to paint the big picture and suggest a workflow, before we descend into the detailed treatment of the key components of an R package. Then in Chapter 3 you’ll learn how to prepare your system for package development, and in Chapter 4 you’ll learn the basic structure of a package and how that varies across different states. Next, in Chapter 5, we’ll cover the core workflows that come up repeatedly for package developers. The first part of the book ends with another case study (Chapter 6), this time focusing on how you might convert a script to a package and discussing the challenges you’ll face along the way.\nThe remainder of the book is designed to be read as needed. Pick and choose between the chapters as the various topics come up in your development process.\nFirst we cover key package components: Chapter 7 discusses where your code lives and how to organize it, Chapter 8 shows you how to include data in your package, and Chapter 9 covers a few less important files and directories that need to be discussed somewhere.\nNext we’ll dive into to the package metadata, starting with DESCRIPTION in Chapter 10. We’ll then go deep into dependencies. In Chapter 11, we’ll cover the costs and benefits of taking on dependencies and provide some technical background on package namespaces and the search path. In Chapter 12, we focus on practical matters, such as how to use different types of dependencies in different parts of your package. This is also where we discuss exporting functions, which is what makes it possible for other packages and projects to depend on your package. We’ll finish off this part with a look at licensing in Chapter 13.\nTo ensure your package works as designed (and continues to work as you make changes), it’s essential to test your code, so the next three chapters cover the art and science of testing. Chapter 14 gets you started with the basics of testing with the testthat package. Chapter 15 teaches you how to design and organise tests in the most effective way. Then we finish off our coverage of testing in Chapter 16 which teaches you advanced skills to tackle challenging situations.\nIf you want other people (including future-you!) to understand how to use the functions in your package, you’ll need to document them. Chapter 17 gets you started using roxygen2 to document the functions in your package. Function documentation is only helpful if you know what function to look up, so next in Chapter 18 we’ll discuss vignettes, which help you document the package as a whole. We’ll finish up documentation with a discussion of other important markdown files like README.md and NEWS.md in Chapter 19, and creating a package website with pkgdown in Chapter 20.\nThe book concludes by zooming back out to consider development practices, such as the benefit of using version control and continuous integration (Chapter 21). We wrap things up by discussing the lifecycle (Chapter 22) of a package, including releasing it on CRAN (Chapter 23).\nThis is a lot to learn, but don’t feel overwhelmed. Start with a minimal subset of useful features (e.g. just an R/ directory!) and build up over time. To paraphrase the Zen monk Shunryu Suzuki: “Each package is perfect the way it is — and it can use a little improvement”."
  },
  {
    "objectID": "introduction.html#whats-not-here",
    "href": "introduction.html#whats-not-here",
    "title": "1  Introduction",
    "section": "\n1.3 What’s not here",
    "text": "1.3 What’s not here\nThere are also specific practices that have little to no treatment here simply because we do not use them enough to have any special insight. Does this mean that we actively discourage those practices? Probably not, as we try to be explicit about practices we think you should avoid. So if something is not covered here, it just means that a couple hundred heavily-used R packages are built without meaningful reliance on that technique. That observation should motivate you to evaluate how likely it is that your development requirements truly don’t overlap with ours. But sometimes the answer is a clear “yes”, in which case you’ll simply need to consult another resource.\n\n\n\n\nMarwick, Ben, Carl Boettiger, and Lincoln Mullen. 2018a. “Packaging Data Analytical Work Reproducibly Using r (and Friends).” The American Statistician 72 (1): 80–88. https://doi.org/10.1080/00031305.2017.1375986.\n\n\n———. 2018b. “Packaging Data Analytical Work Reproducibly Using r (and Friends).” PeerJ Preprints 6 (March): e3192v2. https://doi.org/10.7287/peerj.preprints.3192v2."
  },
  {
    "objectID": "whole-game.html#load-devtools-and-friends",
    "href": "whole-game.html#load-devtools-and-friends",
    "title": "2  The Whole Game",
    "section": "\n2.1 Load devtools and friends",
    "text": "2.1 Load devtools and friends\nYou can initiate your new package from any active R session. You don’t need to worry about whether you’re in an existing or new project or not. The functions we use ensure that we create a new clean project for the package.\nLoad the devtools package, which is the public face of a set of packages that support various aspects of package development. The most obvious of these is the usethis package, which you’ll see is also being loaded.\n\nlibrary(devtools)\n#> Loading required package: usethis\n\nDo you have an old version of devtools? Compare your version against ours and upgrade if necessary.\n\npackageVersion(\"devtools\")\n#> [1] '2.4.5'"
  },
  {
    "objectID": "whole-game.html#toy-package-regexcite",
    "href": "whole-game.html#toy-package-regexcite",
    "title": "2  The Whole Game",
    "section": "\n2.2 Toy package: regexcite",
    "text": "2.2 Toy package: regexcite\nTo help walk you through the process, we use various functions from devtools to build a small toy package from scratch, with features commonly seen in released packages:\n\nFunctions to address a specific need, in this case helpers for work with regular expressions.\nVersion control and an open development process.\n\nThis is completely optional in your work, but highly recommended. You’ll see how Git and GitHub help us expose all the intermediate stages of our toy package.\n\n\nAccess to established workflows for installation, getting help, and checking quality.\n\nDocumentation for individual functions via roxygen2.\nUnit testing with testthat.\nDocumentation for the package as a whole via an executable README.Rmd.\n\n\n\nWe call the package regexcite and it contains a couple of functions that make common tasks with regular expressions easier. Please note that these functions are very simple and we’re only using them here as a means to guide you through the package development process. If you’re looking for actual helpers for work with regular expressions, there are several proper R packages that address this problem space:\n\n\nstringr (which uses stringi)\nstringi\nrex\nrematch2\n\nAgain, the regexcite package itself is just a device for demonstrating a typical workflow for package development with devtools."
  },
  {
    "objectID": "whole-game.html#preview-the-finished-product",
    "href": "whole-game.html#preview-the-finished-product",
    "title": "2  The Whole Game",
    "section": "\n2.3 Preview the finished product",
    "text": "2.3 Preview the finished product\nThe regexcite package is tracked during its development with the Git version control system. This is purely optional and you can certainly follow along without implementing this. A nice side benefit is that we eventually connect it to a remote repository on GitHub, which means you can see the glorious result we are working towards by visiting regexcite on GitHub: https://github.com/jennybc/regexcite. By inspecting the commit history and especially the diffs, you can see exactly what changes at each step of the process laid out below."
  },
  {
    "objectID": "whole-game.html#create_package",
    "href": "whole-game.html#create_package",
    "title": "2  The Whole Game",
    "section": "\n2.4 create_package()\n",
    "text": "2.4 create_package()\n\nCall create_package() to initialize a new package in a directory on your computer. create_package() will automatically create that directory if it doesn’t exist yet (and that is usually the case). See Section 5.1 for more on creating packages.\nMake a deliberate choice about where to create this package on your computer. It should probably be somewhere within your home directory, alongside your other R projects. It should not be nested inside another RStudio Project, R package, or Git repo. Nor should it be in an R package library, which holds packages that have already been built and installed. The conversion of the source package we create here into an installed package is part of what devtools facilitates. Don’t try to do devtools’ job for it!\nOnce you’ve selected where to create this package, substitute your chosen path into a create_package() call like this:\n\ncreate_package(\"~/path/to/regexcite\")\n\nFor the creation of this book we have to work in a temporary directory, because the book is built non-interactively in the cloud. Behind the scenes, we’re executing our own create_package() command, but don’t be surprised if our output differs a bit from yours.\n\n#> ✔ Creating '/tmp/Rtmpah8JbR/regexcite/'\n#> ✔ Setting active project to '/tmp/Rtmpah8JbR/regexcite'\n#> ✔ Creating 'R/'\n#> ✔ Writing 'DESCRIPTION'\n#> Package: regexcite\n#> Title: What the Package Does (One Line, Title Case)\n#> Version: 0.0.0.9000\n#> Authors@R (parsed):\n#>     * First Last <first.last@example.com> [aut, cre] (YOUR-ORCID-ID)\n#> Description: What the package does (one paragraph).\n#> License: `use_mit_license()`, `use_gpl3_license()` or friends to pick a\n#>     license\n#> Encoding: UTF-8\n#> Roxygen: list(markdown = TRUE)\n#> RoxygenNote: 7.2.3\n#> ✔ Writing 'NAMESPACE'\n#> ✔ Writing 'regexcite.Rproj'\n#> ✔ Adding '^regexcite\\\\.Rproj$' to '.Rbuildignore'\n#> ✔ Adding '.Rproj.user' to '.gitignore'\n#> ✔ Adding '^\\\\.Rproj\\\\.user$' to '.Rbuildignore'\n#> ✔ Setting active project to '<no active project>'\n\nIf you’re working in RStudio, you should find yourself in a new instance of RStudio, opened into your new regexcite package (and Project). If you somehow need to do this manually, navigate to the directory and double click on regexcite.Rproj. RStudio has special handling for packages and you should now see a Build tab in the same pane as Environment and History.\nYou probably need to call library(devtools) again, because create_package() has probably dropped you into a fresh R session, in your new package.\n\nlibrary(devtools)\n\nWhat’s in this new directory that is also an R package and, probably, an RStudio Project? Here’s a listing (locally, you can consult your Files pane):\n\n\n\n\npath\ntype\n\n\n\n.Rbuildignore\nfile\n\n\n.gitignore\nfile\n\n\nDESCRIPTION\nfile\n\n\nNAMESPACE\nfile\n\n\nR\ndirectory\n\n\nregexcite.Rproj\nfile\n\n\n\n\n\n\n\n\n\n\n\nRStudio\n\n\n\nIn the Files pane, go to More (gear symbol) > Show Hidden Files to toggle the visibility of hidden files (a.k.a. “dotfiles”). A select few are visible all the time, but sometimes you want to see them all.\n\n\n\n\n.Rbuildignore lists files that we need to have around but that should not be included when building the R package from source. If you aren’t using RStudio, create_package() may not create this file (nor .gitignore) at first, since there’s no RStudio-related machinery that needs to be ignored. However, you will likely develop the need for .Rbuildignore at some point, regardless of what editor you are using. It is discussed in more detail in Section 4.3.1.\n\n.Rproj.user, if you have it, is a directory used internally by RStudio.\n\n.gitignore anticipates Git usage and tells Git to ignore some standard, behind-the-scenes files created by R and RStudio. Even if you do not plan to use Git, this is harmless.\n\nDESCRIPTION provides metadata about your package. We edit this shortly and Chapter 10 covers the general topic of the DESCRIPTION file.\n\nNAMESPACE declares the functions your package exports for external use and the external functions your package imports from other packages. At this point, it is empty, except for a comment declaring that this is a file you should not edit by hand.\nThe R/ directory is the “business end” of your package. It will soon contain .R files with function definitions.\n\nregexcite.Rproj is the file that makes this directory an RStudio Project. Even if you don’t use RStudio, this file is harmless. Or you can suppress its creation with create_package(..., rstudio = FALSE). More in Section 5.2."
  },
  {
    "objectID": "whole-game.html#use_git",
    "href": "whole-game.html#use_git",
    "title": "2  The Whole Game",
    "section": "\n2.5 use_git()\n",
    "text": "2.5 use_git()\n\nThe regexcite directory is an R source package and an RStudio Project. Now we make it also a Git repository, with use_git(). (By the way, use_git() works in any project, regardless of whether it’s an R package.)\n\nuse_git()\n#> ✔ Initialising Git repo\n#> ✔ Adding '.Rhistory', '.Rdata', '.httr-oauth', '.DS_Store' to '.gitignore'\n\nIn an interactive session, you will be asked if you want to commit some files here and you should accept the offer. Behind the scenes, we’ll also commit those same files.\nSo what has changed in the package? Only the creation of a .git directory, which is hidden in most contexts, including the RStudio file browser. Its existence is evidence that we have indeed initialized a Git repo here.\n\n\n\n\npath\ntype\n\n\n.git\ndirectory\n\n\n\n\nIf you’re using RStudio, it probably requested permission to relaunch itself in this Project, which you should do. You can do so manually by quitting, then relaunching RStudio by double clicking on regexcite.Rproj. Now, in addition to package development support, you have access to a basic Git client in the Git tab of the Environment/History/Build pane.\n\nClick on History (the clock icon in the Git pane) and, if you consented, you will see an initial commit made via use_git():\n\n\n\n\ncommit\nauthor\nmessage\n\n\nda7f61fcbe…\njennybc jennybc@users.noreply.github.com\n\nInitial commit\n\n\n\n\n\n\n\n\n\n\nRStudio\n\n\n\nRStudio can initialize a Git repository, in any Project, even if it’s not an R package, as long you’ve set up RStudio + Git integration. Do Tools > Version Control > Project Setup. Then choose Version control system: Git and initialize a new git repository for this project."
  },
  {
    "objectID": "whole-game.html#write-the-first-function",
    "href": "whole-game.html#write-the-first-function",
    "title": "2  The Whole Game",
    "section": "\n2.6 Write the first function",
    "text": "2.6 Write the first function\nA fairly common task when dealing with strings is the need to split a single string into many parts. The strsplit() function in base R does exactly this.\n\n(x <- \"alfa,bravo,charlie,delta\")\n#> [1] \"alfa,bravo,charlie,delta\"\nstrsplit(x, split = \",\")\n#> [[1]]\n#> [1] \"alfa\"    \"bravo\"   \"charlie\" \"delta\"\n\nTake a close look at the return value.\n\nstr(strsplit(x, split = \",\"))\n#> List of 1\n#>  $ : chr [1:4] \"alfa\" \"bravo\" \"charlie\" \"delta\"\n\nThe shape of this return value often surprises people or, at least, inconveniences them. The input is a character vector of length one and the output is a list of length one. This makes total sense in light of R’s fundamental tendency towards vectorization. But sometimes it’s still a bit of a bummer. Often you know that your input is morally a scalar, i.e. it’s just a single string, and really want the output to be the character vector of its parts.\nThis leads R users to employ various methods of “unlist”-ing the result:\n\nunlist(strsplit(x, split = \",\"))\n#> [1] \"alfa\"    \"bravo\"   \"charlie\" \"delta\"\n\nstrsplit(x, split = \",\")[[1]]\n#> [1] \"alfa\"    \"bravo\"   \"charlie\" \"delta\"\n\nThe second, safer solution is the basis for the inaugural function of regexcite: strsplit1().\n\nstrsplit1 <- function(x, split) {\n  strsplit(x, split = split)[[1]]\n}\n\nThis book does not teach you how to write functions in R. To learn more about that take a look at the Functions chapter of R for Data Science and the Functions chapter of Advanced R.\n\n\n\n\n\n\nTip\n\n\n\nThe name of strsplit1() is a nod to the very handy paste0(), which first appeared in R 2.15.0 in 2012. paste0() was created to address the extremely common use case of paste()-ing strings together without a separator. paste0() has been lovingly described as “statistical computing’s most influential contribution of the 21st century”.\nThe strsplit1() function was so inspiring that it’s now a real function in the stringr package: stringr::str_split_1()!"
  },
  {
    "objectID": "whole-game.html#use_r",
    "href": "whole-game.html#use_r",
    "title": "2  The Whole Game",
    "section": "\n2.7 use_r()\n",
    "text": "2.7 use_r()\n\nWhere should you put the definition of strsplit1()? Save it in a .R file, in the R/ subdirectory of your package. A reasonable starting position is to make a new .R file for each user-facing function in your package and name the file after the function. As you add more functions, you’ll want to relax this and begin to group related functions together. We’ll save the definition of strsplit1() in the file R/strsplit1.R.\nThe helper use_r() creates and/or opens a script below R/. It really shines in a more mature package, when navigating between .R files and the associated test file. But, even here, it’s useful to keep yourself from getting too carried away while working in Untitled4.\n\nuse_r(\"strsplit1\")\n#> • Edit 'R/strsplit1.R'\n#> • Call `use_test()` to create a matching test file\n\nPut the definition of strsplit1() and only the definition of strsplit1() in R/strsplit1.R and save it. The file R/strsplit1.R should NOT contain any of the other top-level code we have recently executed, such as the definition of our practice input x, library(devtools), or use_git(). This foreshadows an adjustment you’ll need to make as you transition from writing R scripts to R packages. Packages and scripts use different mechanisms to declare their dependency on other packages and to store example or test code. We explore this further in Chapter 7."
  },
  {
    "objectID": "whole-game.html#sec-whole-game-load-all",
    "href": "whole-game.html#sec-whole-game-load-all",
    "title": "2  The Whole Game",
    "section": "\n2.8 load_all()\n",
    "text": "2.8 load_all()\n\nHow do we test drive strsplit1()? If this were a regular R script, we might use RStudio to send the function definition to the R Console and define strsplit1() in the global environment. Or maybe we’d call source(\"R/strsplit1.R\"). For package development, however, devtools offers a more robust approach.\nCall load_all() to make strsplit1() available for experimentation.\n\nload_all()\n#> ℹ Loading regexcite\n\nNow call strsplit1(x) to see how it works.\n\n(x <- \"alfa,bravo,charlie,delta\")\n#> [1] \"alfa,bravo,charlie,delta\"\nstrsplit1(x, split = \",\")\n#> [1] \"alfa\"    \"bravo\"   \"charlie\" \"delta\"\n\nNote that load_all() has made the strsplit1() function available, although it does not exist in the global environment.\n\nexists(\"strsplit1\", where = globalenv(), inherits = FALSE)\n#> [1] FALSE\n\nIf you see TRUE instead of FALSE, that indicates you’re still using a script-oriented workflow and sourcing your functions. Here’s how to get back on track:\n\nClean out the global environment and restart R.\nRe-attach devtools with library(devtools) and re-load regexcite with load_all().\nRedefine the test input x and call strsplit1(x, split = \",\") again. This should work!\nRun exists(\"strsplit1\", where = globalenv(), inherits = FALSE) again and you should see FALSE.\n\nload_all() simulates the process of building, installing, and attaching the regexcite package. As your package accumulates more functions, some exported, some not, some of which call each other, some of which call functions from packages you depend on, load_all() gives you a much more accurate sense of how the package is developing than test driving functions defined in the global environment. Also load_all() allows much faster iteration than actually building, installing, and attaching the package. See Section 5.4 for more about load_all().\nTo review what we’ve done so far:\n\nWe wrote our first function, strsplit1(), to split a string into a character vector (not a list containing a character vector).\nWe used load_all() to quickly make this function available for interactive use, as if we’d built and installed regexcite and attached it via library(regexcite).\n\n\n\n\n\n\n\nRStudio\n\n\n\nRStudio exposes load_all() in the Build menu, in the Build pane via More > Load All, and in keyboard shortcuts Ctrl + Shift + L (Windows & Linux) or Cmd + Shift + L (macOS).\n\n\n\n2.8.1 Commit strsplit1()\n\nIf you’re using Git, use your preferred method to commit the new R/strsplit1.R file. We do so behind the scenes here and here’s the associated diff.\n\ndiff --git a/R/strsplit1.R b/R/strsplit1.R\nnew file mode 100644\nindex 0000000..29efb88\n--- /dev/null\n+++ b/R/strsplit1.R\n@@ -0,0 +1,3 @@\n+strsplit1 <- function(x, split) {\n+  strsplit(x, split = split)[[1]]\n+}\n\nFrom this point on, we commit after each step. Remember these commits are available in the public repository."
  },
  {
    "objectID": "whole-game.html#check",
    "href": "whole-game.html#check",
    "title": "2  The Whole Game",
    "section": "\n2.9 check()\n",
    "text": "2.9 check()\n\nWe have informal, empirical evidence that strsplit1() works. But how can we be sure that all the moving parts of the regexcite package still work? This may seem silly to check, after such a small addition, but it’s good to establish the habit of checking this often.\nR CMD check, executed in the shell, is the gold standard for checking that an R package is in full working order. check() is a convenient way to run this without leaving your R session.\nNote that check() produces rather voluminous output, optimized for interactive consumption. We intercept that here and just reveal a summary. Your local check() output will be different.\n\ncheck()\n\n\n── R CMD check results ─────────────────── regexcite 0.0.0.9000 ────\nDuration: 28s\n\n❯ checking DESCRIPTION meta-information ... WARNING\n  Non-standard license specification:\n    `use_mit_license()`, `use_gpl3_license()` or friends to pick a\n    license\n  Standardizable: FALSE\n\n0 errors ✔ | 1 warning ✖ | 0 notes ✔\n\nIt is essential to actually read the output of the check! Deal with problems early and often. It’s just like incremental development of .R and .Rmd files. The longer you go between full checks that everything works, the harder it becomes to pinpoint and solve your problems.\nAt this point, we expect 1 warning (and 0 errors, 0 notes):\nNon-standard license specification:\n  `use_mit_license()`, `use_gpl3_license()` or friends to pick a\n  license\nWe’ll address that soon, by doing exactly what it says. You can learn more about check() in Section 5.5.\n\n\n\n\n\n\nRStudio\n\n\n\nRStudio exposes check() in the Build menu, in the Build pane via Check, and in keyboard shortcuts Ctrl + Shift + E (Windows & Linux) or Cmd + Shift + E (macOS)."
  },
  {
    "objectID": "whole-game.html#edit-description",
    "href": "whole-game.html#edit-description",
    "title": "2  The Whole Game",
    "section": "\n2.10 Edit DESCRIPTION\n",
    "text": "2.10 Edit DESCRIPTION\n\nThe DESCRIPTION file provides metadata about your package and is covered fully in Chapter 10. This is a good time to have a look at regexcite’s current DESCRIPTION. You’ll see it’s populated with boilerplate content, which needs to be replaced.\nTo add your own metadata, make these edits:\n\nMake yourself the author. If you don’t have an ORCID, you can omit the comment = ... portion.\nWrite some descriptive text in the Title and Description fields.\n\n\n\n\n\n\n\nRStudio\n\n\n\nUse Ctrl + . in RStudio and start typing “DESCRIPTION” to activate a helper that makes it easy to open a file for editing. In addition to a filename, your hint can be a function name. This is very handy once a package has lots of files.\n\n\nWhen you’re done, DESCRIPTION should look similar to this:\n\n\n\nPackage: regexcite\nTitle: Make Regular Expressions More Exciting\nVersion: 0.0.0.9000\nAuthors@R: \n    person(\"Jane\", \"Doe\", , \"jane@example.com\", role = c(\"aut\", \"cre\"))\nDescription: Convenience functions to make some common tasks with string\n    manipulation and regular expressions a bit easier.\nLicense: `use_mit_license()`, `use_gpl3_license()` or friends to pick a\n    license\nEncoding: UTF-8\nRoxygen: list(markdown = TRUE)\nRoxygenNote: 7.1.2"
  },
  {
    "objectID": "whole-game.html#use_mit_license",
    "href": "whole-game.html#use_mit_license",
    "title": "2  The Whole Game",
    "section": "\n2.11 use_mit_license()\n",
    "text": "2.11 use_mit_license()\n\n\nPick a License, Any License. – Jeff Atwood\n\nWe currently have a placeholder in the License field of DESCRIPTION that’s deliberately invalid and suggests a resolution.\nLicense: `use_mit_license()`, `use_gpl3_license()` or friends to pick a\n    license\nTo configure a valid license for the package, call use_mit_license().\n\nuse_mit_license()\n#> ✔ Setting License field in DESCRIPTION to 'MIT + file LICENSE'\n#> ✔ Writing 'LICENSE'\n#> ✔ Writing 'LICENSE.md'\n#> ✔ Adding '^LICENSE\\\\.md$' to '.Rbuildignore'\n\nThis configures the License field correctly for the MIT license, which promises to name the copyright holders and year in a LICENSE file. Open the newly created LICENSE file and confirm it looks something like this:\n\nYEAR: 2023\nCOPYRIGHT HOLDER: regexcite authors\n\nLike other license helpers, use_mit_license() also puts a copy of the full license in LICENSE.md and adds this file to .Rbuildignore. It’s considered a best practice to include a full license in your package’s source, such as on GitHub, but CRAN disallows the inclusion of this file in a package tarball."
  },
  {
    "objectID": "whole-game.html#sec-whole-game-document",
    "href": "whole-game.html#sec-whole-game-document",
    "title": "2  The Whole Game",
    "section": "\n2.12 document()\n",
    "text": "2.12 document()\n\nWouldn’t it be nice to get help on strsplit1(), just like we do with other R functions? This requires that your package have a special R documentation file, man/strsplit1.Rd, written in an R-specific markup language that is sort of like LaTeX. Luckily we don’t necessarily have to author that directly.\nWe write a specially formatted comment right above strsplit1(), in its source file, and then let a package called roxygen2 handle the creation of man/strsplit1.Rd. The motivation and mechanics of roxygen2 are covered in Chapter 17.\nIf you use RStudio, open R/strsplit1.R in the source editor and put the cursor somewhere in the strsplit1() function definition. Now do Code > Insert roxygen skeleton. A very special comment should appear above your function, in which each line begins with #'. RStudio only inserts a barebones template, so you will need to edit it to look something like that below.\nIf you don’t use RStudio, create the comment yourself. Regardless, you should modify it to look something like this:\n\n#' Split a string\n#'\n#' @param x A character vector with one element.\n#' @param split What to split on.\n#'\n#' @return A character vector.\n#' @export\n#'\n#' @examples\n#' x <- \"alfa,bravo,charlie,delta\"\n#' strsplit1(x, split = \",\")\nstrsplit1 <- function(x, split) {\n  strsplit(x, split = split)[[1]]\n}\n\n\nBut we’re not done yet! We still need to trigger the conversion of this new roxygen comment into man/strsplit1.Rd with document():\n\ndocument()\n#> ℹ Updating regexcite documentation\n#> Setting `RoxygenNote` to \"7.2.3\"\n#> ℹ Loading regexcite\n#> Writing 'NAMESPACE'\n#> Writing 'strsplit1.Rd'\n\n\n\n\n\n\n\nRStudio\n\n\n\nRStudio exposes document() in the Build menu, in the Build pane via More > Document, and in keyboard shortcuts Ctrl + Shift + D (Windows & Linux) or Cmd + Shift + D (macOS).\n\n\nYou should now be able to preview your help file like so:\n\n?strsplit1\n\nYou’ll see a message like “Rendering development documentation for ‘strsplit1’”, which reminds that you are basically previewing draft documentation. That is, this documentation is present in your package’s source, but is not yet present in an installed package. In fact, we haven’t installed regexcite yet, but we will soon. If ?strplit doesn’t work for you, you may need to call load_all() first, then try again.\nNote also that your package’s documentation won’t be properly wired up until it has been formally built and installed. This polishes off niceties like the links between help files and the creation of a package index.\n\n2.12.1 NAMESPACE changes\nIn addition to converting strsplit1()’s special comment into man/strsplit1.Rd, the call to document() updates the NAMESPACE file, based on @export tags found in roxygen comments. Open NAMESPACE for inspection. The contents should be:\n\n\n# Generated by roxygen2: do not edit by hand\n\nexport(strsplit1)\n\nThe export directive in NAMESPACE is what makes strsplit1() available to a user after attaching regexcite via library(regexcite). Just as it is entirely possible to author .Rd files “by hand”, you can manage NAMESPACE explicitly yourself. But we choose to delegate this to devtools (and roxygen2)."
  },
  {
    "objectID": "whole-game.html#check-again",
    "href": "whole-game.html#check-again",
    "title": "2  The Whole Game",
    "section": "\n2.13 check() again",
    "text": "2.13 check() again\nregexcite should pass R CMD check cleanly now and forever more: 0 errors, 0 warnings, 0 notes.\n\ncheck()\n\n\n── R CMD check results ─────────────────── regexcite 0.0.0.9000 ────\nDuration: 30.4s\n\n0 errors ✔ | 0 warnings ✔ | 0 notes ✔"
  },
  {
    "objectID": "whole-game.html#install",
    "href": "whole-game.html#install",
    "title": "2  The Whole Game",
    "section": "\n2.14 install()\n",
    "text": "2.14 install()\n\nNow that we know we have a minimum viable product, let’s install the regexcite package into your library via install():\n\ninstall()\n\n\n── R CMD build ─────────────────────────────────────────────────────\n* checking for file ‘/tmp/Rtmpah8JbR/regexcite/DESCRIPTION’ ... OK\n* preparing ‘regexcite’:\n* checking DESCRIPTION meta-information ... OK\n* checking for LF line-endings in source and make files and shell scripts\n* checking for empty or unneeded directories\n* building ‘regexcite_0.0.0.9000.tar.gz’\nRunning /opt/R/4.2.3/lib/R/bin/R CMD INSTALL \\\n  /tmp/Rtmpah8JbR/regexcite_0.0.0.9000.tar.gz --install-tests \n* installing to library ‘/home/runner/work/_temp/Library’\n* installing *source* package ‘regexcite’ ...\n** using staged installation\n** R\n** byte-compile and prepare package for lazy loading\n** help\n*** installing help indices\n** building package indices\n** testing if installed package can be loaded from temporary location\n** testing if installed package can be loaded from final location\n** testing if installed package keeps a record of temporary installation path\n* DONE (regexcite)\n\n\n\n\n\n\n\nRStudio\n\n\n\nRStudio exposes similar functionality in the Build menu and in the Build pane via Install and Restart, and in keyboard shortcuts Ctrl + Shift + B (Windows & Linux) or Cmd + Shift + B (macOS).\n\n\nAfter installation is complete, we can attach and use regexcite like any other package. Let’s revisit our small example from the top. This is also a good time to restart your R session and ensure you have a clean workspace.\n\nlibrary(regexcite)\n\nx <- \"alfa,bravo,charlie,delta\"\nstrsplit1(x, split = \",\")\n#> [1] \"alfa\"    \"bravo\"   \"charlie\" \"delta\"\n\nSuccess!"
  },
  {
    "objectID": "whole-game.html#use_testthat",
    "href": "whole-game.html#use_testthat",
    "title": "2  The Whole Game",
    "section": "\n2.15 use_testthat()\n",
    "text": "2.15 use_testthat()\n\nWe’ve tested strsplit1() informally, in a single example. We can formalize this as a unit test. This means we express a concrete expectation about the correct strsplit1() result for a specific input.\nFirst, we declare our intent to write unit tests and to use the testthat package for this, via use_testthat():\n\nuse_testthat()\n#> ✔ Adding 'testthat' to Suggests field in DESCRIPTION\n#> ✔ Setting Config/testthat/edition field in DESCRIPTION to '3'\n#> ✔ Creating 'tests/testthat/'\n#> ✔ Writing 'tests/testthat.R'\n#> • Call `use_test()` to initialize a basic test file and open it for editing.\n\nThis initializes the unit testing machinery for your package. It adds Suggests: testthat to DESCRIPTION, creates the directory tests/testthat/, and adds the script tests/testthat.R. You’ll notice that testthat is probably added with a minimum version of 3.0.0 and a second DESCRIPTION field, Config/testthat/edition: 3. We’ll talk more about those details in Chapter 14.\nHowever, it’s still up to YOU to write the actual tests!\nThe helper use_test() opens and/or creates a test file. You can provide the file’s basename or, if you are editing the relevant source file in RStudio, it will be automatically generated. For many of you, if R/strsplit1.R is the active file in RStudio, you can just call use_test(). However, since this book is built non-interactively, we must provide the basename explicitly:\n\nuse_test(\"strsplit1\")\n#> ✔ Writing 'tests/testthat/test-strsplit1.R'\n#> • Edit 'tests/testthat/test-strsplit1.R'\n\nThis creates the file tests/testthat/test-strsplit1.R. If it had already existed, use_test() would have just opened it. You will notice that there is an example test in the newly created file - delete that code and replace it with this content:\n\ntest_that(\"strsplit1() splits a string\", {\n  expect_equal(strsplit1(\"a,b,c\", split = \",\"), c(\"a\", \"b\", \"c\"))\n})\n\nThis tests that strsplit1() gives the expected result when splitting a string.\nRun this test interactively, as you will when you write your own. If test_that() or strsplit1() can’t be found, that suggests that you probably need to call load_all().\nGoing forward, your tests will mostly run en masse and at arm’s length via test():\n\n\ntest()\n#> ℹ Testing regexcite\n#> ✔ | F W S  OK | Context\n#> \n#> ⠏ |         0 | strsplit1                                           \n#> ✔ |         1 | strsplit1\n#> \n#> ══ Results ═════════════════════════════════════════════════════════\n#> [ FAIL 0 | WARN 0 | SKIP 0 | PASS 1 ]\n\n\n\n\n\n\n\nRStudio\n\n\n\nRStudio exposes test() in the Build menu, in the Build pane via More > Test package, and in keyboard shortcuts Ctrl + Shift + T (Windows & Linux) or Cmd + Shift + T (macOS).\n\n\nYour tests are also run whenever you check() the package. In this way, you basically augment the standard checks with some of your own, that are specific to your package. It is a good idea to use the covr package to track what proportion of your package’s source code is exercised by the tests. More details can be found in Chapter 14."
  },
  {
    "objectID": "whole-game.html#use_package",
    "href": "whole-game.html#use_package",
    "title": "2  The Whole Game",
    "section": "\n2.16 use_package()\n",
    "text": "2.16 use_package()\n\nYou will inevitably want to use a function from another package in your own package. We will need to use package-specific methods for declaring the other packages we need (i.e. our dependencies) and for using these packages in ours. If you plan to submit a package to CRAN, note that this even applies to functions in packages that you think of as “always available”, such as stats::median() or utils::head().\nOne common dilemma when using R’s regular expression functions is uncertainty about whether to request perl = TRUE or perl = FALSE. And then there are often, but not always, other arguments that alter how patterns are matched, such as fixed, ignore.case, and invert. It can be hard to keep track of which functions use which arguments and how the arguments interact, so many users never get to the point where they retain these details without rereading the docs.\nThe stringr package “provides a cohesive set of functions designed to make working with strings as easy as possible”. In particular, stringr uses one regular expression system everywhere (ICU regular expressions) and uses the same interface in every function for controlling matching behaviors, such as case sensitivity. Some people find this easier to internalize and program around. Let’s imagine you decide you’d rather build regexcite based on stringr (and stringi) than base R’s regular expression functions.\nFirst, declare your general intent to use some functions from the stringr namespace with use_package():\n\nuse_package(\"stringr\")\n#> ✔ Adding 'stringr' to Imports field in DESCRIPTION\n#> • Refer to functions with `stringr::fun()`\n\nThis adds the stringr package to the Imports field of DESCRIPTION. And that is all it does.\nLet’s revisit strsplit1() to make it more stringr-like. Here’s a new take on it1:\n\nstr_split_one <- function(string, pattern, n = Inf) {\n  stopifnot(is.character(string), length(string) <= 1)\n  if (length(string) == 1) {\n    stringr::str_split(string = string, pattern = pattern, n = n)[[1]]\n  } else {\n    character()\n  }\n}\n\nNotice that we:\n\nRename the function to str_split_one(), to signal that that is a wrapper around stringr::str_split().\nAdopt the argument names from stringr::str_split(). Now we have string and pattern (and n), instead of x and split.\nIntroduce a bit of argument checking and edge case handling. This is unrelated to the switch to stringr and would be equally beneficial in the version built on strsplit().\nUse the package::function() form when calling stringr::str_split(). This specifies that we want to call the str_split() function from the stringr namespace. There is more than one way to call a function from another package and the one we endorse here is explained fully in Chapter 12.\n\nWhere should we write this new function definition? If we want to keep following the convention where we name the .R file after the function it defines, we now need to do some fiddly file shuffling. Because this comes up fairly often in real life, we have the rename_files() function, which choreographs the renaming of a file in R/ and its associated companion files below test/.\n\nrename_files(\"strsplit1\", \"str_split_one\")\n#> ✔ Moving 'R/strsplit1.R' to 'R/str_split_one.R'\n#> ✔ Moving 'tests/testthat/test-strsplit1.R' to 'tests/testthat/test-str_split_one.R'\n\nRemember: the file name work is purely aspirational. We still need to update the contents of these files!\nHere are the updated contents of R/str_split_one.R. In addition to changing the function definition, we’ve also updated the roxygen header to reflect the new arguments and to include examples that show off the stringr features.\n\n#' Split a string\n#'\n#' @param string A character vector with, at most, one element.\n#' @inheritParams stringr::str_split\n#'\n#' @return A character vector.\n#' @export\n#'\n#' @examples\n#' x <- \"alfa,bravo,charlie,delta\"\n#' str_split_one(x, pattern = \",\")\n#' str_split_one(x, pattern = \",\", n = 2)\n#'\n#' y <- \"192.168.0.1\"\n#' str_split_one(y, pattern = stringr::fixed(\".\"))\nstr_split_one <- function(string, pattern, n = Inf) {\n  stopifnot(is.character(string), length(string) <= 1)\n  if (length(string) == 1) {\n    stringr::str_split(string = string, pattern = pattern, n = n)[[1]]\n  } else {\n    character()\n  }\n}\n\nDon’t forget to also update the test file!\nHere are the updated contents of tests/testthat/test-str_split_one.R. In addition to the change in the function’s name and arguments, we’ve added a couple more tests.\n\ntest_that(\"str_split_one() splits a string\", {\n  expect_equal(str_split_one(\"a,b,c\", \",\"), c(\"a\", \"b\", \"c\"))\n})\n\ntest_that(\"str_split_one() errors if input length > 1\", {\n  expect_error(str_split_one(c(\"a,b\",\"c,d\"), \",\"))\n})\n\ntest_that(\"str_split_one() exposes features of stringr::str_split()\", {\n  expect_equal(str_split_one(\"a,b,c\", \",\", n = 2), c(\"a\", \"b,c\"))\n  expect_equal(str_split_one(\"a.b\", stringr::fixed(\".\")), c(\"a\", \"b\"))\n})\n\nBefore we take the new str_split_one() out for a test drive, we need to call document(). Why? Remember that document() does two main jobs:\n\nConverts our roxygen comments into proper R documentation.\n(Re)generates NAMESPACE.\n\nThe second job is especially important here, since we will no longer export strsplit1() and we will newly export str_split_one(). Don’t be dismayed by the warning about \"Objects listed as exports, but not present in namespace: strsplit1\". That always happens when you remove something from the namespace.\n\ndocument()\n#> ℹ Updating regexcite documentation\n#> ℹ Loading regexcite\n#> Warning: Objects listed as exports, but not present in namespace:\n#> • strsplit1\n#> Writing 'NAMESPACE'\n#> Writing 'str_split_one.Rd'\n#> Deleting 'strsplit1.Rd'\n\nTry out the new str_split_one() function by simulating package installation via load_all():\n\nload_all()\n#> ℹ Loading regexcite\nstr_split_one(\"a, b, c\", pattern = \", \")\n#> [1] \"a\" \"b\" \"c\""
  },
  {
    "objectID": "whole-game.html#use_github",
    "href": "whole-game.html#use_github",
    "title": "2  The Whole Game",
    "section": "\n2.17 use_github()\n",
    "text": "2.17 use_github()\n\nYou’ve seen us making commits during the development process for regexcite. You can see an indicative history at https://github.com/jennybc/regexcite. Our use of version control and the decision to expose the development process means you can inspect the state of the regexcite source at each developmental stage. By looking at so-called diffs, you can see exactly how each devtools helper function modifies the source files that constitute the regexcite package.\nHow would you connect your local regexcite package and Git repository to a companion repository on GitHub? Here are three approaches:\n\n\nuse_github() is a helper that we recommend for the long-term. We won’t demonstrate it here because it requires some credential setup on your end. We also don’t want to tear down and rebuild the public regexcite package every time we build this book.\nSet up the GitHub repo first! It sounds counter-intuitive, but the easiest way to get your work onto GitHub is to initiate there, then use RStudio to start working in a synced local copy. This approach is described in Happy Git’s workflows New project, GitHub first and Existing project, GitHub first.\nCommand line Git can always be used to add a remote repository post hoc. This is described in the Happy Git workflow Existing project, GitHub last.\n\nAny of these approaches will connect your local regexcite project to a GitHub repo, public or private, which you can push to or pull from using the Git client built into RStudio."
  },
  {
    "objectID": "whole-game.html#use_readme_rmd",
    "href": "whole-game.html#use_readme_rmd",
    "title": "2  The Whole Game",
    "section": "\n2.18 use_readme_rmd()\n",
    "text": "2.18 use_readme_rmd()\n\nNow that your package is on GitHub, the README.md file matters. It is the package’s home page and welcome mat, at least until you decide to give it a website (see pkgdown), add a vignette (see Chapter 18), or submit it to CRAN (see Chapter 23).\nThe use_readme_rmd() function initializes a basic, executable README.Rmd ready for you to edit:\n\nuse_readme_rmd()\n#> ✔ Writing 'README.Rmd'\n#> ✔ Adding '^README\\\\.Rmd$' to '.Rbuildignore'\n#> • Update 'README.Rmd' to include installation instructions.\n#> ✔ Writing '.git/hooks/pre-commit'\n\nIn addition to creating README.Rmd, this adds some lines to .Rbuildignore, and creates a Git pre-commit hook to help you keep README.Rmd and README.md in sync.\nREADME.Rmd already has sections that prompt you to:\n\nDescribe the purpose of the package.\nProvide installation instructions. If a GitHub remote is detected when use_readme_rmd() is called, this section is pre-filled with instructions on how to install from GitHub.\nShow a bit of usage.\n\nHow to populate this skeleton? Copy stuff liberally from DESCRIPTION and any formal and informal tests or examples you have. Anything is better than nothing. This is helpful because people probably won’t install your package and comb through individual help files to figure out how to use it.\nWe like to write the README in R Markdown, so it can feature actual usage. The inclusion of live code also makes it less likely that your README grows stale and out-of-sync with your actual package.\nTo make your own edits, if RStudio has not already done so, open README.Rmd for editing. Make sure it shows some usage of str_split_one().\nThe README.Rmd we use is here: README.Rmd and here’s what it contains:\n\n---\noutput: github_document\n---\n\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n```{r, include = FALSE}\nknitr::opts_chunk$set(\n  collapse = TRUE,\n  comment = \"#>\",\n  fig.path = \"man/figures/README-\",\n  out.width = \"100%\"\n)\n```\n\n**NOTE: This is a toy package created for expository purposes, for the second edition of [R Packages](https://r-pkgs.org). It is not meant to actually be useful. If you want a package for factor handling, please see [stringr](https://stringr.tidyverse.org), [stringi](https://stringi.gagolewski.com/),\n[rex](https://cran.r-project.org/package=rex), and\n[rematch2](https://cran.r-project.org/package=rematch2).**\n\n# regexcite\n\n<!-- badges: start -->\n<!-- badges: end -->\n\nThe goal of regexcite is to make regular expressions more exciting!\nIt provides convenience functions to make some common tasks with string manipulation and regular expressions a bit easier.\n\n## Installation\n\nYou can install the development version of regexcite from [GitHub](https://github.com/) with:\n      \n``` r\n# install.packages(\"devtools\")\ndevtools::install_github(\"jennybc/regexcite\")\n```\n\n## Usage\n\nA fairly common task when dealing with strings is the need to split a single string into many parts.\nThis is what `base::strplit()` and `stringr::str_split()` do.\n\n```{r}\n(x <- \"alfa,bravo,charlie,delta\")\nstrsplit(x, split = \",\")\nstringr::str_split(x, pattern = \",\")\n```\n\nNotice how the return value is a **list** of length one, where the first element holds the character vector of parts.\nOften the shape of this output is inconvenient, i.e. we want the un-listed version.\n\nThat's exactly what `regexcite::str_split_one()` does.\n\n```{r}\nlibrary(regexcite)\n\nstr_split_one(x, pattern = \",\")\n```\n\nUse `str_split_one()` when the input is known to be a single string.\nFor safety, it will error if its input has length greater than one.\n\n`str_split_one()` is built on `stringr::str_split()`, so you can use its `n` argument and stringr's general interface for describing the `pattern` to be matched.\n\n```{r}\nstr_split_one(x, pattern = \",\", n = 2)\n\ny <- \"192.168.0.1\"\nstr_split_one(y, pattern = stringr::fixed(\".\"))\n```\n\nDon’t forget to render it to make README.md! The pre-commit hook should remind you if you try to commit README.Rmd, but not README.md, and also when README.md appears to be out-of-date.\nThe very best way to render README.Rmd is with build_readme(), because it takes care to render with the most current version of your package, i.e. it installs a temporary copy from the current source.\n\nbuild_readme()\n#> ℹ Installing regexcite in temporary library\n#> ℹ Building '/tmp/Rtmpah8JbR/regexcite/README.Rmd'\n\nYou can see the rendered README.md simply by visiting regexcite on GitHub.\nFinally, don’t forget to do one last commit. And push, if you’re using GitHub."
  },
  {
    "objectID": "whole-game.html#the-end-check-and-install",
    "href": "whole-game.html#the-end-check-and-install",
    "title": "2  The Whole Game",
    "section": "\n2.19 The end: check() and install()\n",
    "text": "2.19 The end: check() and install()\n\nLet’s run check() again to make sure all is still well.\n\ncheck()\n\n\n── R CMD check results ─────────────────── regexcite 0.0.0.9000 ────\nDuration: 33.6s\n\n0 errors ✔ | 0 warnings ✔ | 0 notes ✔\n\nregexcite should have no errors, warnings or notes. This would be a good time to re-build and install it properly. And celebrate!\n\ninstall()\n\n\n── R CMD build ─────────────────────────────────────────────────────\n* checking for file ‘/tmp/Rtmpah8JbR/regexcite/DESCRIPTION’ ... OK\n* preparing ‘regexcite’:\n* checking DESCRIPTION meta-information ... OK\n* checking for LF line-endings in source and make files and shell scripts\n* checking for empty or unneeded directories\nRemoved empty directory ‘regexcite/tests/testthat/_snaps’\n* building ‘regexcite_0.0.0.9000.tar.gz’\nRunning /opt/R/4.2.3/lib/R/bin/R CMD INSTALL \\\n  /tmp/Rtmpah8JbR/regexcite_0.0.0.9000.tar.gz --install-tests \n* installing to library ‘/home/runner/work/_temp/Library’\n* installing *source* package ‘regexcite’ ...\n** using staged installation\n** R\n** tests\n** byte-compile and prepare package for lazy loading\n** help\n*** installing help indices\n** building package indices\n** testing if installed package can be loaded from temporary location\n** testing if installed package can be loaded from final location\n** testing if installed package keeps a record of temporary installation path\n* DONE (regexcite)\n\nFeel free to visit the regexcite package on GitHub, which appears exactly as developed here. The commit history reflects each individual step, so use the diffs to see the addition and modification of files, as the package evolved. The rest of this book goes in greater detail for each step you’ve seen here and much more."
  },
  {
    "objectID": "whole-game.html#review",
    "href": "whole-game.html#review",
    "title": "2  The Whole Game",
    "section": "\n2.20 Review",
    "text": "2.20 Review\nThis chapter is meant to give you a sense of the typical package development workflow, summarized as a diagram in Figure 2.1. Everything you see here has been touched on in this chapter, with the exception of GitHub Actions, which you will learn more about in Section 21.2.1.\n\n\n\n\nFigure 2.1: The devtools package development workflow.\n\n\n\n\nHere is a review of the key functions you’ve seen in this chapter, organized roughly by their role in the development process.\nThese functions setup parts of the package and are typically called once per package:\n\ncreate_package()\nuse_git()\nuse_mit_license()\nuse_testthat()\nuse_github()\nuse_readme_rmd()\n\nYou will call these functions on a regular basis, as you add functions and tests or take on dependencies:\n\nuse_r()\nuse_test()\nuse_package()\n\nYou will call these functions multiple times per day or per hour, during development:\n\nload_all()\ndocument()\ntest()\ncheck()"
  },
  {
    "objectID": "setup.html#setup-prep",
    "href": "setup.html#setup-prep",
    "title": "3  System setup",
    "section": "\n3.1 Prepare your system",
    "text": "3.1 Prepare your system\nTo get started, make sure you have the latest version of R (at least 4.2.3, which is the version being used to render this book), then run the following code to get the packages you’ll need:\n\ninstall.packages(c(\"devtools\", \"roxygen2\", \"testthat\", \"knitr\"))\n\nMake sure you have a recent version of the RStudio integrated development environment (IDE). New versions are released regularly, so we recommend updating often to get access to the latest and greatest features.\nDownload the current version of RStudio Desktop here: https://posit.co/download/rstudio-desktop/. Most readers can use the free, open source version of RStudio Desktop."
  },
  {
    "objectID": "setup.html#sec-setup-usage",
    "href": "setup.html#sec-setup-usage",
    "title": "3  System setup",
    "section": "\n3.2 devtools, usethis, and you",
    "text": "3.2 devtools, usethis, and you\n\n“I am large, I contain multitudes.”\n— Walt Whitman, Song of Myself\n\nAs mentioned in Section 1.1, devtools is a ‘meta-package’, encompassing and exposing functionality maintained in several smaller packages1. For example, devtools might provide a wrapper function in order to set user-friendly defaults, introduce helpful interactive behaviour, or to combine functionality from multiple sub-packages. In some cases it simply re-exports a function from another package to make it easily available when devtools is attached.\nWhat’s our recommended approach to using devtools and its constituent packages? It varies, depending on your intention:\n\nIf you are using the functions interactively to help you develop your package, you should think of devtools as the provider of your favorite functions for package development. In this case you should attach devtools with library(devtools) and call the functions without qualification (e.g., load_all()).\nIf you are using functions from devtools and friends within the package code you are writing, you should NOT depend on devtools, but should instead access functions via the package that is their primary home.\n\ndevtools should rarely appear in the role of pkg in a qualified call of the form pkg::fcn(). Instead, pkg should be the package where fcn() is defined. For example, if you are creating a function in your package in which you need to query the state of the user’s R session, use sessioninfo::session_info() in your package instead of devtools::session_info().\n\n\nIf you find bugs, try to report them on the package that is a function’s primary home. The help for devtools::fcn() usually states when devtools is re-exporting a function from another package.\n\nThe usethis package is the one constituent package that more people may be aware of and that they may use directly. It holds the functions that act on the files and folders in an R project, most especially for any project that is also an R package. devtools makes it easy to access usethis functions interactively, as when when you call library(devtools), usethis is also attached. Then you can use any function in usethis without qualification, e.g., just call use_testthat(). If you choose to specify the namespace, such as when working in a more programmatic style, then make sure you qualify the call with usethis, e.g., usethis::use_testthat().\n\n3.2.1 Personal startup configuration\nYou can attach devtools like so:\n\nlibrary(devtools)\n\nBut it soon grows aggravating to repeatedly attach devtools in every R session. Therefore, we strongly recommend attaching2 devtools in your .Rprofile startup file, like so:\n\nif (interactive()) {\n  suppressMessages(require(devtools))\n}\n\nFor convenience, the function use_devtools() creates .Rprofile, if needed, opens it for editing, and puts the necessary lines of code on the clipboard and the screen. Another package you may want to handle this way is testthat.\n\n\n\n\n\n\nWarning\n\n\n\nIn general, it’s a bad idea to attach packages in .Rprofile, as it invites you to create R scripts that don’t reflect all of their dependencies via explicit calls to library(foo). But devtools is a workflow package that smooths the process of package development and is, therefore, unlikely to get baked into any analysis scripts. Note how we still take care to only attach in interactive sessions.\n\n\nusethis consults certain options when, for example, creating R packages de novo. This allows you to specify personal defaults for yourself as a package maintainer or for your preferred license. Here’s an example of a code snippet that could go in .Rprofile:\n\noptions(\n  usethis.full_name = \"Jane Doe\",\n  usethis.description = list(\n    `Authors@R` = 'person(\"Jane\", \"Doe\", email = \"jane@example.com\", role = c(\"aut\", \"cre\"), \n    comment = c(ORCID = \"YOUR-ORCID-ID\"))',\n    License = \"MIT + file LICENSE\",\n    Version = \"0.0.0.9000\"\n  )\n)\n\nThe following code shows how to install the development versions of devtools and usethis. At times, this book may describe new features that are in the development version of devtools and related packages, but that haven’t been released yet.\n\ndevtools::install_github(\"r-lib/devtools\")\ndevtools::install_github(\"r-lib/usethis\")\n\n# or, alternatively\npak::pak(\"r-lib/devtools\")\npak::pak(\"r-lib/usethis\")"
  },
  {
    "objectID": "setup.html#setup-tools",
    "href": "setup.html#setup-tools",
    "title": "3  System setup",
    "section": "\n3.3 R build toolchain",
    "text": "3.3 R build toolchain\nTo be fully capable of building R packages from source, you’ll also need a compiler and a few other command line tools. This may not be strictly necessary until you want to build packages containing C or C++ code. Especially if you are using RStudio, you can set this aside for now. The IDE will alert you and provide support once you try to do something that requires you to setup your development environment. Read on for advice on doing this yourself.\n\n3.3.1 Windows\nOn Windows the collection of tools needed for building packages from source is called Rtools.\nRtools is NOT an R package. It is NOT installed with install.packages(). Instead, download it from https://cran.r-project.org/bin/windows/Rtools/ and run the installer.\nDuring the Rtools installation you may see a window asking you to “Select Additional Tasks”.\n\nDo not select the box for “Edit the system PATH”. devtools and RStudio should put Rtools on the PATH automatically when it is needed.\nDo select the box for “Save version information to registry”. It should be selected by default.\n\n3.3.2 macOS\nYou need to install the Xcode command line tools, which requires that you register as an Apple developer (don’t worry, it’s free).\nThen, in the shell, do:\nxcode-select --install\nAlternatively, you can install the current release of full Xcode from the Mac App Store. This includes a very great deal that you do not need, but it offers the advantage of App Store convenience.\n\n3.3.3 Linux\nMake sure you’ve installed not only R, but also the R development tools. For example, on Ubuntu (and Debian) you need to install the r-base-dev package with:\nsudo apt install r-base-dev\nOn Fedora and RedHat, the development tools (called R-core-devel) will be installed automatically when you install with R with sudo dnf install R."
  },
  {
    "objectID": "setup.html#verify-system-prep",
    "href": "setup.html#verify-system-prep",
    "title": "3  System setup",
    "section": "\n3.4 Verify system prep",
    "text": "3.4 Verify system prep\nYou can request a “(package) development situation report” with devtools::dev_sitrep():\n\ndevtools::dev_sitrep()\n#> ── R ───────────────────────────────────────────────────────────────────────\n#> • version: 4.1.2\n#> • path: '/Library/Frameworks/R.framework/Versions/4.1/Resources/'\n#> ── RStudio ─────────────────────────────────────────────────────────────────\n#> • version: 2022.2.0.443\n#> ── devtools ────────────────────────────────────────────────────────────────\n#> • version: 2.4.3.9000\n#> • devtools or its dependencies out of date:\n#>   'gitcreds', 'gh'\n#>   Update them with `devtools::update_packages(\"devtools\")`\n#> ── dev package ─────────────────────────────────────────────────────────────\n#> • package: 'rpkgs'\n#> • path: '/Users/jenny/rrr/r-pkgs/'\n#> • rpkgs dependencies out of date:\n#>   'gitcreds', 'generics', 'tidyselect', 'dplyr', 'tidyr', 'broom', 'gh'\n#>  Update them with `devtools::install_dev_deps()`\n\nIf this reveals that certain tools or packages are missing or out-of-date, you are encouraged to update them."
  },
  {
    "objectID": "structure.html#sec-package-states",
    "href": "structure.html#sec-package-states",
    "title": "4  Package structure and state",
    "section": "\n4.1 Package states",
    "text": "4.1 Package states\nWhen you create or modify a package, you work on its “source code” or “source files”. You interact with the in-development package in its source form. This is NOT the package form you are most familiar with from day-to-day usage. Package development workflows make much more sense if you understand the five states an R package can be in:\n\nsource\nbundled\nbinary\ninstalled\nin-memory\n\nYou already know some of the functions that put packages into these states. For example, install.packages() can move a package from the source, bundled, or binary states into the installed state. devtools::install_github() takes a source package on GitHub and moves it into the installed state. The library() function loads an installed package into memory, making it available for immediate and direct use."
  },
  {
    "objectID": "structure.html#sec-source-package",
    "href": "structure.html#sec-source-package",
    "title": "4  Package structure and state",
    "section": "\n4.2 Source package",
    "text": "4.2 Source package\nA source package is just a directory of files with a specific structure. It includes particular components, such as a DESCRIPTION file, an R/ directory containing .R files, and so on. Most of the remaining chapters in this book are dedicated to detailing these components.\nIf you are new to package development, you may have never seen a package in source form! You might not even have any source packages on your computer. The easiest way to see a package in source form right away is to browse around its code on the web.\nMany R packages are developed in the open on GitHub (or GitLab or similar). The best case scenario is that you visit the package’s CRAN landing page, e.g.:\n\nforcats: https://cran.r-project.org/package=forcats\n\nreadxl: https://cran.r-project.org/package=readxl\n\n\nand one of its URLs links to a repository on a public hosting service, e.g.:\n\nforcats: https://github.com/tidyverse/forcats\n\nreadxl: https://github.com/tidyverse/readxl\n\n\nSome maintainers forget to list this URL, even though the package is developed in a public repository, but you still might be able to discover it via search.\nEven if a package is not developed on a public platform, you can visit its source in the unofficial, read-only mirror maintained by R-hub. Examples:\n\nMASS: https://github.com/cran/MASS\n\ncar: https://github.com/cran/car\n\n\nNote that exploring a package’s source and history within the cran GitHub organisation is not the same as exploring the package’s true development venue, because this source and its evolution is just reverse-engineered from the package’s CRAN releases. This presents a redacted view of the package and its history, but, by definition, it includes everything that is essential."
  },
  {
    "objectID": "structure.html#sec-bundled-package",
    "href": "structure.html#sec-bundled-package",
    "title": "4  Package structure and state",
    "section": "\n4.3 Bundled package",
    "text": "4.3 Bundled package\nA bundled package is a package that’s been compressed into a single file. By convention (from Linux), package bundles in R use the extension .tar.gz and are sometimes referred to as “source tarballs”. This means that multiple files have been reduced to a single file (.tar) and then compressed using gzip (.gz). While a bundle is not that useful on its own, it’s a platform-agnostic, transportation-friendly intermediary between a source package and an installed package.\nIn the rare case that you need to make a bundle from a package you’re developing locally, use devtools::build(). Under the hood, this calls pkgbuild::build() and, ultimately, R CMD build, which is described further in the Building package tarballs section of Writing R Extensions.\nThis should tip you off that a package bundle or “source tarball” is not simply the result of making a tar archive of the source files, then compressing with gzip. By convention, in the R world, a few more operations are carried out when making the .tar.gz file and this is why we’ve elected to refer to this form as a package bundle, in this book.\nEvery CRAN package is available in bundled form, via the “Package source” field of its landing page. Continuing our examples from above, you could download the bundles forcats_0.4.0.tar.gz and readxl_1.3.1.tar.gz (or whatever the current versions may be). You could unpack such a bundle in the shell (not the R console) like so:\ntar xvf forcats_0.4.0.tar.gz\nIf you decompress a bundle, you’ll see it looks almost the same as a source package. Figure 4.1 shows the files present in the source, bundled, and binary forms of a fictional package named zzzpackage. We’ve deliberately crafted this example to include most of the package parts covered in this book. Not every package will include every file seen here, nor does this diagram include every possible file that might appear in a package.\n\n\n\n\nFigure 4.1: Package forms: source vs. bundled vs. binary.\n\n\n\n\nThe main differences between a source package and an uncompressed bundle are:\n\nVignettes have been built, so rendered outputs, such as HTML, appear below inst/doc/ and a vignette index appears in the build/ directory.\nA local source package might contain temporary files used to save time during development, like compilation artefacts in src/. These are never found in a bundle.\nAny files listed in .Rbuildignore are not included in the bundle. These are typically files that facilitate your development process, but that should be excluded from the distributed product.\n\n\n4.3.1 .Rbuildignore\n\nYou won’t need to contemplate the exact structure of package .tar.gz files very often, but you do need to understand the .Rbuildignore file. It controls which files from the source package make it into the downstream forms.\nEach line of .Rbuildignore is a Perl-compatible regular expression that is matched, without regard to case, against the path to each file in the source package1. If the regular expression matches, that file or directory is excluded. Note there are some default exclusions implemented by R itself, mostly relating to classic version control systems and editors, such as SVN, Git, and Emacs.\nWe usually modify .Rbuildignore with the usethis::use_build_ignore() function, which takes care of easy-to-forget details, such as regular expression anchoring and escaping. To exclude a specific file or directory (the most common use case), you MUST anchor the regular expression. For example, to exclude a directory called “notes”, the .Rbuildignore entry must be ^notes$, whereas the unanchored regular expression notes will match any file name containing “notes”, e.g. R/notes.R, man/important-notes.R, data/endnotes.Rdata, etc. We find that use_build_ignore() helps us get more of our .Rbuildignore entries right the first time.\n.Rbuildignore is a way to resolve some of the tension between the practices that support your development process and CRAN’s requirements for submission and distribution (Chapter 23). Even if you aren’t planning to release on CRAN, following these conventions will allow you to make the best use of R’s built-in tooling for package checking and installation. The files you should .Rbuildignore fall into two broad, semi-overlapping classes:\n\nFiles that help you generate package contents programmatically. Examples:\n\nUsing README.Rmd to generate an informative and current README.md (Section 19.1).\nStoring .R scripts to create and update internal or exported data (Section 8.1.1).\n\n\nFiles that drive package development, checking, and documentation, outside of CRAN’s purview. Examples:\n\nFiles relating to the RStudio IDE (Section 5.2).\nUsing the pkgdown package to generate a website (Chapter 20).\nConfiguration files related to continuous integration/deployment (Section 21.2).\n\n\n\nHere is a non-exhaustive list of typical entries in the .Rbuildignore file for a package in the tidyverse:\n^.*\\.Rproj$         # Designates the directory as an RStudio Project\n^\\.Rproj\\.user$     # Used by RStudio for temporary files\n^README\\.Rmd$       # An Rmd file used to generate README.md\n^LICENSE\\.md$       # Full text of the license\n^cran-comments\\.md$ # Comments for CRAN submission\n^data-raw$          # Code used to create data included in the package\n^pkgdown$           # Resources used for the package website\n^_pkgdown\\.yml$     # Configuration info for the package website\n^\\.github$          # GitHub Actions workflows\nNote that the comments above must not appear in an actual .Rbuildignore file; they are included here only for exposition.\nWe’ll mention when you need to add files to .Rbuildignore whenever it’s important. Remember that usethis::use_build_ignore() is an attractive way to manage this file. Furthermore, many usethis functions that add a file that should be listed in .Rbuildignore take care of this automatically. For example, use_read_rmd() adds “^README\\.Rmd$” to .Rbuildignore."
  },
  {
    "objectID": "structure.html#sec-structure-binary",
    "href": "structure.html#sec-structure-binary",
    "title": "4  Package structure and state",
    "section": "\n4.4 Binary package",
    "text": "4.4 Binary package\nIf you want to distribute your package to an R user who doesn’t have package development tools, you’ll need to provide a binary package. The primary maker and distributor of binary packages is CRAN, not individual maintainers. But even if you delegate the responsibility of distributing your package to CRAN, it’s still important for a maintainer to understand the nature of a binary package.\nLike a package bundle, a binary package is a single file. Unlike a bundled package, a binary package is platform specific and there are two basic flavors: Windows and macOS. (Linux users are generally required to have the tools necessary to install from .tar.gz files, although the emergence of resources like Posit Public Package Manager is giving Linux users the same access to binary packages as their colleagues on Windows and macOS.)\nBinary packages for macOS are stored as .tgz, whereas Windows binary packages end in .zip. If you need to make a binary package, use devtools::build(binary = TRUE) on the relevant operating system. Under the hood, this calls pkgbuild::build(binary = TRUE) and, ultimately, R CMD INSTALL --build, which is described further in the Building binary packages section of Writing R Extensions. If you choose to release your package on CRAN (Chapter 23), you submit your package in bundled form, then CRAN creates and distributes the package binaries.\nCRAN packages are usually available in binary form, for both macOS and Windows, for the current, previous, and (possibly) development versions of R. Continuing our examples from above, you could download binary packages such as:\n\nforcats for macOS: forcats_0.4.0.tgz\n\nreadxl for Windows: readxl_1.3.1.zip\n\n\nand this is, indeed, part of what’s usually going on behind the scenes when you call install.packages().\nIf you uncompress a binary package, you’ll see that the internal structure is rather different from a source or bundled package. Figure 4.1 includes this comparison, so this is a good time to revisit that diagram. Here are some of the most notable differences:\n\nThere are no .R files in the R/ directory - instead there are three files that store the parsed functions in an efficient file format. This is basically the result of loading all the R code and then saving the functions with save(). (In the process, this adds a little extra metadata to make things as fast as possible).\nA Meta/ directory contains a number of .rds files. These files contain cached metadata about the package, like what topics the help files cover and a parsed version of the DESCRIPTION file. (You can use readRDS() to see exactly what’s in those files). These files make package loading faster by caching costly computations.\nThe actual help content appears in help/ and html/ (no longer in man/).\nIf you had any code in the src/ directory, there will now be a libs/ directory that contains the results of compiling the code.\nIf you had any objects in data/, they have now been converted into a more efficient form.\nThe contents of inst/ are moved to the top-level directory. For example, vignette files are now in doc/.\nSome files and folders have been dropped, such as README.md, build/, tests/, and vignettes/."
  },
  {
    "objectID": "structure.html#sec-installed-package",
    "href": "structure.html#sec-installed-package",
    "title": "4  Package structure and state",
    "section": "\n4.5 Installed package",
    "text": "4.5 Installed package\nAn installed package is a binary package that’s been decompressed into a package library (described in Section 4.7). Figure 4.2 illustrates the many ways a package can be installed, along with a few other functions for converting a package from one state to another. This diagram is complicated! In an ideal world, installing a package would involve stringing together a set of simple steps: source -> bundle, bundle -> binary, binary -> installed. In the real world, it’s not this simple because there are often (faster) shortcuts available.\n\n\n\n\nFigure 4.2: Many methods for converting between package states.\n\n\n\n\nThe built-in command line tool R CMD INSTALL powers all package installation. It can install a package from source files, a bundle (a.k.a. a source tarball), or a binary package. Details are available in the Installing packages section of R Installation and Administration. Just like with devtools::build(), devtools provides a wrapper function, devtools::install(), that makes this tool available from within an R session.\n\n\n\n\n\n\nRStudio\n\n\n\nRStudio can also help you install your in-development package via the Install and More drop-downs in the Build pane and with Install Package in the Build menu.\n\n\nMost useRs understandably like to install packages from the comfort of an R session and directly from CRAN. The built-in function install.packages() meets this need. It can download the package, in various forms, install it, and optionally attend to the installation of dependencies.\nThere is a price, however, for the convenience of installing R packages from within an R session. As you might expect, it can be a bit tricky to re-install a package that is already in use in the current session. This actually works most of the time, but sometimes it does not, especially when installing an R package with compiled code on Windows. Due to how file handles are locked on Windows, an attempt to install a new version of a package that’s in use can result in a corrupt installation where the package’s R code has been updated, but its compiled code has not. When troubleshooting, Windows users should strive to install packages in a clean R session, with as few packages loaded as possible.\nThe pak package (https://pak.r-lib.org/) is a relative newcomer (at the time of writing) and provides a promising alternative to install.packages(), as well as other more specialized functions such as devtools::install_github(). It’s too early to make a blanket recommendation for using pak for all of your package installation needs, but we are certainly using it more and more in our personal workflows. One of pak’s flagship features is that it nicely solves the “locked DLL” problem described above, i.e. updating a package with compiled code on Windows. As you get deeper into package development, you will find yourself doing a whole new set of tasks, such as installing a dependency from an in-development branch or scrutinizing package dependency trees. pak provides a rich toolkit for this and many other related tasks. We predict that pak will soon become our official recommendation for how to install packages (and more).\nHowever, in the meantime, we describe the status quo. devtools has long offered a family of install_*() functions to address some needs beyond the reach of install.packages() or to make existing capabilities easier to access. These functions are actually maintained in the remotes package and are re-exported by devtools. (Given what we said above, it is likely that remotes will essentially become superseded, in favor of pak, but we’re not quite there yet.)\n\nlibrary(remotes)\n\nfuns <- as.character(lsf.str(\"package:remotes\"))\ngrep(\"^install_.+\", funs, value = TRUE)\n#>  [1] \"install_bioc\"      \"install_bitbucket\" \"install_cran\"     \n#>  [4] \"install_deps\"      \"install_dev\"       \"install_git\"      \n#>  [7] \"install_github\"    \"install_gitlab\"    \"install_local\"    \n#> [10] \"install_remote\"    \"install_svn\"       \"install_url\"      \n#> [13] \"install_version\"\n\ninstall_github() is the most useful of these functions and is also featured in Figure 4.2. It is the flagship example of a family of functions that can download a package from a remote location that is not CRAN and do whatever is necessary to install it and its dependencies. The rest of the devtools/remotes install_*() functions are aimed at making things that are technically possible with base tooling a bit easier or more explicit, such as install_version() which installs a specific version of a CRAN package.\nAnalogous to .Rbuildignore, described in section Section 4.3.1, .Rinstignore lets you keep files present in a package bundle out of the installed package. However, in contrast to .Rbuildignore, this is rather obscure and rarely needed."
  },
  {
    "objectID": "structure.html#in-memory-package",
    "href": "structure.html#in-memory-package",
    "title": "4  Package structure and state",
    "section": "\n4.6 In-memory package",
    "text": "4.6 In-memory package\nWe finally arrive at a command familiar to everyone who uses R:\n\nlibrary(usethis)\n\nAssuming usethis is installed, this call makes its functions available for use, i.e. now we can do:\n\ncreate_package(\"/path/to/my/coolpackage\")\n\nThe usethis package has been loaded into memory and, in fact, has also been attached to the search path. The distinction between loading and attaching packages is not important when you’re writing scripts, but it’s very important when you’re writing packages. You’ll learn more about the difference and why it’s important in Section 11.4.\nlibrary() is not a great way to iteratively tweak and test drive a package you’re developing, because it only works for an installed package. In Section 5.4, you’ll learn how devtools::load_all() accelerates development by allowing you to load a source package directly into memory."
  },
  {
    "objectID": "structure.html#sec-library",
    "href": "structure.html#sec-library",
    "title": "4  Package structure and state",
    "section": "\n4.7 Package libraries",
    "text": "4.7 Package libraries\nWe just discussed the library() function, whose name is inspired by what it does. When you call library(somepackage), R looks through the current libraries for an installed package named “somepackage” and, if successful, it makes somepackage available for use.\nIn R, a library is a directory containing installed packages, sort of like a library for books. Unfortunately, in the R world, you will frequently encounter confused usage of the words “library” and “package”. It’s common for someone to refer to dplyr, for example, as a library when it is actually a package. There are a few reasons for the confusion. First, R’s terminology arguably runs counter to broader programming conventions, where the usual meaning of “library” is closer to what we mean by “package”. The name of the library() function itself probably reinforces the wrong associations. Finally, this vocabulary error is often harmless, so it’s easy for R users to fall into the wrong habit and for people who point out this mistake to look like insufferable pedants. But here’s the bottom line:\n\nWe use the library() function to load 2 a package.\n\nThe distinction between the two is important and useful as you get involved in package development.\nYou can have multiple libraries on your computer. In fact, many of you already do, especially if you’re on Windows. You can use .libPaths() to see which libraries are currently active. Here’s how this might look on Windows:\n\n# on Windows\n.libPaths()\n#> [1] \"C:/Users/jenny/Documents/R/win-library/4.2\"\n#> [2] \"C:/Program Files/R/R-4.2.2/library\"\n\nlapply(.libPaths(), list.dirs, recursive = FALSE, full.names = FALSE)\n#> [[1]]\n#>   [1] \"abc\"           \"anytime\"       \"askpass\"       \"assertthat\"   \n#>  ...\n#> [145] \"zeallot\"      \n#> \n#> [[2]]\n#>  [1] \"base\"         \"boot\"         \"class\"        \"cluster\"     \n#>  [5] \"codetools\"    \"compiler\"     \"datasets\"     \"foreign\"     \n#>  [9] \"graphics\"     \"grDevices\"    \"grid\"         \"KernSmooth\"  \n#> [13] \"lattice\"      \"MASS\"         \"Matrix\"       \"methods\"     \n#> [17] \"mgcv\"         \"nlme\"         \"nnet\"         \"parallel\"    \n#> [21] \"rpart\"        \"spatial\"      \"splines\"      \"stats\"       \n#> [25] \"stats4\"       \"survival\"     \"tcltk\"        \"tools\"       \n#> [29] \"translations\" \"utils\"\n\nHere’s a similar look on macOS (but your results may vary):\n\n# on macOS\n.libPaths()\n#> [1] \"/Users/jenny/Library/R/arm64/4.2/library\"\n#> [2] \"/Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/library\"\n\nlapply(.libPaths(), list.dirs, recursive = FALSE, full.names = FALSE)\n#> [[1]]\n#>    [1] \"abc\"                  \"abc.data\"             \"abind\"                \n#>  ...\n#> [1033] \"Zelig\"                \"zip\"                  \"zoo\"                 \n#> \n#> [[2]]\n#>  [1] \"base\"         \"boot\"         \"class\"        \"cluster\"     \n#>  [5] \"codetools\"    \"compiler\"     \"datasets\"     \"foreign\"     \n#>  [9] \"graphics\"     \"grDevices\"    \"grid\"         \"KernSmooth\"  \n#> [13] \"lattice\"      \"MASS\"         \"Matrix\"       \"methods\"     \n#> [17] \"mgcv\"         \"nlme\"         \"nnet\"         \"parallel\"    \n#> [21] \"rpart\"        \"spatial\"      \"splines\"      \"stats\"       \n#> [25] \"stats4\"       \"survival\"     \"tcltk\"        \"tools\"       \n#> [29] \"translations\" \"utils\"\n\nIn both cases we see two active libraries, consulted in this order:\n\nA user library\nA system-level or global library\n\nThis setup is typical on Windows, but is something you usually need to opt into on macOS and Linux3. With this setup, add-on packages installed from CRAN (or elsewhere) or under local development are kept in the user library. Above, the macOS system is used as a primary development machine and has many packages here (~1000), whereas the Windows system is only used occasionally and is much more spartan. The core set of base and recommended packages that ship with R live in the system-level library and are the same on all operating systems. This separation appeals to many developers and makes it easy to, for example, clean out your add-on packages without disturbing your base R installation.\nIf you’re on macOS or Linux and only see one library, there is no urgent need to change anything. But next time you upgrade R, consider creating a user-level library. By default, R looks for a user library found at the path stored in the environment variable R_LIBS_USER, which itself defaults to ~/Library/R/m/x.y/library, on macOS, and ~/R/m-library/x.y on Linux (where m is a concise description of your CPU architecture, and x.y is the R version). You can see this path with Sys.getenv(\"R_LIBS_USER\"). These directories do not exist by default, and the use of them must be enabled by creating the directory. When you install a new version of R, and prior to installing any add-on packages, use dir.create(Sys.getenv(\"R_LIBS_USER\"), recursive = TRUE) to create a user library in the default location. Now you will have the library setup seen above. Alternatively, you could set up a user library elsewhere and tell R about that by setting the R_LIBS_USER environment variable in .Renviron. The simplest way to edit your .Renviron file is with usethis::edit_r_environ(), which will create the file if it doesn’t exist, and open it for editing.\nThe filepaths for these libraries also make it clear they are associated with a specific version of R (4.2.x at the time of writing), which is also typical. This reflects and enforces the fact that you need to reinstall your add-on packages when you update R from, say, 4.1 to 4.2, which is a change in the minor version. You generally do not need to re-install add-on packages for a patch release, e.g., going from R 4.2.1 to 4.2.2.\nAs your R usage grows more sophisticated, it’s common to start managing package libraries with more intention. For example, tools like renv (and its predecessor packrat) automate the process of managing project-specific libraries. This can be important for making data products reproducible, portable, and isolated from one another. A package developer might prepend the library search path with a temporary library, containing a set of packages at specific versions, in order to explore issues with backwards and forwards compatibility, without affecting other day-to-day work. Reverse dependency checks are another example where we explicitly manage the library search path.\nHere are the main levers that control which libraries are active, in order of scope and persistence:\n\nEnvironment variables, like R_LIBS and R_LIBS_USER, which are consulted at startup.\nCalling .libPaths() with one or more filepaths.\nExecuting small snippets of code with a temporarily altered library search path via withr::with_libpaths().\nArguments to individual functions, like install.packages(lib =) and library(lib.loc =).\n\nFinally, it’s important to note that library() should NEVER be used inside a package. Packages and scripts rely on different mechanisms for declaring their dependencies and this is one of the biggest adjustments you need to make in your mental model and habits. We explore this topic fully in Section 10.6 and Chapter 12."
  },
  {
    "objectID": "workflow101.html#sec-workflow101-create-package",
    "href": "workflow101.html#sec-workflow101-create-package",
    "title": "5  Fundamental development workflows",
    "section": "\n5.1 Create a package",
    "text": "5.1 Create a package\n\n5.1.1 Survey the existing landscape\nMany packages are born out of one person’s frustration at some common task that should be easier. How should you decide whether something is package-worthy? There’s no definitive answer, but it’s helpful to appreciate at least two types of payoff:\n\nProduct: your life will be better when this functionality is implemented formally, in a package.\nProcess: greater mastery of R will make you more effective in your work.\n\nIf all you care about is the existence of a product, then your main goal is to navigate the space of existing packages. Silge, Nash, and Graves organized a survey and sessions around this at useR! 2017 and their write up for the R Journal (Silge, Nash, and Graves 2018) provides a comprehensive roundup of resources.\nIf you are looking for ways to increase your R mastery, you should still educate yourself about the landscape. But there are plenty of good reasons to make your own package, even if there is relevant prior work. The way experts got that way is by actually building things, often very basic things, and you deserve the same chance to learn by tinkering. If you’re only allowed to work on things that have never been touched, you’re likely looking at problems that are either very obscure or very difficult.\nIt’s also valid to evaluate the suitability of existing tools on the basis of user interface, defaults, and edge case behaviour. A package may technically do what you need, but perhaps it’s very unergonomic for your use case. In this case, it may make sense for you to develop your own implementation or to write wrapper functions that smooth over the sharp edges.\nIf your work falls into a well-defined domain, educate yourself about the existing R packages, even if you’ve resolved to create your own package. Do they follow specific design patterns? Are there specific data structures that are common as the primary input and output? For example, there is a very active R community around spatial data analysis (r-spatial.org) that has successfully self-organised to promote greater consistency across packages with different maintainers. In modeling, the hardhat package provides scaffolding for creating a modeling package that plays well with the tidymodels ecosystem. Your package will get more usage and will need less documentation if it fits nicely into the surrounding landscape.\n\n5.1.2 Name your package\n\n“There are only two hard things in Computer Science: cache invalidation and naming things.”\n— Phil Karlton\n\nBefore you can create your package, you need to come up with a name for it. This can be the hardest part of creating a package! (Not least because no one can automate it for you.)\n\n5.1.2.1 Formal requirements\nThere are three formal requirements:\n\nThe name can only consist of letters, numbers, and periods, i.e., ..\nIt must start with a letter.\nIt cannot end with a period.\n\nUnfortunately, this means you can’t use either hyphens or underscores, i.e., - or _, in your package name. We recommend against using periods in package names, due to confusing associations with file extensions and S3 methods.\n\n5.1.2.2 Things to consider\nIf you plan to share your package with others, it’s important to come up with a good name. Here are some tips:\n\nPick a unique name that’s easy to Google. This makes it easy for potential users to find your package (and associated resources) and for you to see who’s using it.\n\nDon’t pick a name that’s already in use on CRAN or Bioconductor. You may also want to consider some other types of name collision:\n\nIs there an in-development package maturing on, say, GitHub that already has some history and seems to be heading towards release?\nIs this name already used for another piece of software or for a library or framework in, e.g., the Python or JavaScript ecosystem?\n\n\nAvoid using both upper and lower case letters: doing so makes the package name hard to type and even harder to remember. For example, it’s hard to remember if it’s Rgtk2 or RGTK2 or RGtk2.\nGive preference to names that are pronounceable, so people are comfortable talking about your package and have a way to hear it inside their head.\n\nFind a word that evokes the problem and modify it so that it’s unique. Here are some examples:\n\nlubridate makes dates and times easier.\nrvest “harvests” the content from web pages.\nr2d3 provides utilities for working with D3 visualizations.\nforcats is an anagram of factors, which we use for categorical data.\n\n\n\nUse abbreviations, like the following:\n\nRcpp = R + C++ (plus plus)\nbrms = Bayesian Regression Models using Stan\n\n\n\nAdd an extra R, for example:\n\nstringr provides string tools.\nbeepr plays notification sounds.\ncallr calls R, from R.\n\n\n\nDon’t get sued.\n\nIf you’re creating a package that talks to a commercial service, check the branding guidelines. For example, rDrop isn’t called rDropbox because Dropbox prohibits any applications from using the full trademarked name.\n\n\n\nNick Tierney presents a fun typology of package names in his Naming Things blog post, which also includes more inspiring examples. He also has some experience with renaming packages; the post So, you’ve decided to change your r package name is a good resource if you don’t get this right the first time.\n\n5.1.2.3 Use the available package\nIt is impossible to abide by all of the above suggestions simultaneously, so you will need to make some trade-offs. The available package has a function called available() that helps you evaluate a potential package name from many angles:\n\nlibrary(available)\n\navailable(\"doofus\")\n#> Urban Dictionary can contain potentially offensive results,\n#>   should they be included? [Y]es / [N]o:\n#> 1: 1\n#> ── doofus ──────────────────────────────────────────────────────────────────\n#> Name valid: ✔\n#> Available on CRAN: ✔ \n#> Available on Bioconductor: ✔\n#> Available on GitHub:  ✔ \n#> Abbreviations: http://www.abbreviations.com/doofus\n#> Wikipedia: https://en.wikipedia.org/wiki/doofus\n#> Wiktionary: https://en.wiktionary.org/wiki/doofus\n#> Sentiment:???\n\navailable::available() does the following:\n\nChecks for validity.\nChecks availability on CRAN, Bioconductor, and beyond.\nSearches various websites to help you discover any unintended meanings. In an interactive session, the URLs you see above are opened in browser tabs.\nAttempts to report whether name has positive or negative sentiment.\n\npak::pkg_name_check() is alternative function with a similar purpose. Since the pak package is under more active development than available, it may emerge as the better option going forward.\n\n5.1.3 Package creation\nOnce you’ve come up with a name, there are two ways to create the package.\n\nCall usethis::create_package().\nIn RStudio, do File > New Project > New Directory > R Package. This ultimately calls usethis::create_package(), so really there’s just one way.\n\nThis produces the smallest possible working package, with three components:\n\nAn R/ directory, which you’ll learn about in Chapter 7.\nA basic DESCRIPTION file, which you’ll learn about in Chapter 10.\nA basic NAMESPACE file, which you’ll learn about in Section 11.2.2.\n\nIt may also include an RStudio project file, pkgname.Rproj, that makes your package easy to use with RStudio, as described below. Basic .Rbuildignore and .gitignore files are also left behind.\n\n\n\n\n\n\nWarning\n\n\n\nDon’t use package.skeleton() to create a package. Because this function comes with R, you might be tempted to use it, but it creates a package that immediately throws errors with R CMD build. It anticipates a different development process than we use here, so repairing this broken initial state just makes unnecessary work for people who use devtools (and, especially, roxygen2). Use create_package().\n\n\n\n5.1.4 Where should you create_package()?\nThe main and only required argument to create_package() is the path where your new package will live:\n\ncreate_package(\"path/to/package/pkgname\")\n\nRemember that this is where your package lives in its source form (Section 4.2), not in its installed form (Section 4.5). Installed packages live in a library and we discussed conventional setups for libraries in Section 4.7.\nWhere should you keep source packages? The main principle is that this location should be distinct from where installed packages live. In the absence of external considerations, a typical user should designate a directory inside their home directory for R (source) packages. We discussed this with colleagues and the source of many tidyverse packages lives inside directories like ~/rrr/, ~/documents/tidyverse/, ~/r/packages/, or ~/pkg/. Some of us use one directory for this, others divide source packages among a few directories based on their development role (contributor vs. not), GitHub organization (tidyverse vs r-lib), development stage (active vs. not), and so on.\nThe above probably reflects that we are primarily tool-builders. An academic researcher might organize their files around individual publications, whereas a data scientist might organize around data products and reports. There is no particular technical or traditional reason for one specific approach. As long as you keep a clear distinction between source and installed packages, just pick a strategy that works within your overall system for file organization, and use it consistently."
  },
  {
    "objectID": "workflow101.html#sec-workflow101-rstudio-projects",
    "href": "workflow101.html#sec-workflow101-rstudio-projects",
    "title": "5  Fundamental development workflows",
    "section": "\n5.2 RStudio Projects",
    "text": "5.2 RStudio Projects\ndevtools works hand-in-hand with RStudio, which we believe is the best development environment for most R users. To be clear, you can use devtools without using RStudio and you can develop packages in RStudio without using devtools. But there is a special, two-way relationship that makes it very rewarding to use devtools and RStudio together.\n\n\n\n\n\n\nRStudio\n\n\n\nAn RStudio Project, with a capital “P”, is a regular directory on your computer that includes some (mostly hidden) RStudio infrastructure to facilitate your work on one or more projects, with a lowercase “p”. A project might be an R package, a data analysis, a Shiny app, a book, a blog, etc.\n\n\n\n5.2.1 Benefits of RStudio Projects\nFrom Section 4.2, you already know that a source package lives in a directory on your computer. We strongly recommend that each source package is also an RStudio Project. Here are some of the payoffs:\n\nProjects are very “launch-able”. It’s easy to fire up a fresh instance of RStudio in a Project, with the file browser and working directory set exactly the way you need, ready for work.\n\nEach Project is isolated; code run in one Project does not affect any other Project.\n\nYou can have several RStudio Projects open at once and code executed in Project A does not have any effect on the R session and workspace of Project B.\n\n\nYou get handy code navigation tools like F2 to jump to a function definition and Ctrl + . to look up functions or files by name.\n\nYou get useful keyboard shortcuts and a clickable interface for common package development tasks, like generating documentation, running tests, or checking the entire package.\n\n\n\n\nFigure 5.1: Keyboard Shortcut Quick Reference in RStudio.\n\n\n\n\n\n\n\n\n\n\n\n\nRStudio\n\n\n\nTo see the most useful keyboard shortcuts, press Alt + Shift + K or use Help > Keyboard Shortcuts Help. You should see something like Figure 5.1.\nRStudio also provides the Command Palette which gives fast, searchable access to all of the IDE’s commands – especially helpful when you can’t remember a particular keyboard shortcut. It is invoked via Ctrl + Shift + P (Windows & Linux) or Cmd + Shift + P (macOS).\n\n\n\n\n\n\n\n\nRStudio\n\n\n\nFollow @rstudiotips on Twitter for a regular dose of RStudio tips and tricks.\n\n\n\n5.2.2 How to get an RStudio Project\nIf you follow our recommendation to create new packages with create_package(), each new package will also be an RStudio Project, if you’re working from RStudio.\nIf you need to designate the directory of a pre-existing source package as an RStudio Project, choose one of these options:\n\nIn RStudio, do File > New Project > Existing Directory.\nCall create_package() with the path to the pre-existing R source package.\nCall usethis::use_rstudio(), with the active usethis project set to an existing R package. In practice, this probably means you just need to make sure your working directory is inside the pre-existing package directory.\n\n5.2.3 What makes an RStudio Project?\nA directory that is an RStudio Project will contain an .Rproj file. Typically, if the directory is named “foo”, the Project file is foo.Rproj. And if that directory is also an R package, then the package name is usually also “foo”. The path of least resistance is to make all of these names coincide and to NOT nest your package inside a subdirectory inside the Project. If you settle on a different workflow, just know it may feel like you are fighting with the tools.\nAn .Rproj file is just a text file. Here is a representative project file you might see in a Project initiated via usethis:\nVersion: 1.0\n\nRestoreWorkspace: No\nSaveWorkspace: No\nAlwaysSaveHistory: Default\n\nEnableCodeIndexing: Yes\nEncoding: UTF-8\n\nAutoAppendNewline: Yes\nStripTrailingWhitespace: Yes\nLineEndingConversion: Posix\n\nBuildType: Package\nPackageUseDevtools: Yes\nPackageInstallArgs: --no-multiarch --with-keep.source\nPackageRoxygenize: rd,collate,namespace\nYou don’t need to modify this file by hand. Instead, use the interface available via Tools > Project Options (Figure 5.2) or Project Options in the Projects menu in the top-right corner (Figure 5.3).\n\n\n\n\nFigure 5.2: Project Options in RStudio.\n\n\n\n\n\n\n\n\nFigure 5.3: Projects Menu in RStudio.\n\n\n\n\n\n5.2.4 How to launch an RStudio Project\nDouble-click the foo.Rproj file in macOS’s Finder or Windows Explorer to launch the foo Project in RStudio.\nYou can also launch Projects from within RStudio via File > Open Project (in New Session) or the Projects menu in the top-right corner.\nIf you use a productivity or launcher app, you can probably configure it to do something delightful for .Rproj files. We both use Alfred for this 1, which is macOS only, but similar tools exist for Windows. In fact, this is a very good reason to use a productivity app in the first place.\nIt is very normal – and productive! – to have multiple Projects open at once.\n\n5.2.5 RStudio Project vs. active usethis project\nYou will notice that most usethis functions don’t take a path: they operate on the files in the “active usethis project”. The usethis package assumes that 95% of the time all of these coincide:\n\nThe current RStudio Project, if using RStudio.\nThe active usethis project.\nCurrent working directory for the R process.\n\nIf things seem funky, call proj_sitrep() to get a “situation report”. This will identify peculiar situations and propose ways to get back to a happier state.\n\n# these should usually be the same (or unset)\nproj_sitrep()\n#> *   working_directory: '/Users/jenny/rrr/readxl'\n#> * active_usethis_proj: '/Users/jenny/rrr/readxl'\n#> * active_rstudio_proj: '/Users/jenny/rrr/readxl'"
  },
  {
    "objectID": "workflow101.html#working-directory-and-filepath-discipline",
    "href": "workflow101.html#working-directory-and-filepath-discipline",
    "title": "5  Fundamental development workflows",
    "section": "\n5.3 Working directory and filepath discipline",
    "text": "5.3 Working directory and filepath discipline\nAs you develop your package, you will be executing R code. This will be a mix of workflow calls (e.g., document() or test()) and ad hoc calls that help you write your functions, examples, and tests. We strongly recommend that you keep the top-level of your source package as the working directory of your R process. This will generally happen by default, so this is really a recommendation to avoid development workflows that require you to fiddle with working directory.\nIf you’re totally new to package development, you don’t have much basis for supporting or resisting this proposal. But those with some experience may find this recommendation somewhat upsetting. You may be wondering how you are supposed to express paths when working in subdirectories, such as tests/. As it becomes relevant, we’ll show you how to exploit path-building helpers, such as testthat::test_path(), that determine paths at execution time.\nThe basic idea is that by leaving working directory alone, you are encouraged to write paths that convey intent explicitly (“read foo.csv from the test directory”) instead of implicitly (“read foo.csv from current working directory, which I think is going to be the test directory”). A sure sign of reliance on implicit paths is incessant fiddling with your working directory, because you’re using setwd() to manually fulfill the assumptions that are implicit in your paths.\nUsing explicit paths can design away a whole class of path headaches and makes day-to-day development more pleasant as well. There are two reasons why implicit paths are hard to get right:\n\nRecall the different forms that a package can take during the development cycle (Chapter 4). These states differ from each other in terms of which files and folders exist and their relative positions within the hierarchy. It’s tricky to write relative paths that work across all package states.\nEventually, your package will be processed with built-in tools like R CMD build, R CMD check, and R CMD INSTALL, by you and potentially CRAN. It’s hard to keep track of what the working directory will be at every stage of these processes.\n\nPath helpers like testthat::test_path(), fs::path_package(), and the rprojroot package are extremely useful for building resilient paths that hold up across the whole range of situations that come up during development and usage. Another way to eliminate brittle paths is to be rigorous in your use of proper methods for storing data inside your package (Chapter 8) and to target the session temp directory when appropriate, such as for ephemeral testing artefacts (Chapter 14)."
  },
  {
    "objectID": "workflow101.html#sec-workflow101-load-all",
    "href": "workflow101.html#sec-workflow101-load-all",
    "title": "5  Fundamental development workflows",
    "section": "\n5.4 Test drive with load_all()\n",
    "text": "5.4 Test drive with load_all()\n\nThe load_all() function is arguably the most important part of the devtools workflow.\n\n# with devtools attached and\n# working directory set to top-level of your source package ...\n\nload_all()\n\n# ... now experiment with the functions in your package\n\nload_all() is the key step in this “lather, rinse, repeat” cycle of package development:\n\nTweak a function definition.\nload_all()\nTry out the change by running a small example or some tests.\n\nWhen you’re new to package development or to devtools, it’s easy to overlook the importance of load_all() and fall into some awkward habits from a data analysis workflow.\n\n5.4.1 Benefits of load_all()\n\nWhen you first start to use a development environment, like RStudio or VS Code, the biggest win is the ability to send lines of code from an .R script for execution in R console. The fluidity of this is what makes it tolerable to follow the best practice of regarding your source code as real 2 (as opposed to objects in the workspace) and saving .R files (as opposed to saving and reloading .Rdata).\nload_all() has the same significance for package development and, ironically, requires that you NOT test drive package code in the same way as script code. load_all() simulates the full blown process for seeing the effect of a source code change, which is clunky enough 3 that you won’t want to do it very often. Figure 5.4 reinforces that the library() function can only load a package that has been installed, whereas load_all() gives a high-fidelity simulation of this, based on the current package source.\n\n\n\n\nFigure 5.4: devtools::load_all() vs. library().\n\n\n\n\nThe main benefits of load_all() include:\n\nYou can iterate quickly, which encourages exploration and incremental progress.\n\nThis iterative speedup is especially noticeable for packages with compiled code.\n\n\nYou get to develop interactively under a namespace regime that accurately mimics how things are when someone uses your installed package, with the following additional advantages:\n\nYou can call your own internal functions directly, without using ::: and without being tempted to temporarily define your functions in the global workspace.\nYou can also call functions from other packages that you’ve imported into your NAMESPACE, without being tempted to attach these dependencies via library().\n\n\n\nload_all() removes friction from the development workflow and eliminates the temptation to use workarounds that often lead to mistakes around namespace and dependency management.\n\n5.4.2 Other ways to call load_all()\n\nWhen working in a Project that is a package, RStudio offers several ways to call load_all():\n\nKeyboard shortcut: Cmd+Shift+L (macOS), Ctrl+Shift+L (Windows, Linux)\nBuild pane’s More … menu\nBuild > Load All\n\ndevtools::load_all() is a thin wrapper around pkgload::load_all() that adds a bit of user-friendliness. It is unlikely you will use load_all() programmatically or inside another package, but if you do, you should probably use pkgload::load_all() directly."
  },
  {
    "objectID": "workflow101.html#sec-workflow101-r-cmd-check",
    "href": "workflow101.html#sec-workflow101-r-cmd-check",
    "title": "5  Fundamental development workflows",
    "section": "\n5.5 check() and R CMD check\n",
    "text": "5.5 check() and R CMD check\n\nBase R provides various command line tools and R CMD check is the official method for checking that an R package is valid. It is essential to pass R CMD check if you plan to submit your package to CRAN, but we highly recommend holding yourself to this standard even if you don’t intend to release your package on CRAN. R CMD check detects many common problems that you’d otherwise discover the hard way.\nOur recommended way to run R CMD check is in the R console via devtools:\n\ndevtools::check()\n\nWe recommend this because it allows you to run R CMD check from within R, which dramatically reduces friction and increases the likelihood that you will check() early and often! This emphasis on fluidity and fast feedback is exactly the same motivation as given for load_all(). In the case of check(), it really is executing R CMD check for you. It’s not just a high fidelity simulation, which is the case for load_all().\n\n\n\n\n\n\nRStudio\n\n\n\nRStudio exposes check() in the Build menu, in the Build pane via Check, and in keyboard shortcuts Ctrl + Shift + E (Windows & Linux) or Cmd + Shift + E (macOS).\n\n\nA rookie mistake that we see often in new package developers is to do too much work on their package before running R CMD check. Then, when they do finally run it, it’s typical to discover many problems, which can be very demoralizing. It’s counter-intuitive but the key to minimizing this pain is to run R CMD check more often: the sooner you find a problem, the easier it is to fix4. We model this behaviour very intentionally in Chapter 2.\nThe upper limit of this approach is to run R CMD check every time you make a change. We don’t run check() manually quite that often, but when we’re actively working on a package, it’s typical to check() multiple times per day. Don’t tinker with your package for days, weeks, or months, waiting for some special milestone to finally run R CMD check. If you use GitHub (Section 21.1), we’ll show you how to set things up so that R CMD check runs automatically every time you push (Section 21.2.1).\n\n5.5.1 Workflow\nHere’s what happens inside devtools::check():\n\nEnsures that the documentation is up-to-date by running devtools::document().\nBundles the package before checking it (Section 4.3). This is the best practice for checking packages because it makes sure the check starts with a clean slate: because a package bundle doesn’t contain any of the temporary files that can accumulate in your source package, e.g. artifacts like .so and .o files which accompany compiled code, you can avoid the spurious warnings such files will generate.\nSets the NOT_CRAN environment variable to \"true\". This allows you to selectively skip tests on CRAN. See ?testthat::skip_on_cran and Section 16.4.1 for details.\n\nThe workflow for checking a package is simple, but tedious:\n\nRun devtools::check(), or press Ctrl/Cmd + Shift + E.\nFix the first problem.\nRepeat until there are no more problems.\n\nR CMD check returns three types of messages:\n\nERRORs: Severe problems that you should fix regardless of whether or not you’re submitting to CRAN.\nWARNINGs: Likely problems that you must fix if you’re planning to submit to CRAN (and a good idea to look into even if you’re not).\nNOTEs: Mild problems or, in a few cases, just an observation. If you are submitting to CRAN, you should strive to eliminate all NOTEs, even if they are false positives. If you have no NOTEs, human intervention is not required, and the package submission process will be easier. If it’s not possible to eliminate a NOTE, you’ll need describe why it’s OK in your submission comments, as described in Section 23.7. If you’re not submitting to CRAN, carefully read each NOTE. If it’s easy to eliminate the NOTEs, it’s worth it, so that you can continue to strive for a totally clean result. But if eliminating a NOTE will have a net negative impact on your package, it is reasonable to just tolerate it. Make sure that doesn’t lead to you ignoring other issues that really should be addressed.\n\nR CMD check consists of dozens of individual checks and it would be overwhelming to enumerate them here. See our online-only guide to R CMD check for details.\n\n5.5.2 Background on R CMD check\n\nAs you accumulate package development experience, you might want to access R CMD check directly at some point. Remember that R CMD check is something you must run in the terminal, not in the R console. You can see its documentation like so:\nR CMD check --help\nR CMD check can be run on a directory that holds an R package in source form (Section 4.2) or, preferably, on a package bundle (Section 4.3):\nR CMD build somepackage\nR CMD check somepackage_0.0.0.9000.tar.gz  \nTo learn more, see the Checking packages section of Writing R Extensions.\n\n\n\n\nSilge, Julia, John C. Nash, and Spencer Graves. 2018. “Navigating the R Package Universe.” The R Journal 10 (2): 558–63. https://doi.org/10.32614/RJ-2018-058."
  },
  {
    "objectID": "package-within.html#alfa-a-script-that-works",
    "href": "package-within.html#alfa-a-script-that-works",
    "title": "6  The package within",
    "section": "\n6.1 Alfa: a script that works",
    "text": "6.1 Alfa: a script that works\n\nLet’s consider data-cleaning.R, a fictional data analysis script for a group that collects reports from people who went for a swim:\n\nWhere did you swim and how hot was it outside?\n\nTheir data usually comes as a CSV file, such as swim.csv:\n\nname,where,temp\nAdam,beach,95\nBess,coast,91\nCora,seashore,28\nDale,beach,85\nEvan,seaside,31\n\ndata-cleaning.R begins by reading swim.csv into a data frame:\n\ninfile <- \"swim.csv\"\n(dat <- read.csv(infile))\n\n\n#>   name    where temp\n#> 1 Adam    beach   95\n#> 2 Bess    coast   91\n#> 3 Cora seashore   28\n#> 4 Dale    beach   85\n#> 5 Evan  seaside   31\n\nThey then classify each observation as using American (“US”) or British (“UK”) English, based on the word chosen to describe the sandy place where the ocean and land meet. The where column is used to build the new english column.\n\ndat$english[dat$where == \"beach\"] <- \"US\"\ndat$english[dat$where == \"coast\"] <- \"US\"\ndat$english[dat$where == \"seashore\"] <- \"UK\"\ndat$english[dat$where == \"seaside\"] <- \"UK\"\n\nSadly, the temperatures are often reported in a mix of Fahrenheit and Celsius. In the absence of better information, they guess that Americans report temperatures in Fahrenheit and therefore those observations are converted to Celsius.\n\ndat$temp[dat$english == \"US\"] <- (dat$temp[dat$english == \"US\"] - 32) * 5/9\ndat\n#>   name    where temp english\n#> 1 Adam    beach 35.0      US\n#> 2 Bess    coast 32.8      US\n#> 3 Cora seashore 28.0      UK\n#> 4 Dale    beach 29.4      US\n#> 5 Evan  seaside 31.0      UK\n\nFinally, this cleaned (cleaner?) data is written back out to a CSV file. They like to capture a timestamp in the filename when they do this1.\n\nnow <- Sys.time()\ntimestamp <- format(now, \"%Y-%B-%d_%H-%M-%S\")\n(outfile <- paste0(timestamp, \"_\", sub(\"(.*)([.]csv$)\", \"\\\\1_clean\\\\2\", infile)))\n#> [1] \"2023-April-15_07-16-46_swim_clean.csv\"\nwrite.csv(dat, file = outfile, quote = FALSE, row.names = FALSE)\n\nHere is data-cleaning.R in its entirety:\n\n\ninfile <- \"swim.csv\"\n(dat <- read.csv(infile))\n\ndat$english[dat$where == \"beach\"] <- \"US\"\ndat$english[dat$where == \"coast\"] <- \"US\"\ndat$english[dat$where == \"seashore\"] <- \"UK\"\ndat$english[dat$where == \"seaside\"] <- \"UK\"\n\ndat$temp[dat$english == \"US\"] <- (dat$temp[dat$english == \"US\"] - 32) * 5/9\ndat\n\nnow <- Sys.time()\ntimestamp <- format(now, \"%Y-%B-%d_%H-%M-%S\")\n(outfile <- paste0(timestamp, \"_\", sub(\"(.*)([.]csv$)\", \"\\\\1_clean\\\\2\", infile)))\nwrite.csv(dat, file = outfile, quote = FALSE, row.names = FALSE)\n\nEven if your typical analytical tasks are quite different, hopefully you see a few familiar patterns here. It’s easy to imagine that this group does very similar pre-processing of many similar data files over time. Their analyses can be more efficient and consistent if they make these standard data maneuvers available to themselves as functions in a package, instead of inlining the same data and logic into dozens or hundreds of data ingest scripts."
  },
  {
    "objectID": "package-within.html#bravo-a-better-script-that-works",
    "href": "package-within.html#bravo-a-better-script-that-works",
    "title": "6  The package within",
    "section": "\n6.2 Bravo: a better script that works",
    "text": "6.2 Bravo: a better script that works\nThe package that lurks within the original script is actually pretty hard to see! It’s obscured by a few suboptimal coding practices, such as the use of repetitive copy/paste-style code and the mixing of code and data. Therefore a good first step is to refactor this code, isolating as much data and logic as possible in proper objects and functions, respectively.\nThis is also a good time to introduce the use of some add-on packages, for several reasons. First, we would actually use the tidyverse for this sort of data wrangling. Second, many people use add-on packages in their scripts, so it is good to see how add-on packages are handled inside a package.\nHere’s the new and improved version of the script.\n\nlibrary(tidyverse)\n\ninfile <- \"swim.csv\"\ndat <- read_csv(infile, col_types = cols(name = \"c\", where = \"c\", temp = \"d\"))\n\nlookup_table <- tribble(\n      ~where, ~english,\n     \"beach\",     \"US\",\n     \"coast\",     \"US\",\n  \"seashore\",     \"UK\",\n   \"seaside\",     \"UK\"\n)\n\ndat <- dat %>% \n  left_join(lookup_table)\n\nf_to_c <- function(x) (x - 32) * 5/9\n\ndat <- dat %>% \n  mutate(temp = if_else(english == \"US\", f_to_c(temp), temp))\ndat\n\nnow <- Sys.time()\ntimestamp <- function(time) format(time, \"%Y-%B-%d_%H-%M-%S\")\noutfile_path <- function(infile) {\n  paste0(timestamp(now), \"_\", sub(\"(.*)([.]csv$)\", \"\\\\1_clean\\\\2\", infile))\n}\nwrite_csv(dat, outfile_path(infile))\n\nThe key changes to note are:\n\nWe are using functions from tidyverse packages (specifically from readr and dplyr) and we make them available with library(tidyverse).\nThe map between different “beach” words and whether they are considered to be US or UK English is now isolated in a lookup table, which lets us create the english column in one go with a left_join(). This lookup table makes the mapping easier to comprehend and would be much easier to extend in the future with new “beach” words.\n\nf_to_c(), timestamp(), and outfile_path() are new helper functions that hold the logic for converting temperatures and forming the timestamped output file name.\n\nIt’s getting easier to recognize the reusable bits of this script, i.e. the bits that have nothing to do with a specific input file, like swim.csv. This sort of refactoring often happens naturally on the way to creating your own package, but if it does not, it’s a good idea to do this intentionally."
  },
  {
    "objectID": "package-within.html#charlie-a-separate-file-for-helper-functions",
    "href": "package-within.html#charlie-a-separate-file-for-helper-functions",
    "title": "6  The package within",
    "section": "\n6.3 Charlie: a separate file for helper functions",
    "text": "6.3 Charlie: a separate file for helper functions\nA typical next step is to move reusable data and logic out of the analysis script and into one or more separate files. This is a conventional opening move, if you want to use these same helper files in multiple analyses.\nHere is the content of beach-lookup-table.csv:\n\nwhere,english\nbeach,US\ncoast,US\nseashore,UK\nseaside,UK\n\nHere is the content of cleaning-helpers.R:\n\nlibrary(tidyverse)\n\nlocalize_beach <- function(dat) {\n  lookup_table <- read_csv(\n    \"beach-lookup-table.csv\",\n    col_types = cols(where = \"c\", english = \"c\")\n  )\n  left_join(dat, lookup_table)\n}\n\nf_to_c <- function(x) (x - 32) * 5/9\n\ncelsify_temp <- function(dat) {\n  mutate(dat, temp = if_else(english == \"US\", f_to_c(temp), temp))\n}\n\nnow <- Sys.time()\ntimestamp <- function(time) format(time, \"%Y-%B-%d_%H-%M-%S\")\noutfile_path <- function(infile) {\n  paste0(timestamp(now), \"_\", sub(\"(.*)([.]csv$)\", \"\\\\1_clean\\\\2\", infile))\n}\n\nWe’ve added some high-level helper functions, localize_beach() and celsify_temp(), to the pre-existing helpers (f_to_c(), timestamp(), and outfile_path()).\nHere is the next version of the data cleaning script, now that we’ve pulled out the helper functions (and lookup table).\n\nlibrary(tidyverse)\nsource(\"cleaning-helpers.R\")\n\ninfile <- \"swim.csv\"\ndat <- read_csv(infile, col_types = cols(name = \"c\", where = \"c\", temp = \"d\"))\n\n(dat <- dat %>% \n    localize_beach() %>% \n    celsify_temp())\n\nwrite_csv(dat, outfile_path(infile))\n\nNotice that the script is getting shorter and, hopefully, easier to read and modify, because repetitive and fussy clutter has been moved out of sight. Whether the code is actually easier to work with is subjective and depends on how natural the “interface” feels for the people who actually preprocess swimming data. These sorts of design decisions are the subject of a separate project: design.tidyverse.org.\nLet’s assume the group agrees that our design decisions are promising, i.e. we seem to be making things better, not worse. Sure, the existing code is not perfect, but this is a typical developmental stage when you’re trying to figure out what the helper functions should be and how they should work."
  },
  {
    "objectID": "package-within.html#delta-a-failed-attempt-at-making-a-package",
    "href": "package-within.html#delta-a-failed-attempt-at-making-a-package",
    "title": "6  The package within",
    "section": "\n6.4 Delta: a failed attempt at making a package",
    "text": "6.4 Delta: a failed attempt at making a package\nWhile this first attempt to create a package will end in failure, it’s still helpful to go through some common missteps, to illuminate what happens behind the scenes.\nHere are the simplest steps that you might take, in an attempt to convert cleaning-helpers.R into a proper package:\n\nUse usethis::create_package(\"path/to/delta\") to scaffold a new R package, with the name “delta”.\n\nThis is a good first step!\n\n\nCopy cleaning-helpers.R into the new package, specifically, to R/cleaning-helpers.R.\n\nThis is morally correct, but mechanically wrong in several ways, as we will soon see.\n\n\nCopy beach-lookup-table.csv into the new package. But where? Let’s try the top-level of the source package.\n\nThis is not going to end well. Shipping data files in a package is a special topic, which is covered in Chapter 8.\n\n\nInstall this package, perhaps using devtools::install() or via Ctrl + Shift + B (Windows & Linux) or Cmd + Shift + B in RStudio.\n\nDespite all of the problems identified above, this actually works! Which is interesting, because we can (try to) use it and see what happens.\n\n\n\nHere is the next version of the data cleaning script that you hope will run after successfully installing this package (which we’re calling “delta”).\n\nlibrary(tidyverse)\nlibrary(delta)\n\ninfile <- \"swim.csv\"\ndat <- read_csv(infile, col_types = cols(name = \"c\", where = \"c\", temp = \"d\"))\n\ndat <- dat %>% \n  localize_beach() %>% \n  celsify_temp()\n\nwrite_csv(dat, outfile_path(infile))\n\nThe only change from our previous script is that\n\nsource(\"cleaning-helpers.R\")\n\nhas been replaced by\n\nlibrary(delta)\n\nHere’s what actually happens if you install the delta package and try to run the data cleaning script:\n\nlibrary(tidyverse)\nlibrary(delta)\n\ninfile <- \"swim.csv\"\ndat <- read_csv(infile, col_types = cols(name = \"c\", where = \"c\", temp = \"d\"))\n\ndat <- dat %>% \n  localize_beach() %>% \n  celsify_temp()\n#> Error in localize_beach(.) : could not find function \"localize_beach\"\n\nwrite_csv(dat, outfile_path(infile))\n#> Error in outfile_path(infile) : could not find function \"outfile_path\"\n\nNone of the helper functions are actually available for use, even though you call library(delta)! In contrast to source()ing a file of helper functions, attaching a package does not dump its functions into the global workspace. By default, functions in a package are only for internal use. You need to export localize_beach(), celsify_temp(), and outfile_path() so your users can call them. In the devtools workflow, we achieve this by putting @export in the special roxygen comment above each function (namespace management is covered in Section 12.3), like so:\n\n#' @export\ncelsify_temp <- function(dat) {\n  mutate(dat, temp = if_else(english == \"US\", f_to_c(temp), temp))\n}\n\nAfter you add the @export tag to localize_beach(), celsify_temp(), and outfile_path(), you run devtools::document() to (re)generate the NAMESPACE file, and re-install the delta package. Now when you re-execute the data cleaning script, it works!\nCorrection: it sort of works sometimes. Specifically, it works if and only if the working directory is set to the top-level of the source package. From any other working directory, you still get an error:\n\ndat <- dat %>% \n  localize_beach() %>% \n  celsify_temp()\n#> Error: 'beach-lookup-table.csv' does not exist in current working directory ('/Users/jenny/tmp').\n\nThe lookup table consulted inside localize_beach() cannot be found. One does not simply dump CSV files into the source of an R package and expect things to “just work”. We will fix this in our next iteration of the package (Chapter 8 has full coverage of how to include data in a package).\nBefore we abandon this initial experiment, let’s also marvel at the fact that you were able to install, attach, and, to a certain extent, use a fundamentally broken package. devtools::load_all() works fine, too! This is a sobering reminder that you should be running R CMD check, probably via devtools::check(), very often during development. This will quickly alert you to many problems that simple installation and usage does not reveal.\nIndeed, check() fails for this package and you see this:\n * installing *source* package ‘delta’ ...\n ** using staged installation\n ** R\n ** byte-compile and prepare package for lazy loading\n Error in library(tidyverse) : there is no package called ‘tidyverse’\n Error: unable to load R code in package ‘delta’\n Execution halted\n ERROR: lazy loading failed for package ‘delta’\n * removing ‘/Users/jenny/rrr/delta.Rcheck/delta’\nWhat do you mean “there is no package called ‘tidyverse’”?!? We’re using it, with no problems, in our main script! Also, we’ve already installed and used this package, why can’t R CMD check find it?\nThis error is what happens when the strictness of R CMD check meets the very first line of R/cleaning-helpers.R:\n\nlibrary(tidyverse)\n\nThis is not how you declare that your package depends on another package (the tidyverse, in this case). This is also not how you make functions in another package available for use in yours. Dependencies must be declared in DESCRIPTION (and that’s not all). Since we declared no dependencies, R CMD check takes us at our word and tries to install our package with only the base packages available, which means this library(tidyverse) call fails. A “regular” installation succeeds, simply because the tidyverse is available in your regular library, which hides this particular mistake.\nTo review, copying cleaning-helpers.R to R/cleaning-helpers.R, without further modification, was problematic in (at least) the following ways:\n\nDoes not account for exported vs. non-exported functions.\nThe CSV file holding our lookup table cannot be found in the installed package.\nDoes not properly declare our dependency on other add-on packages."
  },
  {
    "objectID": "package-within.html#echo-a-working-package",
    "href": "package-within.html#echo-a-working-package",
    "title": "6  The package within",
    "section": "\n6.5 Echo: a working package",
    "text": "6.5 Echo: a working package\nWe’re ready to make the most minimal version of this package that actually works.\nHere is the new version of R/cleaning-helpers.R2:\n\nlookup_table <- dplyr::tribble(\n      ~where, ~english,\n     \"beach\",     \"US\",\n     \"coast\",     \"US\",\n  \"seashore\",     \"UK\",\n   \"seaside\",     \"UK\"\n)\n\n#' @export\nlocalize_beach <- function(dat) {\n  dplyr::left_join(dat, lookup_table)\n}\n\nf_to_c <- function(x) (x - 32) * 5/9\n\n#' @export\ncelsify_temp <- function(dat) {\n  dplyr::mutate(dat, temp = dplyr::if_else(english == \"US\", f_to_c(temp), temp))\n}\n\nnow <- Sys.time()\ntimestamp <- function(time) format(time, \"%Y-%B-%d_%H-%M-%S\")\n\n#' @export\noutfile_path <- function(infile) {\n  paste0(timestamp(now), \"_\", sub(\"(.*)([.]csv$)\", \"\\\\1_clean\\\\2\", infile))\n}\n\nWe’ve gone back to defining the lookup_table with R code, since the initial attempt to read it from CSV created some sort of filepath snafu. This is OK for small, internal, static data, but remember to see Chapter 8 for more general techniques for storing data in a package.\nAll of the calls to tidyverse functions have now been qualified with the name of the specific package that actually provides the function, e.g. dplyr::mutate(). There are other ways to access functions in another package, explained in Section 12.4, but this is our recommended default. It is also our strong recommendation that no one depend on the tidyverse meta-package in a package3. Instead, it is better to identify the specific package(s) you actually use. In this case, the package only uses dplyr.\nThe library(tidyverse) call is gone and instead we declare the use of dplyr in the Imports field of DESCRIPTION:\nPackage: echo\n(... other lines omitted ...)\nImports: \n    dplyr\nThis, together with the use of namespace-qualified calls, like dplyr::left_join(), constitutes a valid way to use another package within yours. The metadata conveyed via DESCRIPTION is covered in Chapter 10.\nAll of the user-facing functions have an @export tag in their roxygen comment, which means that devtools::document() adds them correctly to the NAMESPACE file. Note that f_to_c() is currently only used internally, inside celsify_temp(), so it is not exported (likewise for timestamp()).\nThis version of the package can be installed, used, AND it technically passes R CMD check, though with 1 warning and 1 note.\n* checking for missing documentation entries ... WARNING\nUndocumented code objects:\n  ‘celsify_temp’ ‘localize_beach’ ‘outfile_path’\nAll user-level objects in a package should have documentation entries.\nSee chapter ‘Writing R documentation files’ in the ‘Writing R\nExtensions’ manual.\n\n* checking R code for possible problems ... NOTE\ncelsify_temp: no visible binding for global variable ‘english’\ncelsify_temp: no visible binding for global variable ‘temp’\nUndefined global functions or variables:\n  english temp\nThe “no visible binding” note is a peculiarity of using dplyr and unquoted variable names inside a package, where the use of bare variable names (english and temp) looks suspicious. You can add either of these lines to any file below R/ to eliminate this note (such as the package-level documentation file described in Section 17.7):\n\n# option 1 (then you should also put utils in Imports)\nutils::globalVariables(c(\"english\", \"temp\"))\n\n# option 2\nenglish <- temp <- NULL\n\nWe’re seeing that it can be tricky to program around a package like dplyr, which makes heavy use of nonstandard evaluation. Behind the scenes, that is the technique that allows dplyr’s end users to use bare (not quoted) variable names. Packages like dplyr prioritize the experience of the typical end user, at the expense of making them trickier to depend on. The two options shown above for suppressing the “no visible binding” note, represent entry-level solutions. For a more sophisticated treatment of these issues, see vignette(\"in-packages\", package = \"dplyr\") and vignette(\"programming\", package = \"dplyr\").\nThe warning about missing documentation is because the exported functions have not been properly documented. This is a valid concern and something you should absolutely address in a real package. You’ve already seen how to create help files with roxygen comments in Section 2.12 and we cover this topic thoroughly in Chapter 17."
  },
  {
    "objectID": "package-within.html#sec-package-within-build-time-run-time",
    "href": "package-within.html#sec-package-within-build-time-run-time",
    "title": "6  The package within",
    "section": "\n6.6 Foxtrot: build time vs. run time",
    "text": "6.6 Foxtrot: build time vs. run time\nThe echo package works, which is great, but group members notice something odd about the timestamps:\n\nSys.time()\n#> [1] \"2023-03-26 22:48:48 PDT\"\n\noutfile_path(\"INFILE.csv\")\n#> [1] \"2020-September-03_11-06-33_INFILE_clean.csv\"\n\nThe datetime in the timestamped filename doesn’t reflect the time reported by the system. In fact, the users claim that the timestamp never seems to change at all! Why is this?\nRecall how we form the filepath for output files:\n\nnow <- Sys.time()\ntimestamp <- function(time) format(time, \"%Y-%B-%d_%H-%M-%S\")\noutfile_path <- function(infile) {\n  paste0(timestamp(now), \"_\", sub(\"(.*)([.]csv$)\", \"\\\\1_clean\\\\2\", infile))\n}\n\nThe fact that we capture now <- Sys.time() outside of the definition of outfile_path() has probably been vexing some readers for a while. now reflects the instant in time when we execute now <- Sys.time(). In the initial approach, now was assigned when we source()d cleaning-helpers.R. That’s not ideal, but it was probably a pretty harmless mistake, because the helper file would be source()d shortly before we wrote the output file.\nBut this approach is quite devastating in the context of a package. now <- Sys.time() is executed when the package is built4. And never again. It is very easy to assume your package code is re-evaluated when the package is attached or used. But it is not. Yes, the code inside your functions is absolutely run whenever they are called. But your functions – and any other objects created in top-level code below R/ – are defined exactly once, at build time.\nBy defining now with top-level code below R/, we’ve doomed our package to timestamp all of its output files with the same (wrong) time. The fix is to make sure the Sys.time() call happens at run time.\nLet’s look again at parts of R/cleaning-helpers.R:\n\nlookup_table <- dplyr::tribble(\n      ~where, ~english,\n     \"beach\",     \"US\",\n     \"coast\",     \"US\",\n  \"seashore\",     \"UK\",\n   \"seaside\",     \"UK\"\n)\n\nnow <- Sys.time()\ntimestamp <- function(time) format(time, \"%Y-%B-%d_%H-%M-%S\")\noutfile_path <- function(infile) {\n  paste0(timestamp(now), \"_\", sub(\"(.*)([.]csv$)\", \"\\\\1_clean\\\\2\", infile))\n}\n\nThere are four top-level <- assignments in this excerpt. The top-level definitions of the data frame lookup_table and the functions timestamp() and outfile_path() are correct. It is appropriate that these be defined exactly once, at build time. The top-level definition of now, which is then used inside outfile_path(), is regrettable.\nHere are better versions of outfile_path():\n\n# always timestamp as \"now\"\noutfile_path <- function(infile) {\n  ts <- timestamp(Sys.time())\n  paste0(ts, \"_\", sub(\"(.*)([.]csv$)\", \"\\\\1_clean\\\\2\", infile))\n}\n\n# allow user to provide a time, but default to \"now\"\noutfile_path <- function(infile, time = Sys.time()) {\n  ts <- timestamp(time)\n  paste0(ts, \"_\", sub(\"(.*)([.]csv$)\", \"\\\\1_clean\\\\2\", infile))\n}\n\nThis illustrates that you need to have a different mindset when defining objects inside a package. The vast majority of those objects should be functions and these functions should generally only use data they create or that is passed via an argument. There are some types of sloppiness that are fairly harmless when a function is defined immediately before its use, but that can be more costly for functions distributed as a package."
  },
  {
    "objectID": "package-within.html#sec-package-within-side-effects",
    "href": "package-within.html#sec-package-within-side-effects",
    "title": "6  The package within",
    "section": "\n6.7 Golf: side effects",
    "text": "6.7 Golf: side effects\nThe timestamps now reflect the current time, but the group raises a new concern. As it stands, the timestamps reflect who has done the data cleaning and which part of the world they’re in. The heart of the timestamp strategy is this format string5:\n\nformat(Sys.time(), \"%Y-%B-%d_%H-%M-%S\")\n#> [1] \"2023-April-15_07-16-47\"\n\nThis formats Sys.time() in such a way that it includes the month name (not number) and the local time6.\nTable 6.1 shows what happens when such a timestamp is produced by several hypothetical colleagues cleaning some data at exactly the same instant in time.\n\n\n\n\nTable 6.1: Timestamp varies by locale and timezone.\n\n\n\n\n\n\n\nlocation\ntimestamp\nLC_TIME\ntz\n\n\n\nRome, Italy\n2020-settembre-05_00-30-00\nit_IT.UTF-8\nEurope/Rome\n\n\nWarsaw, Poland\n2020-września-05_00-30-00\npl_PL.UTF-8\nEurope/Warsaw\n\n\nSao Paulo, Brazil\n2020-setembro-04_19-30-00\npt_BR.UTF-8\nAmerica/Sao_Paulo\n\n\nGreenwich, England\n2020-September-04_23-30-00\nen_GB.UTF-8\nEurope/London\n\n\n“Computer World!”\n2020-September-04_22-30-00\nC\nUTC\n\n\n\n\n\n\nNote that the month names vary, as does the time, and even the date! The safest choice is to form timestamps with respect to a fixed locale and time zone (presumably the non-geographic choices represented by “Computer World!” above).\nYou do some research and learn that you can force a certain locale via Sys.setlocale() and force a certain time zone by setting the TZ environment variable. Specifically, we set the LC_TIME component of the locale to “C” and the time zone to “UTC” (Coordinated Universal Time). Here’s your first attempt to improve timestamp():\n\ntimestamp <- function(time = Sys.time()) {\n  Sys.setlocale(\"LC_TIME\", \"C\")\n  Sys.setenv(TZ = \"UTC\")\n  format(time, \"%Y-%B-%d_%H-%M-%S\")\n}\n\nBut your Brazilian colleague notices that datetimes print differently, before and after she uses outfile_path() from your package:\nBefore:\n\nformat(Sys.time(), \"%Y-%B-%d_%H-%M-%S\")\n\n\n#> [1] \"2023-abril-15_04-16-47\"\n\nAfter:\n\noutfile_path(\"INFILE.csv\")\n#> [1] \"2023-April-15_07-16-47_INFILE_clean.csv\"\n\nformat(Sys.time(), \"%Y-%B-%d_%H-%M-%S\")\n#> [1] \"2023-April-15_07-16-47\"\n\nNotice that her month name switched from Portuguese to English and the time is clearly being reported in a different time zone. The calls to Sys.setlocale() and Sys.setenv() inside timestamp() have made persistent (and very surprising) changes to her R session. This sort of side effect is very undesirable and is extremely difficult to track down and debug, especially in more complicated settings.\nHere are better versions of timestamp():\n\n# use withr::local_*() functions to keep the changes local to timestamp()\ntimestamp <- function(time = Sys.time()) {\n  withr::local_locale(c(\"LC_TIME\" = \"C\"))\n  withr::local_timezone(\"UTC\")\n  format(time, \"%Y-%B-%d_%H-%M-%S\")\n}\n\n# use the tz argument to format.POSIXct()\ntimestamp <- function(time = Sys.time()) {\n  withr::local_locale(c(\"LC_TIME\" = \"C\"))\n  format(time, \"%Y-%B-%d_%H-%M-%S\", tz = \"UTC\")\n}\n\n# put the format() call inside withr::with_*()\ntimestamp <- function(time = Sys.time()) {\n  withr::with_locale(\n    c(\"LC_TIME\" = \"C\"),\n    format(time, \"%Y-%B-%d_%H-%M-%S\", tz = \"UTC\")\n  )\n}\n\nThese show various methods to limit the scope of our changes to LC_TIME and the timezone. A good rule of thumb is to make the scope of such changes as narrow as is possible and practical. The tz argument of format() is the most surgical way to deal with the timezone, but nothing similar exists for LC_TIME. We make the temporary locale modification using the withr package, which provides a very flexible toolkit for temporary state changes. This (and base::on.exit()) are discussed further in Section 7.5. Note that if you use withr as we do above, you would need to list it in DESCRIPTION in Imports (Chapter 12, Section 11.1.3).\nThis underscores a point from the previous section: you need to adopt a different mindset when defining functions inside a package. Try to avoid making any changes to the user’s overall state. If such changes are unavoidable, make sure to reverse them (if possible) or to document them explicitly (if related to the function’s primary purpose)."
  },
  {
    "objectID": "package-within.html#concluding-thoughts",
    "href": "package-within.html#concluding-thoughts",
    "title": "6  The package within",
    "section": "\n6.8 Concluding thoughts",
    "text": "6.8 Concluding thoughts\nFinally, after several iterations, we have successfully extracted the repetitive data cleaning code for the swimming survey into an R package. This example concludes the first part of book and marks the transition into more detailed reference material on specific package components. Before we move on, let’s review the lessons learned in this chapter.\n\n6.8.1 Script vs.package\nWhen you first hear that expert R users often put their code into packages, you might wonder exactly what that means. Specifically, what happens to your existing R scripts, R Markdown reports, and Shiny apps? Does all of that code somehow get put into a package? The answer is “no”, in most contexts.\nTypically, you identify certain recurring operations that occur across multiple projects and this is what you extract into an R package. You will still have R scripts, R Markdown reports, and Shiny apps, but by moving specific pieces of code into a formal package, your data products tend to become more concise and easier to maintain.\n\n6.8.2 Finding the package within\nAlthough the example in this chapter is rather simple, it still captures the typical process of developing an R package for personal or organizational use. You typically start with a collection of idiosyncratic and related R scripts, scattered across different projects. Over time, you begin to notice that certain needs come up over and over again.\nEach time you revisit a similar analysis, you might try to elevate your game a bit, compared to the previous iteration. You refactor copy/paste-style code using more robust patterns and start to encapsulate key “moves” in helper functions, which might eventually migrate into their own file. Once you reach this stage, you’re in a great position to take the next step and create a package.\n\n6.8.3 Package code is different\nWriting package code is a bit different from writing R scripts and it’s natural to feel some discomfort when making this adjustment. Here are the most common gotchas that trip many of us up at first:\n\nPackage code requires new ways of working with functions in other packages. The DESCRIPTION file is the principle way to declare dependencies; we don’t do this via library(somepackage).\nIf you want data or files to be persistently available, there are package-specific methods of storage and retrieval. You can’t just put files in the package and hope for the best.\nIt’s necessary to be explicit about which functions are user-facing and which are internal helpers. By default, functions are not exported for use by others.\nA new level of discipline is required to ensure that code runs at the intended time (build time vs. run time) and that there are no unintended side effects."
  },
  {
    "objectID": "code.html#sec-code-organising",
    "href": "code.html#sec-code-organising",
    "title": "7  R code",
    "section": "\n7.1 Organise functions into files",
    "text": "7.1 Organise functions into files\nThe only hard rule is that your package must store its function definitions in R scripts, i.e. files with extension .R, that live in the R/ directory1. However, a few more conventions can make the source code of your package easier to navigate and relieve you of re-answering “How should I name this?” each time you create a new file. The Tidyverse Style Guide offers some general advice about file names and also advice that specifically applies to files in a package. We expand on this here.\nThe file name should be meaningful and convey which functions are defined within. While you’re free to arrange functions into files as you wish, the two extremes are bad: don’t put all functions into one file and don’t put each function into its own separate file. This advice should inform your general policy, but there are exceptions to every rule. If a specific function is very large or has lots of documentation, it can make sense to give it its own file, named after the function. More often, a single .R file will contain multiple function definitions: such as a main function and its supporting helpers, a family of related functions, or some combination of the two.\nTable 7.1 presents some examples from the actual source of the tidyr package at version 1.1.2. There are some departures from the hard-and-fast rules given above, which illustrates that there’s a lot of room for judgment here.\n\n\n\n\nTable 7.1: Different ways to organize functions in files.\n\nOrganising principle\nSource file\nComments\n\n\n\nOne function\ntidyr/R/uncount.R\nDefines exactly one function, uncount(), that’s not particulary large, but doesn’t fit naturally into any other .R file\n\n\nMain function plus helpers\ntidyr/R/separate.R\nDefines the user-facing separate() (an S3 generic), a data.frame method, and private helpers\n\n\nFamily of functions\ntidyr/R/rectangle.R\nDefines a family of functions for “rectangling” nested lists (hoist() and the unnest() functions), all documented together in a big help topic, plus private helpers\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nAnother file you often see in the wild is R/utils.R. This is a common place to define small utilities that are used inside multiple package functions. Since they serve as helpers to multiple functions, placing them in R/utils.R makes them easier to re-discover when you return to your package after a long break.\nBob Rudis assembled a collection of such files and did some analysis in the post Dissecting R Package “Utility Belts”.\n\n\nIf it’s very hard to predict which file a function lives in, that suggests it’s time to separate your functions into more files or reconsider how you are naming your functions and/or files.\n\n\n\n\n\n\nRStudio\n\n\n\nThe organisation of functions within files is less important in RStudio, which offers two ways to jump to the definition of a function:\n\n\nPress Ctrl + . (the period) to bring up the Go to File/Function tool, as shown in Figure 7.1, then start typing the name. Keep typing to narrow the list and eventually pick a function (or file) to visit. This works for both functions and files in your project.\n\n\n\n\nFigure 7.1: Go to File/Function in RStudio.\n\n\n\n\n\nWith your cursor in a function name or with a function name selected, press F2. This works for functions defined in your package or in another package.\n\nAfter navigating to a function with one of these methods, return to where you started by clicking the back arrow at the top-left of the editor () or by pressing Ctrl + F9 (Windows & Linux) or Cmd + F9 (macOS)."
  },
  {
    "objectID": "code.html#sec-code-load-all",
    "href": "code.html#sec-code-load-all",
    "title": "7  R code",
    "section": "\n7.2 Fast feedback via load_all()\n",
    "text": "7.2 Fast feedback via load_all()\n\nAs you add or modify functions defined in files below R/, you will naturally want to try them out. We want to reiterate our strong recommendation to use devtools::load_all() to make them available for interactive exploration instead of, for example, source()ing files below R/. The main coverage of load_all() is in Section 5.4 and load_all() also shows up as one of the natural development tasks in Section 2.8. The importance of load_all() in the testthat workflow is explained in Section 15.2.5. Compared to the alternatives, load_all() helps you to iterate more quickly and provides an excellent approximation to the namespace regime of an installed package."
  },
  {
    "objectID": "code.html#code-style",
    "href": "code.html#code-style",
    "title": "7  R code",
    "section": "\n7.3 Code style",
    "text": "7.3 Code style\nWe recommend following the tidyverse style guide (https://style.tidyverse.org), which goes into much more detail than we can here. Its format also allows it to be a more dynamic document than this book.\nAlthough the style guide explains the “what” and the “why”, another important decision is how to enforce a specific code style. For this we recommend the styler package (https://styler.r-lib.org); its default behaviour enforces the tidyverse style guide. There are many ways to apply styler to your code, depending on the context:\n\n\nstyler::style_pkg() restyles an entire R package.\n\nstyler::style_dir() restyles all files in a directory.\n\nusethis::use_tidy_style() is wrapper that applies one of the above functions depending on whether the current project is an R package or not.\n\nstyler::style_file() restyles a single file.\n\nstyler::style_text() restyles a character vector.\n\n\n\n\n\n\n\nRStudio\n\n\n\nWhen styler is installed, the RStudio Addins menu will offer several additional ways to style code:\n\nthe active selection\nthe active file\nthe active package\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nIf you don’t use Git or another version control system, applying a function like styler::style_pkg() is nerve-wracking and somewhat dangerous, because you lack a way to see exactly what changed and to accept/reject such changes in a granular way.\n\n\nThe styler package can also be integrated with various platforms for hosting source code and doing continuous integration. For example, the tidyverse packages use a GitHub Action that restyles a package when triggered by a special comment (/style) on a pull request. This allows maintainers to focus on reviewing the substance of the pull request, without having to nitpick small issues of whitespace or indentation23 ."
  },
  {
    "objectID": "code.html#sec-code-when-executed",
    "href": "code.html#sec-code-when-executed",
    "title": "7  R code",
    "section": "\n7.4 Understand when code is executed",
    "text": "7.4 Understand when code is executed\nUp until now, you’ve probably been writing scripts, R code saved in a file that you execute interactively, perhaps using an IDE and/or source(), or noninteractively via Rscript. There are two main differences between code in scripts and packages:\n\nIn a script, code is run … when you run it! The awkwardness of this statement reflects that it’s hard to even think about this issue with a script. However, we must, in order to appreciate that the code in a package is run when the package is built. This has big implications for how you write the code below R/: package code should only create objects, the vast majority of which will be functions.\nFunctions in your package will be used in situations that you didn’t imagine. This means your functions need to be thoughtful in the way that they interact with the outside world.\n\nWe expand on the first point here and the second in the next section. These topics are also illustrated concretely in Section 6.6.\nWhen you source() a script, every line of code is executed and the results are immediately made available. Things are different with package code, because it is loaded in two steps. When the binary package is built (often, by CRAN) all the code in R/ is executed and the results are saved. When you attach a package with library(), these cached results are re-loaded and certain objects (mostly functions) are made available for your use. The full details on what it means for a package to be in binary form are given in Section 4.4. We refer to the creation of the binary package as (binary) “build time” and, specifically, we mean when R CMD INSTALL --build is run. (You might think that this is what R CMD build does, but that actually makes a bundled package, a.k.a. a “source tarball”.) For macOS and Windows users of CRAN packages, build time is whenever CRAN built the binary package for their OS. For those who install packages from source, build time is essentially when they (built and) installed the package.\nConsider the assignment x <- Sys.time(). If you put this in a script, x tells you when the script was source()d. But if you put that same code at the top-level in a package, x tells you when the package binary was built. In Section 6.6, we show a complete example of this in the context of forming timestamps inside a package.\nThe main takeaway is this:\n\nAny R code outside of a function is suspicious and should be carefully reviewed.\n\nWe explore a few real-world examples below that show how easy it is to get burned by this “build time vs. load time” issue. Luckily, once you diagnose this problem, it is generally not difficult to fix.\n\n7.4.1 Example: A path returned by system.file()\n\nThe shinybootstrap2 package once had this code below R/:\n\ndataTableDependency <- list(\n  htmlDependency(\n    \"datatables\", \"1.10.2\",\n    c(file = system.file(\"www/datatables\", package = \"shinybootstrap2\")),\n    script = \"js/jquery.dataTables.min.js\"\n  ),\n  htmlDependency(\n    \"datatables-bootstrap\", \"1.10.2\",\n    c(file = system.file(\"www/datatables\", package = \"shinybootstrap2\")),\n    stylesheet = c(\"css/dataTables.bootstrap.css\", \"css/dataTables.extra.css\"),\n    script = \"js/dataTables.bootstrap.js\"\n  )\n)\n\nSo dataTableDependency was a list object defined in top-level package code and its value was constructed from paths obtained via system.file(). As described in a GitHub issue,\n\nThis works fine when the package is built and tested on the same machine. However, if the package is built on one machine and then used on another (as is the case with CRAN binary packages), then this will fail – the dependency will point to the wrong directory on the host.\n\nThe heart of the solution is to make sure that system.file() is called from a function, at run time. Indeed, this fix was made here (in commit 138db47) and in a few other packages that had similar code and a related check was added in htmlDependency() itself. This particular problem would now be caught by R CMD check, due to changes that came with staged installation as of R 3.6.0.\n\n7.4.2 Example: Available colours\nThe crayon package has a function, crayon::show_ansi_colors(), that displays an ANSI colour table on your screen, basically to show what sort of styling is possible. In an early version, the function looked something like this:\n\nshow_ansi_colors <- function(colors = num_colors()) {\n  if (colors < 8) {\n    cat(\"Colors are not supported\")\n  } else if (colors < 256) {\n    cat(ansi_colors_8, sep = \"\")\n    invisible(ansi_colors_8)\n  } else {\n    cat(ansi_colors_256, sep = \"\")\n    invisible(ansi_colors_256)\n  }\n}\n\nansi_colors_8 <- # code to generate a vector covering basic terminal colors\n  \nansi_colors_256 <- # code to generate a vector covering 256 colors\n\nwhere ansi_colors_8 and ansi_colors_256 were character vectors exploring a certain set of colours, presumably styled via ANSI escapes.\nThe problem was those objects were formed and cached when the binary package was built. Since that often happens on a headless server, this likely happens under conditions where terminal colours might not be enabled or even available. Users of the installed package could still call show_ansi_colors() and num_colors() would detect the number of colours supported by their system (256 on most modern computers). But then an un-coloured object would print to screen (the original GitHub issue is r-lib/crayon#37).\nThe solution was to compute the display objects with a function at run time (in commit e2b368a:\n\nshow_ansi_colors <- function(colors = num_colors()) {\n  if (colors < 8) {\n    cat(\"Colors are not supported\")\n  } else if (colors < 256) {\n    cat(ansi_colors_8(), sep = \"\")\n    invisible(ansi_colors_8())\n  } else {\n    cat(ansi_colors_256(), sep = \"\")\n    invisible(ansi_colors_256())\n  }\n}\n\nansi_colors_8 <- function() {\n  # code to generate a vector covering basic terminal colors\n}\n  \nansi_colors_256 <- function() {\n  # code to generate a vector covering 256 colors\n}\n\nLiterally, the same code is used, it is simply pushed down into the body of a function taking no arguments (similar to the shinybootstrap2 example). Each reference to, e.g., the ansi_colors_8 object is replaced by a call to the ansi_colors_8() function.\nThe main takeaway is that functions that assess or expose the capabilities of your package on a user’s system must fully execute on your user’s system. It’s fairly easy to accidentally rely on results that were cached at build time, quite possibly on a different machine.\n\n7.4.3 Example: Aliasing a function\nOne last example shows that, even if you are careful to only define functions below R/, there are still some subtleties to consider. Imagine that you want the function foo() in your package to basically be an alias for the function blah() from some other package, e.g. pkgB. You might be tempted to do this:\n\nfoo <- pkgB::blah\n\nHowever, this will cause foo() in your package to reflect the definition of pkgB::blah() at the version present on the machine where the binary package is built (often CRAN), at that moment in time. If a bug is discovered in pkgB::blah() and subsequently fixed, your package will still use the older, buggy version, until your package is rebuilt (often by CRAN) and your users upgrade, which is completely out of your control. This alternative approach protects you from this:\n\nfoo <- function(...) pkgB::blah(...)\n\nNow, when your user calls foo(), they are effectively calling pkgB::blah(), at the version installed on their machine at that very moment.\nA real example of this affected an older version of knitr, related to how the default “evaluate” hook was being set to evaluate::evaluate() (original issue is yihui/knitr#1441, resolved in commit d6b53e0)."
  },
  {
    "objectID": "code.html#sec-code-r-landscape",
    "href": "code.html#sec-code-r-landscape",
    "title": "7  R code",
    "section": "\n7.5 Respect the R landscape",
    "text": "7.5 Respect the R landscape\nAnother big difference between a script and a package is that other people are going to use your package, and they’re going to use it in situations that you never imagined. This means you need to pay attention to the R landscape, which includes not just the available functions and objects, but all the global settings.\nYou have changed the R landscape if you’ve loaded a package with library(), or changed a global option with options(), or modified the working directory with setwd(). If the behaviour of other functions differs before and after running your function, you’ve modified the landscape. Section 6.7 has a concrete example of this involving time zones and the locale-specific printing of datetimes. Changing the landscape is bad because it makes code much harder to understand.\nThere are some functions that modify global settings that you should never use because there are better alternatives:\n\nDon’t use library() or require(). These modify the search path, affecting what functions are available from the global environment. Instead, you should use the DESCRIPTION to specify your package’s requirements, as described in Chapter 10. This also makes sure those packages are installed when your package is installed.\nNever use source() to load code from a file. source() modifies the current environment, inserting the results of executing the code. There is no reason to use source() inside your package, i.e. in a file below R/. Sometimes people source() files below R/ during package development, but as we’ve explained in Section 5.4 and Section 7.2, load_all() is a much better way to load your current code for exploration. If you’re using source() to create a dataset, it is better to use the methods in Chapter 8 for including data in a package.\n\nHere is a non-exhaustive list of other functions that should be used with caution:\n\noptions()\npar()\nsetwd()\nSys.setenv()\nSys.setlocale()\n\nset.seed() (or anything that changes the state of the random number generator)\n\nIf you must use them, make sure to clean up after yourself. Below we show how to do this using functions from the withr package and in base R.\nThe flip side of this coin is that you should avoid relying on the user’s landscape, which might be different to yours. For example, functions that rely on sorting strings are dangerous, because sort order depends on the system locale. Below we see that locales one might actually encounter in practice (C, English, French, etc.) differ in how they sort non-ASCII strings or uppercase versus lowercase letters.\n\nx <- c(\"bernard\", \"bérénice\", \"béatrice\", \"boris\")\n\nwithr::with_locale(c(LC_COLLATE = \"fr_FR\"), sort(x))\n#> [1] \"béatrice\" \"bérénice\" \"bernard\"  \"boris\"\nwithr::with_locale(c(LC_COLLATE = \"C\"), sort(x))\n#> [1] \"bernard\"  \"boris\"    \"béatrice\" \"bérénice\"\n\nx <- c(\"a\", \"A\", \"B\", \"b\", \"A\", \"b\")\n\nwithr::with_locale(c(LC_COLLATE = \"en_CA\"), sort(x))\n#> [1] \"a\" \"A\" \"A\" \"b\" \"b\" \"B\"\nwithr::with_locale(c(LC_COLLATE = \"C\"), sort(x))\n#> [1] \"A\" \"A\" \"B\" \"a\" \"b\" \"b\"\n\nIf you write your functions as if all users have the same system locale as you, your code might fail.\n\n7.5.1 Manage state with withr\nIf you need to modify the R landscape inside a function, then it is important to ensure your change is reversed on exit of that function. This is exactly what base::on.exit() is designed to do. You use on.exit() inside a function to register code to run later, that restores the landscape to its original state. It is important to note that proper tools, such as on.exit(), work even if we exit the function abnormally, i.e. due to an error. This is why it’s worth using the official methods described here over any do-it-yourself solution.\nWe usually manage state using the withr package, which provides a flexible, on.exit()-like toolkit (on.exit() itself is covered in the next section). withr::defer() can be used as a drop-in replacement for on.exit(). Why do we like withr so much? First, it offers many pre-built convenience functions for state changes that come up often. We also appreciate withr’s default stack-like behaviour (LIFO = last in, first out), its usability in interactive sessions, and its envir argument (in more advanced usage).\nThe general pattern is to capture the original state, schedule its eventual restoration “on exit”, then make the state change. Some setters, such as options() or par(), return the old value when you provide a new value, leading to usage that looks like this:\n\nf <- function(x, y, z) {\n  ...                        # width option \"as found\"\n  old <- options(width = 20) # width option is 20\n  defer(options(old))        # width option is 20\n  ...                        # width option is 20\n}                            # original width option restored\n\nCertain state changes, such as modifying session options, come up so often that withr offers pre-made helpers. Table 7.2 shows a few of the state change helpers in withr that you are most likely to find useful:\n\n\nTable 7.2: Selected functions from withr.\n\nDo / undo this\nwithr functions\n\n\n\nSet an R option\n\nwith_options(), local_options()\n\n\n\nSet an environment variable\n\nwith_envvar(), local_envvar()\n\n\n\nChange working directory\n\nwith_dir(), local_dir()\n\n\n\nSet a graphics parameter\n\nwith_par(), local_par()\n\n\n\n\n\nYou’ll notice each helper comes in two forms that are useful in different situations:\n\n\nwith_*() functions are best for executing small snippets of code with a temporarily modified state. (These functions are inspired by how base::with() works.)\n\nf <- function(x, sig_digits) {\n  # imagine lots of code here\n  withr::with_options(\n    list(digits = sig_digits),\n    print(x)\n  )\n  # ... and a lot more code here\n}\n\n\n\nlocal_*() functions are best for modifying state “from now until the function exits”.\n\ng <- function(x, sig_digits) {\n  withr::local_options(list(digits = sig_digits))\n  print(x)\n  # imagine lots of code here\n}\n\n\n\nDeveloping code interactively with withr is pleasant, because deferred actions can be scheduled even on the global environment. Those cleanup actions can then be executed with withr::deferred_run() or cleared without execution with withr::deferred_clear(). Without this feature, it can be tricky to experiment with code that needs cleanup “on exit”, because it behaves so differently when executed in the console versus at arm’s length inside a function.\nMore in-depth coverage is given in the withr vignette Changing and restoring state and withr will also prove useful when we talk about testing in Chapter 14.\n\n7.5.2 Restore state with base::on.exit()\n\nHere is how the general “save, schedule restoration, change” pattern looks when using base::on.exit().\n\nf <- function(x, y, z) {\n  ...\n  old <- options(mfrow = c(2, 2), pty = \"s\")\n  on.exit(options(old), add = TRUE)\n  ...\n}\n\nOther state changes aren’t available with that sort of setter and you must implement it yourself.\n\ng <- function(a, b, c) {\n  ...\n  scratch_file <- tempfile()\n  on.exit(unlink(scratch_file), add = TRUE)\n  file.create(scratch_file)\n  ...\n}\n\nNote that we specify on.exit(..., add = TRUE), because you almost always want this behaviour, i.e. to add to the list of deferred cleanup tasks rather than to replace them entirely. This (and the default value of after) are related to our preference for withr::defer(), when we’re willing to take a dependency on withr. These issues are explored in a withr vignette.\n\n7.5.3 Isolate side effects\nCreating plots and printing output to the console are two other ways of affecting the global R environment. Often you can’t avoid these (because they’re important!) but it’s good practice to isolate them in functions that only produce output. This also makes it easier for other people to repurpose your work for new uses. For example, if you separate data preparation and plotting into two functions, others can use your data prep work (which is often the hardest part!) to create new visualisations.\n\n7.5.4 When you do need side-effects\nOccasionally, packages do need side-effects. This is most common if your package talks to an external system — you might need to do some initial setup when the package loads. To do that, you can use two special functions: .onLoad() and .onAttach(). These are called when the package is loaded and attached. You’ll learn about the distinction between the two in Section 11.4. For now, you should always use .onLoad() unless explicitly directed otherwise.\nSome common uses of .onLoad() and .onAttach() are:\n\n\nTo set custom options for your package with options(). To avoid conflicts with other packages, ensure that you prefix option names with the name of your package. Also be careful not to override options that the user has already set. Here’s a (highly redacted) version of dplyr’s .onLoad() function which sets an option that controls progress reporting:\n\n.onLoad <- function(libname, pkgname) {\n  op <- options()\n  op.dplyr <- list(\n    dplyr.show_progress = TRUE\n  )\n  toset <- !(names(op.dplyr) %in% names(op))\n  if (any(toset)) options(op.dplyr[toset])\n\n  invisible()\n}\n\nThis allows functions in dplyr to use getOption(\"dplyr.show_progress\") to determine whether to show progress bars, relying on the fact that a sensible default value has already been set.\n\n\n\n\nTo display an informative message when the package is attached. This might make usage conditions clear or display package capabilities based on current system conditions. Startup messages are one place where you should use .onAttach() instead of .onLoad(). To display startup messages, always use packageStartupMessage(), and not message(). (This allows suppressPackageStartupMessages() to selectively suppress package startup messages).\n\n.onAttach <- function(libname, pkgname) {\n  packageStartupMessage(\"Welcome to my package\")\n}\n\n\n\nAs you can see in the examples, .onLoad() and .onAttach() are called with two arguments: libname and pkgname. They’re rarely used (they’re a holdover from the days when you needed to use library.dynam() to load compiled code). They give the path where the package is installed (the “library”), and the name of the package.\nIf you use .onLoad(), consider using .onUnload() to clean up any side effects. By convention, .onLoad() and friends are usually saved in a file called R/zzz.R. (Note that .First.lib() and .Last.lib() are old versions of .onLoad() and .onUnload() and should no longer be used.)\nOne especially hairy thing to do in a function like .onLoad() or .onAttach() is to change the state of the random number generator. Once upon a time, ggplot2 used sample() when deciding whether to show a startup message, but only in interactive sessions. This, in turn, created a reproducibility puzzle for users who were using set.seed() for their own purposes, prior to attaching ggplot2 with library(ggplot2), and running the code both interactively and noninteractively. The chosen solution was to wrap the offending startup code inside withr::with_preserve_seed(), which leaves the user’s random seed as it found it."
  },
  {
    "objectID": "code.html#constant-health-checks",
    "href": "code.html#constant-health-checks",
    "title": "7  R code",
    "section": "\n7.6 Constant health checks",
    "text": "7.6 Constant health checks\nHere is a typical sequence of calls when using devtools for package development:\n\nEdit one or more files below R/.\n\ndocument() (if you’ve made any changes that impact help files or NAMESPACE)\nload_all()\nRun some examples interactively.\n\ntest() (or test_active_file())\ncheck()\n\nAn interesting question is how frequently and rapidly you move through this development cycle. We often find ourselves running through the above sequence several times in an hour or in a day while adding or modifying a single function.\nThose newer to package development might be most comfortable slinging R code and much less comfortable writing and compiling documentation, simulating package build & installation, testing, and running R CMD check. And it is human nature to embrace the familiar and postpone the unfamiliar. This often leads to a dysfunctional workflow where the full sequence above unfolds infrequently, maybe once per month or every couple of months, very slowly and often with great pain:\n\nEdit one or more files below R/.\nBuild, install, and use the package. Iterate occasionally with previous step.\nWrite documentation (once the code is “done”).\nWrite tests (once the code is “done”).\nRun R CMD check right before submitting to CRAN or releasing in some other way.\n\nWe’ve already talked about the value of fast feedback, in the context of load_all(). But this also applies to running document(), test(), and check(). There are defects you just can’t detect from using load_all() and running a few interactive examples that are immediately revealed by more formal checks. Finding and fixing 5 bugs, one at a time, right after you created each one is much easier than troubleshooting all 5 at once (possibly interacting with each other), weeks or months after you last touched the code.\n\n\n\n\n\n\nSubmitting to CRAN\n\n\n\nIf you’re planning on submitting your package to CRAN, you must use only ASCII characters in your .R files. In practice, this means you are limited to the digits 0 to 9, lowercase letters ‘a’ to ‘z’, uppercase letters ‘A’ to ‘Z’, and common punctuation.\nBut sometimes you need to inline a small bit of character data that includes, e.g., a Greek letter (µ), an accented character (ü), or a symbol (30°). You can use any Unicode character as long as you specify it in the special Unicode escape \"\\u1234\" format. The easiest way to find the correct code point is to use stringi::stri_escape_unicode():\n\nx <- \"This is a bullet •\"\ny <- \"This is a bullet \\u2022\"\nidentical(x, y)\n#> [1] TRUE\ncat(stringi::stri_escape_unicode(x))\n#> This is a bullet \\u2022\n\nSometimes you have the opposite problem. You don’t intentionally have any non-ASCII characters in your R code, but automated checks reveal that you do.\nW  checking R files for non-ASCII characters ...\n   Found the following file with non-ASCII characters:\n     foo.R\n   Portable packages must use only ASCII characters in their R code,\n   except perhaps in comments.\n   Use \\uxxxx escapes for other characters.\nThe most common offenders are “curly” or “smart” single and double quotes that sneak in through copy/paste. The functions tools::showNonASCII() and tools::showNonASCIIfile(file) help you find the offending file(s) and line(s).\n\ntools::showNonASCIIfile(\"R/foo.R\")\n#> 666: #' If you<e2><80><99>ve copy/pasted quotes, watch out!"
  },
  {
    "objectID": "data.html#sec-data-data",
    "href": "data.html#sec-data-data",
    "title": "8  Data",
    "section": "\n8.1 Exported data",
    "text": "8.1 Exported data\nThe most common location for package data is (surprise!) data/. We recommend that each file in this directory be an .rda file created by save() containing a single R object, with the same name as the file. The easiest way to achieve this is to use usethis::use_data().\n\nmy_pkg_data <- sample(1000)\nusethis::use_data(my_pkg_data)\n\nLet’s imagine we are working on a package named “pkg”. The snippet above creates data/my_pkg_data.rda inside the source of the pkg package and adds LazyData: true in your DESCRIPTION. This makes the my_pkg_data R object available to users of pkg via pkg::my_pkg_data or, after attaching pkg with library(pkg), as my_pkg_data.\nThe snippet above is something the maintainer executes once (or every time they need to update my_pkg_data). This is workflow code and should not appear in the R/ directory of the source package. (We’ll talk about a suitable place to keep this code below.) For larger datasets, you may want to experiment with the compression setting, which is under the control of the compress argument. The default is “bzip2”, but sometimes “gzip” or “xz” can create smaller files.\nIt’s possible to use other types of files below data/, but we don’t recommend it because .rda files are already fast, small, and explicit. The other possibilities are described in the documentation for utils::data() and in the Data in packages section of Writing R Extensions. In terms of advice to package authors, the help topic for data() seems to implicitly make the same recommendations as we do above:\n\nStore one R object in each data/*.rda file\nUse the same name for that object and its .rda file\nUse lazy-loading, by default\n\nIf the DESCRIPTION contains LazyData: true, then datasets will be lazily loaded. This means that they won’t occupy any memory until you use them. The following example shows memory usage before and after loading the nycflights13 package. You can see that memory usage doesn’t change significantly until you inspect the flights dataset stored inside the package.\n\nlobstr::mem_used()\n#> 56.33 MB\nlibrary(nycflights13)\nlobstr::mem_used()\n#> 58.24 MB\n\ninvisible(flights)\nlobstr::mem_used()\n#> 98.95 MB\n\nWe recommend that you include LazyData: true in your DESCRIPTION if you are shipping .rda files below data/. If you use use_data() to create such datasets, it will automatically make this modification to DESCRIPTION for you.\n\n\n\n\n\n\nWarning\n\n\n\nIt is important to note that lazily-loaded datasets do not need to be pre-loaded with utils::data() and, in fact, it’s usually best to avoid doing so. Above, once we did library(nycflights13), we could immediately access flights. There is no call to data(flights), because it is not necessary.\nThere are specific downsides to data(some_pkg_data) calls that support a policy of only using data() when it is actually necessary, i.e. for datasets that would not be available otherwise:\n\nBy default, data(some_pkg_data), creates one or more objects in the user’s global workspace. There is the potential to silently overwrite pre-existing objects with new values.\nThere is also no guarantee that data(foo) will create exactly one object named “foo”. It could create more than one object and/or objects with totally different names.\n\nOne argument in favor of calls like data(some_pkg_data, package = \"pkg\") that are not strictly necessary is that it clarifies which package provides some_pkg_data. We prefer alternatives that don’t modify the global workspace, such as a code comment or access via pkg::some_pkg_data.\nThis excerpt from the documentation of data() conveys that it is largely of historical importance:\n\ndata() was originally intended to allow users to load datasets from packages for use in their examples, and as such it loaded the datasets into the workspace .GlobalEnv. This avoided having large datasets in memory when not in use: that need has been almost entirely superseded by lazy-loading of datasets.\n\n\n\n\n8.1.1 Preserve the origin story of package data\nOften, the data you include in data/ is a cleaned up version of raw data you’ve gathered from elsewhere. We highly recommend taking the time to include the code used to do this in the source version of your package. This makes it easy for you to update or reproduce your version of the data. This data-creating script is also a natural place to leave comments about important properties of the data, i.e. which features are important for downstream usage in package documentation.\nWe suggest that you keep this code in one or more .R files below data-raw/. You don’t want it in the bundled version of your package, so this folder should be listed in .Rbuildignore. usethis has a convenience function that can be called when you first adopt the data-raw/ practice or when you add an additional .R file to the folder:\n\nusethis::use_data_raw()\n\nusethis::use_data_raw(\"my_pkg_data\")\n\nuse_data_raw() creates the data-raw/ folder and lists it in .Rbuildignore. A typical script in data-raw/ includes code to prepare a dataset and ends with a call to use_data().\nThese data packages all use the approach recommended here for data-raw/:\n\nbabynames\nnycflights13\ngapminder\n\n\n\n\n\n\n\nggplot2: A cautionary tale\n\n\n\nWe have a confession to make: the origins of many of ggplot2’s example datasets have been lost in the sands of time. In the grand scheme of things, this is not a huge problem, but maintenance is certainly more pleasant when a package’s assets can be reconstructed de novo and easily updated as necessary.\n\n\n\n\n\n\n\n\nSubmitting to CRAN\n\n\n\nGenerally, package data should be smaller than a megabyte - if it’s larger you’ll need to argue for an exemption. This is usually easier to do if the data is in its own package and won’t be updated frequently, i.e. if you approach this as a dedicated “data package”. For reference, the babynames and nycflights packages have had a release once every one to two years, since they first appeared on CRAN.\nIf you are bumping up against size issues, you should be intentional with regards to the method of data compression. The default for usethis::use_data(compress =) is “bzip2”, whereas the default for save(compress =) is (effectively) “gzip”, and “xz” is yet another valid option.\nYou’ll have to experiment with different compression methods and make this decision empirically. tools::resaveRdaFiles(\"data/\") automates this process, but doesn’t inform you of which compression method was chosen. You can learn this after the fact with tools::checkRdaFiles(). Assuming you are keeping track of the code to generate your data, it would be wise to update the corresponding use_data(compress =) call below data-raw/ and re-generate the .rda cleanly.\n\n\n\n8.1.2 Documenting datasets\nObjects in data/ are always effectively exported (they use a slightly different mechanism than NAMESPACE but the details are not important). This means that they must be documented. Documenting data is like documenting a function with a few minor differences. Instead of documenting the data directly, you document the name of the dataset and save it in R/. For example, the roxygen2 block used to document the who data in tidyr is saved in R/data.R and looks something like this:\n\n#' World Health Organization TB data\n#'\n#' A subset of data from the World Health Organization Global Tuberculosis\n#' Report ...\n#'\n#' @format ## `who`\n#' A data frame with 7,240 rows and 60 columns:\n#' \\describe{\n#'   \\item{country}{Country name}\n#'   \\item{iso2, iso3}{2 & 3 letter ISO country codes}\n#'   \\item{year}{Year}\n#'   ...\n#' }\n#' @source <https://www.who.int/teams/global-tuberculosis-programme/data>\n\"who\"\n\nThere are two roxygen tags that are especially important for documenting datasets:\n\n@format gives an overview of the dataset. For data frames, you should include a definition list that describes each variable. It’s usually a good idea to describe variables’ units here.\n@source provides details of where you got the data, often a URL.\n\nNever @export a data set.\n\n8.1.3 Non-ASCII characters in data\nThe R objects you store in data/*.rda often contain strings, with the most common example being character columns in a data frame. If you can constrain these strings to only use ASCII characters, it certainly makes things simpler. But of course, there are plenty of legitimate reasons why package data might include non-ASCII characters.\nIn that case, we recommend that you embrace the UTF-8 Everywhere manifesto and use the UTF-8 encoding. The DESCRIPTION file placed by usethis::create_package() always includes Encoding: UTF-8, so by default a devtools-produced package already advertises that it will use UTF-8.\nMaking sure that the strings embedded in your package data have the intended encoding is something you accomplish in your data preparation code, i.e. in the R scripts below data-raw/. You can use Encoding() to learn the current encoding of the elements in a character vector and functions such as enc2utf8() or iconv() to convert between encodings.\n\n\n\n\n\n\nSubmitting to CRAN\n\n\n\nIf you have UTF-8-encoded strings in your package data, you may see this from R CMD check:\n-   checking data for non-ASCII characters ... NOTE\n    Note: found 352 marked UTF-8 strings\nThis NOTE is truly informational. It requires no action from you. As long as you actually intend to have UTF-8 strings in your package data, all is well.\nIronically, this NOTE is actually suppressed by R CMD check --as-cran, despite the fact that this note does appear in the check results once a package is on CRAN (which implies that CRAN does not necessarily check with --as-cran). By default, devtools::check() sets the --as-cran flag and therefore does not transmit this NOTE. But you can surface it with check(cran = FALSE, env_vars = c(\"_R_CHECK_PACKAGE_DATASETS_SUPPRESS_NOTES_\" = \"false\"))."
  },
  {
    "objectID": "data.html#sec-data-sysdata",
    "href": "data.html#sec-data-sysdata",
    "title": "8  Data",
    "section": "\n8.2 Internal data",
    "text": "8.2 Internal data\nSometimes your package functions need access to pre-computed data. If you put these objects in data/, they’ll also be available to package users, which is not appropriate. Sometimes the objects you need are small and simple enough that you can define them with c() or data.frame() in the code below R/, perhaps in R/data.R. Larger or more complicated objects should be stored in your package’s internal data in R/sysdata.rda.\nHere are some examples of internal package data:\n\nTwo colour-related packages, munsell and dichromat, use R/sysdata.rda to store large tables of colour data.\n\ngoogledrive and googlesheets4 wrap the Google Drive and Google Sheets APIs, respectively. Both use R/sysdata.rda to store data derived from a so-called Discovery Document which “describes the surface of the API, how to access the API and how API requests and responses are structured”.\n\nThe easiest way to create R/sysdata.rda is to use usethis::use_data(internal = TRUE):\n\ninternal_this <- ...\ninternal_that <- ...\n\nusethis::use_data(internal_this, internal_that, internal = TRUE)\n\nUnlike data/, where you use one .rda file per exported data object, you store all of your internal data objects together in the single file R/sysdata.rda.\nLet’s imagine we are working on a package named “pkg”. The snippet above creates R/sysdata.rda inside the source of the pkg package. This makes the objects internal_this and internal_that available for use inside of the functions defined below R/ and in the tests. During interactive development, internal_this and internal_that are available after a call to devtools::load_all(), just like an internal function.\nMuch of the advice given for external data holds for internal data as well:\n\nIt’s a good idea to store the code that generates your individual internal data objects, as well as the use_data() call that writes all of them into R/sysdata.rda. This is workflow code that belongs below data-raw/, not below R/.\n\nusethis::use_data_raw() can be used to initiate the use of data-raw/ or to initiate a new .R script there.\nIf your package is uncomfortably large, experiment with different values of compress in use_data(internal = TRUE).\n\nThere are also key distinctions, where the handling of internal and external data differs:\n\nObjects in R/sysdata.rda are not exported (they shouldn’t be), so they don’t need to be documented.\nUsage of R/sysdata.rda has no impact on DESCRIPTION, i.e. the need to specify the LazyData field is strictly about the exported data below data/."
  },
  {
    "objectID": "data.html#sec-data-extdata",
    "href": "data.html#sec-data-extdata",
    "title": "8  Data",
    "section": "\n8.3 Raw data file",
    "text": "8.3 Raw data file\nIf you want to show examples of loading/parsing raw data, put the original files in inst/extdata/. When the package is installed, all files (and folders) in inst/ are moved up one level to the top-level directory, which is why they can’t have names that conflict with standard parts of an R package, like R/ or DESCRIPTION . The files below inst/extdata/ in the source package will be located below extdata/ in the corresponding installed package.\nThe main reason to include such files is when a key part of a package’s functionality is to act on an external file. Examples of such packages include:\n\nreadr, which reads rectangular data out of delimited files\nreadxl, which reads rectangular data of of Excel spreadsheets\nxml2, which can read XML and HTML from file\narchive, which can read archive files, such as tar or ZIP\n\nAll of these packages have one or more example files below inst/extdata/, which are useful for writing documentation and tests.\nIt is also common for data packages to provide, e.g., a csv version of the package data that is also provided as an R object. Examples of such packages include:\n\npalmerpenguins: penguins and penguins_raw are also represented as extdata/penguins.csv and extdata/penguins_raw.csv\n\ngapminder: gapminder, continent_colors, and country_colors are also represented as extdata/gapminder.tsv, extdata/continent-colors.tsv, and extdata/country-colors.tsv\n\n\nThis has two payoffs: First, it gives teachers and other expositors more to work with once they decide to use a specific dataset. If you’ve started teaching R with palmerpenguins::penguins or gapminder::gapminder and you want to introduce data import, it can be helpful to students if their first use of a new command, like readr::read_csv() or read.csv(), is applied to a familiar dataset. They have pre-existing intuition about the expected result. Finally, if package data evolves over time, having a csv or other plain text representation in the source package can make it easier to see what’s changed.\n\n8.3.1 Filepaths\nThe path to a package file found below extdata/ clearly depends on the local environment, i.e. it depends on where installed packages live on that machine. The base function system.file() can report the full path to files distributed with an R package. It can also be useful to list the files distributed with an R package.\n\nsystem.file(\"extdata\", package = \"readxl\") |> list.files()\n#>  [1] \"clippy.xls\"    \"clippy.xlsx\"   \"datasets.xls\"  \"datasets.xlsx\"\n#>  [5] \"deaths.xls\"    \"deaths.xlsx\"   \"geometry.xls\"  \"geometry.xlsx\"\n#>  [9] \"type-me.xls\"   \"type-me.xlsx\"\n\nsystem.file(\"extdata\", \"clippy.xlsx\", package = \"readxl\")\n#> [1] \"/home/runner/work/_temp/Library/readxl/extdata/clippy.xlsx\"\n\nThese filepaths present yet another workflow dilemma: When you’re developing your package, you engage with it in its source form, but your users engage with it as an installed package. Happily, devtools provides a shim for base::system.file() that is activated by load_all(). This makes interactive calls to system.file() from the global environment and calls from within the package namespace “just work”.\nBe aware that, by default, system.file() returns the empty string, not an error, for a file that does not exist.\n\nsystem.file(\"extdata\", \"I_do_not_exist.csv\", package = \"readr\")\n#> [1] \"\"\n\nIf you want to force a failure in this case, specify mustWork = TRUE:\n\nsystem.file(\"extdata\", \"I_do_not_exist.csv\", package = \"readr\", mustWork = TRUE)\n#> Error in system.file(\"extdata\", \"I_do_not_exist.csv\", package = \"readr\", : no file found\n\nThe fs package offers fs::path_package(). This is essentially base::system.file() with a few added features that we find advantageous, whenever it’s reasonable to take a dependency on fs:\n\nIt errors if the filepath does not exist.\nIt throws distinct errors when the package does not exist vs. when the file does not exist within the package.\nDuring development, it works for interactive calls, calls from within the loaded package’s namespace, and even for calls originating in dependencies.\n\n\nfs::path_package(\"extdata\", package = \"idonotexist\")\n#> Error: Can't find package `idonotexist` in library locations:\n#>   - '/home/runner/work/_temp/Library'\n#>   - '/opt/R/4.2.3/lib/R/site-library'\n#>   - '/opt/R/4.2.3/lib/R/library'\n\nfs::path_package(\"extdata\", \"I_do_not_exist.csv\", package = \"readr\")\n#> Error: File(s) '/home/runner/work/_temp/Library/readr/extdata/I_do_not_exist.csv' do not exist\n\nfs::path_package(\"extdata\", \"chickens.csv\", package = \"readr\")\n#> /home/runner/work/_temp/Library/readr/extdata/chickens.csv\n\n\n\n8.3.2 pkg_example() path helpers\nWe like to offer convenience functions that make example files easy to access. These are just user-friendly wrappers around system.file() or fs::path_package(), but can have added features, such as the ability to list the example files. Here’s the definition and some usage of readxl::readxl_example():\n\nreadxl_example <- function(path = NULL) {\n  if (is.null(path)) {\n    dir(system.file(\"extdata\", package = \"readxl\"))\n  } else {\n    system.file(\"extdata\", path, package = \"readxl\", mustWork = TRUE)\n  }\n}\n\n\nreadxl::readxl_example()\n#>  [1] \"clippy.xls\"    \"clippy.xlsx\"   \"datasets.xls\"  \"datasets.xlsx\"\n#>  [5] \"deaths.xls\"    \"deaths.xlsx\"   \"geometry.xls\"  \"geometry.xlsx\"\n#>  [9] \"type-me.xls\"   \"type-me.xlsx\"\n\nreadxl::readxl_example(\"clippy.xlsx\")\n#> [1] \"/home/runner/work/_temp/Library/readxl/extdata/clippy.xlsx\""
  },
  {
    "objectID": "data.html#sec-data-state",
    "href": "data.html#sec-data-state",
    "title": "8  Data",
    "section": "\n8.4 Internal state",
    "text": "8.4 Internal state\nSometimes there’s information that multiple functions from your package need to access that:\n\nMust be determined at load time (or even later), not at build time. It might even be dynamic.\nDoesn’t make sense to pass in via a function argument. Often it’s some obscure detail that a user shouldn’t even know about.\n\nA great way to manage such data is to use an environment.1 This environment must be created at build time, but you can populate it with values after the package has been loaded and update those values over the course of an R session. This works because environments have reference semantics (whereas more pedestrian R objects, such as atomic vectors, lists, or data frames have value semantics).\nConsider a package that can store the user’s favorite letters or numbers. You might start out with code like this in a file below R/:\n\nfavorite_letters <- letters[1:3]\n\n#' Report my favorite letters\n#' @export\nmfl <- function() {\n  favorite_letters\n}\n\n#' Change my favorite letters\n#' @export\nset_mfl <- function(l = letters[24:26]) {\n  old <- favorite_letters\n  favorite_letters <<- l\n  invisible(old)\n}\n\nfavorite_letters is initialized to (“a”, “b”, “c”) when the package is built. The user can then inspect favorite_letters with mfl(), at which point they’ll probably want to register their favorite letters with set_mfl(). Note that we’ve used the super assignment operator <<- in set_mfl() in the hope that this will reach up into the package environment and modify the internal data object favorite_letters. But a call to set_mfl() fails like so:2\n\nmfl()\n#> [1] \"a\" \"b\" \"c\"\n\nset_mfl(c(\"j\", \"f\", \"b\"))\n#> Error in set_mfl() : \n#>   cannot change value of locked binding for 'favorite_letters'\n\nBecause favorite_letters is a regular character vector, modification requires making a copy and rebinding the name favorite_letters to this new value. And that is what’s disallowed: you can’t change the binding for objects in the package namespace (well, at least not without trying harder than this). Defining favorite_letters this way only works if you will never need to modify it.\nHowever, if we maintain state within an internal package environment, we can modify objects contained in the environment (and even add completely new objects). Here’s an alternative implementation that uses an internal environment named “the”.\n\nthe <- new.env(parent = emptyenv())\nthe$favorite_letters <- letters[1:3]\n\n#' Report my favorite letters\n#' @export\nmfl2 <- function() {\n  the$favorite_letters\n}\n\n#' Change my favorite letters\n#' @export\nset_mfl2 <- function(l = letters[24:26]) {\n  old <- the$favorite_letters\n  the$favorite_letters <- l\n  invisible(old)\n}\n\nNow a user can register their favorite letters:\n\nmfl2()\n#> [1] \"a\" \"b\" \"c\"\n\nset_mfl2(c(\"j\", \"f\", \"b\"))\n\nmfl2()\n#> [1] \"j\" \"f\" \"b\"\n\nNote that this new value for the$favorite_letters persists only for the remainder of the current R session (or until the user calls set_mfl2() again). More precisely, the altered state persists only until the next time the package is loaded (including via load_all()). At load time, the environment the is reset to an environment containing exactly one object, named favorite_letters, with value (“a”, “b”, “c”). It’s like the movie Groundhog Day. (We’ll discuss more persistent package- and user-specific data in the next section.)\nJim Hester introduced our group to the nifty idea of using “the” as the name of an internal package environment. This lets you refer to the objects inside in a very natural way, such as the$token, meaning “the token”. It is also important to specify parent = emptyenv() when defining an internal environment, as you generally don’t want the environment to inherit from any other (nonempty) environment.\nAs seen in the example above, the definition of the environment should happen as a top-level assignment in a file below R/. (In particular, this is a legitimate reason to define a non-function at the top-level of a package; see section Section 7.4 for why this should be rare.) As for where to place this definition, there are two considerations:\n\nDefine it before you use it. If other top-level calls refer to the environment, the definition must come first when the package code is being executed at build time. This is why R/aaa.R is a common and safe choice.\nMake it easy to find later when you’re working on related functionality. If an environment is only used by one family of functions, define it there. If environment usage is sprinkled around the package, define it in a file with package-wide connotations.\n\nHere are some examples of how packages use an internal environment:\n\ngoogledrive: Various functions need to know the file ID for the current user’s home directory on Google Drive. This requires an API call (a relatively expensive and error-prone operation) which yields an eye-watering string of ~40 seemingly random characters that only a computer can love. It would be inhumane to expect a user to know this or to pass it into every function. It would also be inefficient to rediscover the ID repeatedly. Instead, googledrive determines the ID upon first need, then caches it for later use.\nusethis: Most functions need to know the active project, i.e. which directory to target for file modification. This is often the current working directory, but that is not an invariant usethis can rely upon. One potential design is to make it possible to specify the target project as an argument of every function in usethis. But this would create significant clutter in the user interface, as well as internal fussiness. Instead, we determine the active project upon first need, cache it, and provide methods for (re)setting it.\n\nThe blog post Package-Wide Variables/Cache in R Packages gives a more detailed development of this technique."
  },
  {
    "objectID": "data.html#sec-data-persistent",
    "href": "data.html#sec-data-persistent",
    "title": "8  Data",
    "section": "\n8.5 Persistent user data",
    "text": "8.5 Persistent user data\nSometimes there is data that your package obtains, on behalf of itself or the user, that should persist even across R sessions. This is our last and probably least common form of storing package data. For the data to persist this way, it has to be stored on disk and the big question is where to write such a file.\nThis problem is hardly unique to R. Many applications need to leave notes to themselves. It is best to comply with external conventions, which in this case means the XDG Base Directory Specification. You need to use the official locations for persistent file storage, because it’s the responsible and courteous thing to do and also to comply with CRAN policies.\n\n\n\n\n\n\nSubmitting to CRAN\n\n\n\nYou can’t just write persistent data into the user’s home directory. Here’s a relevant excerpt from the CRAN policy at the time of writing:\n\nPackages should not write in the user’s home filespace (including clipboards), nor anywhere else on the file system apart from the R session’s temporary directory ….\nFor R version 4.0 or later (hence a version dependency is required or only conditional use is possible), packages may store user-specific data, configuration and cache files in their respective user directories obtained from tools::R_user_dir(), provided that by [sic] default sizes are kept as small as possible and the contents are actively managed (including removing outdated material).\n\n\n\nThe primary function you should use to derive acceptable locations for user data is tools::R_user_dir()3. Here are some examples of the generated filepaths:\n\ntools::R_user_dir(\"pkg\", which = \"data\")\n#> [1] \"/home/runner/.local/share/R/pkg\"\ntools::R_user_dir(\"pkg\", which = \"config\")\n#> [1] \"/home/runner/.config/R/pkg\"\ntools::R_user_dir(\"pkg\", which = \"cache\")\n#> [1] \"/home/runner/.cache/R/pkg\"\n\nOne last thing you should consider with respect to persistent data is: does this data really need to persist? Do you really need to be the one responsible for storing it?\nIf the data is potentially sensitive, such as user credentials, it is recommended to obtain the user’s consent to store it, i.e. to require interactive consent when initiating the cache. Also consider that the user’s operating system or command line tools might provide a means of secure storage that is superior to any DIY solution that you might implement. The packages keyring, gitcreds, and credentials are examples of packages that tap into externally-provided tooling. Before embarking on any creative solution for storing secrets, consider that your effort is probably better spent integrating with an established tool."
  },
  {
    "objectID": "misc.html#other-directories",
    "href": "misc.html#other-directories",
    "title": "9  Other components",
    "section": "\n9.1 Other directories",
    "text": "9.1 Other directories\nHere are some top-level directories you might encounter in an R source package, in rough order of importance and frequency of use:\n\nsrc/: source and header files for compiled code, most often C and C++. This is an important technique that is used to make R packages more performant and to unlock the power of external libraries for R users. As of the second edition, the book no longer covers this topic, since a truly useful treatment of compiled code requires more space than we can give it here. The tidyverse generally uses the cpp11 package to connect C++ to R; most other packages use Rcpp, the most well-established package for integrating R and C++.\ninst/: for arbitrary additional files that you want include in your package. This includes a few special files, like the CITATION, described below in Section 9.2. Other examples of files that might appear below inst/ include R Markdown templates (see usethis::use_rmarkdown_template()) or RStudio add-ins.\ntools/: auxiliary files needed during configuration, usually found in the company of a configure script. We discuss this more below in Section 9.3.\ndemo/: for package demos. We regard demos as a legacy phenomenon, whose goals are now better met by vignettes (Chapter 18). For actively maintained packages, it probably makes sense to repurpose the content in any existing demos somewhere that’s more visible, e.g. in README.Rmd (Section 19.1) or in vignettes (Chapter 18). These other locations offer other advantages, such as making sure that the code is exercised regularly. This is not true of actual demos, leaving them vulnerable to rot.\nexec/: for executable scripts. Unlike files placed in other directories, files in exec/ are automatically flagged as executable. Empirically, to the extent that R package are shipping scripts for external interpreters, the inst/ directory seems to be preferred location these days.\npo/: translations for messages. This is useful, but beyond the scope of this book. See the Internationalization chapter of “Writing R extensions” and the potools package for more details."
  },
  {
    "objectID": "misc.html#sec-misc-inst",
    "href": "misc.html#sec-misc-inst",
    "title": "9  Other components",
    "section": "\n9.2 Installed files",
    "text": "9.2 Installed files\nWhen a package is installed, everything in inst/ is copied into the top-level directory of the installed package. In some sense inst/ is the opposite of .Rbuildignore - where .Rbuildignore lets you remove arbitrary files and directories from the built package, inst/ lets you add them.\n\n\n\n\n\n\nWarning\n\n\n\nYou are free to put anything you like in inst/ with one caution: because inst/ is copied into the top-level directory, don’t create a subdirectory that collides with any of the directories that make up the official structure of an R package. We recommend avoiding directories with special significance in either the source or installed form of a package, such as: inst/data, inst/help, inst/html, inst/libs, inst/man, inst/Meta, inst/R, inst/src, inst/tests, inst/tools, and inst/vignettes. In most cases, this prevents you from having a malformed package. And even though some of the above directories are technically allowed, they can be an unnecessary source of confusion.\n\n\nHere are some of the most common files and folders found in inst/:\n\ninst/CITATION: how to cite the package, see below for details.\ninst/extdata: additional external data for examples and vignettes. See section Section 8.3 for more detail.\n\nWhat if you need a path to the file at inst/foo to use in, e.g., the code below R/ or in your documentation? The default solution is to use system.file(\"foo\", package = \"yourpackage\"). But this presents a workflow dilemma: When you’re developing your package, you engage with it in its source form (inst/foo), but your users engage with its installed form (/foo). Happily, devtools provides a shim for system.file() that is activated by load_all(). Section Section 8.3.1 covers this in more depth and includes an interesting alternative, fs::path_package() .\n\n9.2.1 Package citation\nThe CITATION file lives in the inst directory and is intimately connected to the citation() function which tells you how to cite R and R packages. Calling citation() without any arguments tells you how to cite base R:\n\ncitation()\n#> \n#> To cite R in publications use:\n#> \n#>   R Core Team (2023). R: A language and environment for\n#>   statistical computing. R Foundation for Statistical\n#>   Computing, Vienna, Austria. URL\n#>   https://www.R-project.org/.\n#> \n#> A BibTeX entry for LaTeX users is\n#> \n#>   @Manual{,\n#>     title = {R: A Language and Environment for Statistical Computing},\n#>     author = {{R Core Team}},\n#>     organization = {R Foundation for Statistical Computing},\n#>     address = {Vienna, Austria},\n#>     year = {2023},\n#>     url = {https://www.R-project.org/},\n#>   }\n#> \n#> We have invested a lot of time and effort in creating R,\n#> please cite it when using it for data analysis. See also\n#> 'citation(\"pkgname\")' for citing R packages.\n\nCalling it with a package name tells you how to cite that package:\n\ncitation(\"tidyverse\")\n#> \n#> To cite package 'tidyverse' in publications use:\n#> \n#>   Wickham H, Averick M, Bryan J, Chang W, McGowan LD,\n#>   François R, Grolemund G, Hayes A, Henry L, Hester J, Kuhn\n#>   M, Pedersen TL, Miller E, Bache SM, Müller K, Ooms J,\n#>   Robinson D, Seidel DP, Spinu V, Takahashi K, Vaughan D,\n#>   Wilke C, Woo K, Yutani H (2019). \"Welcome to the\n#>   tidyverse.\" _Journal of Open Source Software_, *4*(43),\n#>   1686. doi:10.21105/joss.01686\n#>   <https://doi.org/10.21105/joss.01686>.\n#> \n#> A BibTeX entry for LaTeX users is\n#> \n#>   @Article{,\n#>     title = {Welcome to the {tidyverse}},\n#>     author = {Hadley Wickham and Mara Averick and Jennifer Bryan and Winston Chang and Lucy D'Agostino McGowan and Romain François and Garrett Grolemund and Alex Hayes and Lionel Henry and Jim Hester and Max Kuhn and Thomas Lin Pedersen and Evan Miller and Stephan Milton Bache and Kirill Müller and Jeroen Ooms and David Robinson and Dana Paige Seidel and Vitalie Spinu and Kohske Takahashi and Davis Vaughan and Claus Wilke and Kara Woo and Hiroaki Yutani},\n#>     year = {2019},\n#>     journal = {Journal of Open Source Software},\n#>     volume = {4},\n#>     number = {43},\n#>     pages = {1686},\n#>     doi = {10.21105/joss.01686},\n#>   }\n\nThe associated inst/CITATION file looks like this:\n\nbibentry(\n  \"Article\",\n  title = \"Welcome to the {tidyverse}\",\n  author = \"Hadley Wickham, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D'Agostino McGowan, Romain François, Garrett Grolemund, Alex Hayes, Lionel Henry, Jim Hester, Max Kuhn, Thomas Lin Pedersen, Evan Miller, Stephan Milton Bache, Kirill Müller, Jeroen Ooms, David Robinson, Dana Paige Seidel, Vitalie Spinu, Kohske Takahashi, Davis Vaughan, Claus Wilke, Kara Woo, Hiroaki Yutani\",\n  year = 2019,\n  journal = \"Journal of Open Source Software\",\n  volume = 4,\n  number = 43,\n  pages = 1686,\n  doi = \"10.21105/joss.01686\",\n)\n\nYou can call usethis::use_citation() to initiate this file and fill in your details. Read the ?bibentry help topic for more details."
  },
  {
    "objectID": "misc.html#sec-misc-tools",
    "href": "misc.html#sec-misc-tools",
    "title": "9  Other components",
    "section": "\n9.3 Configuration tools",
    "text": "9.3 Configuration tools\nIf a package has a configuration script (configure on Unix-alikes, configure.win on Windows), it is executed as the first step by R CMD  INSTALL. This is typically associated with a package that has a src/ subdirectory containing C/C++ code and the configure script is needed at compile time. If that script needs auxiliary files, those should be located in the tools/ directory. The scripts below tools/ can have an effect on the installed package, but the contents of tools/ will not ultimately be present in the installed package. In any case, this is mostly (but not solely) relevant to packages with compiled code, which is beyond the scope of this book.\nWe bring this up because, in practice, some packages use the tools/ directory for a different but related purpose. Some packages have periodic maintenance tasks for which it is helpful to record detailed instructions. For example, many packages embed some sort of external resource, e.g. code or data:\n\nSource code and headers for an embedded third party C/C++ library.\nWeb toolkits.\nR code that’s inlined (as opposed to imported).\nSpecification for a web API.\nColour palettes, styles, and themes.\n\nThese external assets are also usually evolving over time, so they need to be re-ingested on a regular basis. This makes it particularly rewarding to implement such housekeeping programmatically.\nThis is the second, unofficial use of the tools/ directory, characterized by two big differences with its official purpose: The packages that do this generally do not have a configure script and they list tools/ in .Rbuildignore, meaning that these scripts are not included in the package bundle. These scripts are maintained in the source package for developer convenience, but are never shipped with the package.\nThis practice is closely related to our recommendation to store the instructions for the creation of package data in data-raw/ (section Section 8.1.1) and to record the method of construction for any test fixtures (section Section 16.1.3)."
  },
  {
    "objectID": "description.html#the-description-file",
    "href": "description.html#the-description-file",
    "title": "10  DESCRIPTION",
    "section": "\n10.1 The DESCRIPTION file",
    "text": "10.1 The DESCRIPTION file\nThe job of the DESCRIPTION file is to store important metadata about your package. When you first start writing packages, you’ll mostly use these metadata to record what packages are needed to run your package. However, as time goes by, other aspects of the metadata file will become useful to you, such as revealing what your package does (via the Title and Description) and whom to contact (you!) if there are any problems.\nEvery package must have a DESCRIPTION. In fact, it’s the defining feature of a package (RStudio and devtools consider any directory containing DESCRIPTION to be a package)1. To get you started, usethis::create_package(\"mypackage\") automatically adds a bare-bones DESCRIPTION file. This will allow you to start writing the package without having to worry about the metadata until you need to. This minimal DESCRIPTION will vary a bit depending on your settings, but should look something like this:\n\nPackage: mypackage\nTitle: What the Package Does (One Line, Title Case)\nVersion: 0.0.0.9000\nAuthors@R: \n    person(\"First\", \"Last\", , \"first.last@example.com\", role = c(\"aut\", \"cre\"),\n           comment = c(ORCID = \"YOUR-ORCID-ID\"))\nDescription: What the package does (one paragraph).\nLicense: `use_mit_license()`, `use_gpl3_license()` or friends to pick a\n    license\nEncoding: UTF-8\nRoxygen: list(markdown = TRUE)\nRoxygenNote: 7.2.3\n\nIf you create a lot of packages, you can customize the default content of new DESCRIPTION files by setting the global option usethis.description to a named list. You can pre-configure your preferred name, email, license, etc. See the article on usethis setup for more details.\nDESCRIPTION uses a simple file format called DCF, the Debian control format. You can see most of the structure in the examples in this chapter. Each line consists of a field name and a value, separated by a colon. When values span multiple lines, they need to be indented:\nDescription: The description of a package usually spans multiple lines.\n    The second and subsequent lines should be indented, usually with four\n    spaces.\nIf you ever need to work with a DESCRIPTION file programmatically, take a look at the desc package, which usethis uses heavily under-the-hood.\nThis chapter shows you how to use the most important DESCRIPTION fields."
  },
  {
    "objectID": "description.html#sec-description-title-and-description",
    "href": "description.html#sec-description-title-and-description",
    "title": "10  DESCRIPTION",
    "section": "\n10.2 Title and Description: What does your package do?",
    "text": "10.2 Title and Description: What does your package do?\nThe title and description fields describe what the package does. They differ only in length:\n\n\nTitle is a one line description of the package, and is often shown in a package listing. It should be plain text (no markup), capitalised like a title, and NOT end in a period. Keep it short: listings will often truncate the title to 65 characters.\n\nDescription is more detailed than the title. You can use multiple sentences, but you are limited to one paragraph. If your description spans multiple lines (and it should!), each line must be no more than 80 characters wide. Indent subsequent lines with 4 spaces.\n\nThe Title and Description for ggplot2 are:\nTitle: Create Elegant Data Visualisations Using the Grammar of Graphics\nDescription: A system for 'declaratively' creating graphics,\n    based on \"The Grammar of Graphics\". You provide the data, tell 'ggplot2'\n    how to map variables to aesthetics, what graphical primitives to use,\n    and it takes care of the details.\nA good title and description are important, especially if you plan to release your package to CRAN, because they appear on the package’s CRAN landing page as shown in Figure 10.1:\n\n\n\n\n\nFigure 10.1: How Title and Description appear on ggplot2’s CRAN page.\n\n\n\n\n\n\n\n\n\n\nSubmitting to CRAN\n\n\n\nBoth the Title and Description are a frequent source of rejections for reasons not covered by the automated R CMD check. In addition to the basics above, here are a few more tips:\n\nPut the names of R packages, software, and APIs inside single quotes. This goes for both the Title and the Description. See the ggplot2 example above.\nIf you need to use an acronym, try to do so in Description, not in Title. In either case, explain the acronym in Description, i.e. fully expand it.\nDon’t include the package name, especially in Title, which is often prefixed with the package name.\nDo not start with “A package for …” or “This package does …”. This rule makes sense once you look at the list of CRAN packages by name. The information density of such a listing is much higher without a universal prefix like “A package for …”.\n\nIf these constraints give you writer’s block, it often helps to spend a few minutes reading Title and Description of packages already on CRAN. Once you read a couple dozen, you can usually find a way to say what you want to say about your package that is also likely to pass CRAN’s human-enforced checks.\n\n\nYou’ll notice that Description only gives you a small amount of space to describe what your package does. This is why it’s so important to also include a README.md file that goes into much more depth and shows a few examples. You’ll learn about that in Section 19.1."
  },
  {
    "objectID": "description.html#sec-description-authors-at-r",
    "href": "description.html#sec-description-authors-at-r",
    "title": "10  DESCRIPTION",
    "section": "\n10.3 Author: who are you?",
    "text": "10.3 Author: who are you?\nUse the Authors@R field to identify the package’s author, and whom to contact if something goes wrong. This field is unusual because it contains executable R code rather than plain text. Here’s an example:\nAuthors@R: person(\"Hadley\", \"Wickham\", email = \"hadley@posit.co\",\n  role = c(\"aut\", \"cre\"))\n\nperson(\"Hadley\", \"Wickham\", email = \"hadley@posit.co\", \n  role = c(\"aut\", \"cre\"))\n#> [1] \"Hadley Wickham <hadley@posit.co> [aut, cre]\"\n\nThis command says that Hadley Wickham is both the maintainer (cre) and an author (aut) and that his email address is hadley@posit.co. The person() function has four main inputs:\n\nThe name, specified by the first two arguments, given and family (these are normally supplied by position, not name). In English cultures, given (first name) comes before family (last name). In many cultures, this convention does not hold. For a non-person entity, such as “R Core Team” or “Posit PBC”, use the given argument (and omit family).\nThe email address, which is only an absolute requirement for the maintainer. It’s important to note that this is the address CRAN uses to let you know if your package needs to be fixed in order to stay on CRAN. Make sure to use an email address that’s likely to be around for a while. CRAN policy requires that this be for a person, as opposed to, e.g., a mailing list.\n\nOne or more three letter codes specifying the role. These are the most important roles to know about:\n\ncre: the creator or maintainer, the person you should bother if you have problems. Despite being short for “creator”, this is the correct role to use for the current maintainer, even if they are not the initial creator of the package.\naut: authors, those who have made significant contributions to the package.\nctb: contributors, those who have made smaller contributions, like patches.\ncph: copyright holder. This is used to list additional copyright holders who are not authors, typically companies, like an employer of one or more of the authors.\nfnd: funder, the people or organizations that have provided financial support for the development of the package.\n\n\n\nThe optional comment argument has become more relevant, since person() and CRAN landing pages have gained some nice features around ORCID identifiers. Here’s an example of such usage (note the auto-generated URI):\n\nperson(\n  \"Jennifer\", \"Bryan\",\n  email = \"jenny@posit.co\",\n  role = c(\"aut\", \"cre\"),\n  comment = c(ORCID = \"0000-0002-6983-2759\")\n)\n#> [1] \"Jennifer Bryan <jenny@posit.co> [aut, cre] (<https://orcid.org/0000-0002-6983-2759>)\"\n\n\n\nYou can list multiple authors with c():\nAuthors@R: c(\n    person(\"Hadley\", \"Wickham\", email = \"hadley@posit.co\", role = \"cre\"),\n    person(\"Jennifer\", \"Bryan\", email = \"jenny@posit.co\", role = \"aut\"),\n    person(\"Posit Software, PBC\", role = c(\"cph\", \"fnd\")))\nEvery package must have at least one author (aut) and one maintainer (cre) (they might be the same person). The maintainer (cre) must have an email address. These fields are used to generate the basic citation for the package (e.g. citation(\"pkgname\")). Only people listed as authors will be included in the auto-generated citation (Section 9.2.1). There are a few extra details if you’re including code that other people have written, which you can learn about in Section 13.4.\nAn older, still valid approach is to have separate Maintainer and Author fields in DESCRIPTION. However, we strongly recommend the more modern approach of Authors@R and the person() function, because it offers richer metadata for various downstream uses."
  },
  {
    "objectID": "description.html#url-and-bugreports",
    "href": "description.html#url-and-bugreports",
    "title": "10  DESCRIPTION",
    "section": "\n10.4 URL and BugReports\n",
    "text": "10.4 URL and BugReports\n\nAs well as the maintainer’s email address, it’s a good idea to list other places people can learn more about your package. The URL field is commonly used to advertise the package’s website (Chapter 20) and to link to a public source repository, where development happens. Multiple URLs are separated with a comma. BugReports is the URL where bug reports should be submitted, e.g., as GitHub issues. For example, devtools has:\nURL: https://devtools.r-lib.org/, https://github.com/r-lib/devtools\nBugReports: https://github.com/r-lib/devtools/issues\nIf you use usethis::use_github() to connect your local package to a remote GitHub repository, it will automatically populate URL and BugReports for you. If a package is already connected to a remote GitHub repository, usethis::use_github_links() can be called to just add the relevant links to DESCRIPTION."
  },
  {
    "objectID": "description.html#the-license-field",
    "href": "description.html#the-license-field",
    "title": "10  DESCRIPTION",
    "section": "\n10.5 The License field",
    "text": "10.5 The License field\nThe License field is mandatory and must specify your package’s license in a standard form recognized by R. The official tooling aims to identify standard open source licenses, so it’s important to appreciate that License is basically a machine-readable field. See Chapter 13 for a full discussion."
  },
  {
    "objectID": "description.html#sec-description-imports-suggests",
    "href": "description.html#sec-description-imports-suggests",
    "title": "10  DESCRIPTION",
    "section": "\n10.6 Imports, Suggests, and friends",
    "text": "10.6 Imports, Suggests, and friends\nTwo of the most important and commonly used DESCRIPTION fields are Imports and Suggests, which list other packages that your package depends on. Packages listed in Imports are needed by your users at runtime and will be installed (or potentially updated) when users install your package via install.packages(). The following lines indicate that your package absolutely needs both dplyr and tidyr to work.\nImports:\n    dplyr,\n    tidyr\nPackages listed in Suggests are either needed for development tasks or might unlock optional functionality for your users. The lines below indicate that, while your package can take advantage of ggplot2 and testthat, they’re not absolutely required:\nSuggests:\n    ggplot2,\n    testthat\nBoth Imports and Suggests take a comma-separated list of package names. We recommend putting one package on each line, and keeping them in alphabetical order. A non-haphazard order makes it easier for humans to parse this field and appreciate changes.\nThe easiest way to add a package to Imports or Suggests is with usethis::use_package(). If the dependencies are already in alphabetical order, use_package() will keep it that way. In general, it can be nice to run usethis::use_tidy_description() regularly, which orders and formats DESCRIPTION fields according to a fixed standard.\nIf you add packages to DESCRIPTION with usethis::use_package(), it will also remind you of the recommended way to call them (explained more in Chapter 12).\n\nusethis::use_package(\"dplyr\") # Default is \"Imports\"\n#> ✔ Adding 'dplyr' to Imports field in DESCRIPTION\n#> • Refer to functions with `dplyr::fun()`\n\nusethis::use_package(\"ggplot2\", \"Suggests\")\n#> ✔ Adding 'ggplot2' to Suggests field in DESCRIPTION\n#> • Use `requireNamespace(\"ggplot2\", quietly = TRUE)` to test if package is installed\n#> • Then directly refer to functions with `ggplot2::fun()`\n\n\n10.6.1 Minimum versions\nIf you need a specific version of a package, specify it in parentheses after the package name:\nImports:\n    dplyr (>= 1.0.0),\n    tidyr (>= 1.1.0)\nThe usethis::use_package() convenience function also helps you to set a minimum version:\n\n# exact version\nusethis::use_package(\"dplyr\", min_version = \"1.0.0\")\n\n# min version = currently installed version\nusethis::use_package(\"dplyr\", min_version = TRUE)\n\nYou always want to specify a minimum version (dplyr (>= 1.0.0)) rather than an exact version (dplyr (== 1.0.0)). Since R can’t have multiple versions of the same package loaded at the same time, specifying an exact dependency dramatically increases the chance of conflicting versions2.\nVersioning is most important if you will release your package for use by others. Usually people don’t have exactly the same versions of packages installed that you do. If someone has an older package that doesn’t have a function your package needs, they’ll get an unhelpful error message if your package does not advertise the minimum version it needs. However, if you state a minimum version, they’ll automatically get an upgrade when they install your package.\nThink carefully if you declare a minimum version for a dependency. In some sense, the safest thing to do is to require a version greater than or equal to the package’s current version. For public work, this is most naturally defined as the current CRAN version of a package; private or personal projects may adopt some other convention. But it’s important to appreciate the implications for people who try to install your package: if their local installation doesn’t fulfill all of your requirements around versions, installation will force upgrades of these dependencies. This is desirable if your minimum version requirements are genuine, i.e. your package would be broken otherwise. But if your stated requirements have a less solid rationale, this may be unnecessarily conservative and inconvenient.\nIn the absence of clear, hard requirements, you should set minimum versions (or not) based on your expected user base, the package versions they are likely to have, and a cost-benefit analysis of being too lax versus too conservative. The de facto policy of the tidyverse team is to specify a minimum version when using a known new feature or when someone encounters a version problem in authentic use. This isn’t perfect, but we don’t currently have the tooling to do better, and it seems to work fairly well in practice.\n\n10.6.2 Depends and LinkingTo\n\nThere are three other fields that allow you to express more specialised dependencies:\n\n\nDepends: Prior to the roll-out of namespaces in R 2.14.0 in 2011, Depends was the only way to “depend” on another package. Now, despite the name, you should almost always use Imports, not Depends. You’ll learn why, and when you should still use Depends, in Section 11.4.1.\nThe most legitimate current use of Depends is to state a minimum version for R itself, e.g. Depends: R (>= 4.0.0). Again, think carefully if you do this. This raises the same issues as setting a minimum version for a package you depend on, except the stakes are much higher when it comes to R itself. Users can’t simply consent to the necessary upgrade, so, if other packages depend on yours, your minimum version requirement for R can cause a cascade of package installation failures.\n\nThe backports package is useful if you want to use a function like tools::R_user_dir(), which was introduced in 4.0.0 in 2020, while still supporting older R versions.\nThe tidyverse packages officially support the current R version, the devel version, and four previous versions.3 We proactively test this support in the standard build matrix we use for continuous integration.\nPackages with a lower level of use may not need this level of rigour. The main takeaway is: if you state a minimum of R, you should have a reason and you should take reasonable measures to test your claim regularly.\n\n\nLinkingTo: if your package uses C or C++ code from another package, you need to list it here.\nEnhances: packages listed here are “enhanced” by your package. Typically, this means you provide methods for classes defined in another package (a sort of reverse Suggests). But it’s hard to define what that means, so we don’t recommend using Enhances.\n\n10.6.3 An R version gotcha\nBefore we leave this topic, we give a concrete example of how easily an R version dependency can creep in and have a broader impact than you might expect. The saveRDS() function writes a single R object as an .rds file, an R-specific format. For almost 20 years, .rds files used the “version 2” serialization format. “Version 3” became the new default in R 3.6.0 (released April 2019) and cannot be read by R versions prior to 3.5.0 (released April 2018).\nMany R packages have at least one .rds file lurking within and, if that gets re-generated with a modern R version, by default, the new .rds file will have the “version 3” format. When that R package is next built, such as for a CRAN submission, the required R version is automatically bumped to 3.5.0, signaled by this message:\nNB: this package now depends on R (>= 3.5.0)\n  WARNING: Added dependency on R >= 3.5.0 because serialized objects in\n  serialize/load version 3 cannot be read in older versions of R.\n  File(s) containing such objects:\n    'path/to/some_file.rds'\nLiterally, the DESCRIPTION file in the bundled package says Depends: R (>= 3.5.0), even if DESCRIPTION in the source package says differently4.\nWhen such a package is released on CRAN, the new minimum R version is viral, in the sense that all packages listing the original package in Imports or even Suggests have, to varying degrees, inherited the new dependency on R >= 3.5.0.\nThe immediate take-away is to be very deliberate about the version of .rds files until R versions prior to 3.5.0 have fallen off the edge of what you intend to support. This particular .rds issue won’t be with us forever, but similar issues crop up elsewhere, such as in the standards implicit in compiled C or C++ source code. The broader message is that the more reverse dependencies your package has, the more thought you need to give to your package’s stated minimum versions, especially for R itself."
  },
  {
    "objectID": "description.html#other-fields",
    "href": "description.html#other-fields",
    "title": "10  DESCRIPTION",
    "section": "\n10.7 Other fields",
    "text": "10.7 Other fields\nA few other DESCRIPTION fields are heavily used and worth knowing about:\n\nVersion is very important as a way of communicating where your package is in its lifecycle and how it is evolving over time. Learn more in Chapter 22.\nLazyData is relevant if your package makes data available to the user. If you specify LazyData: true, the datasets are lazy-loaded, which makes them more immediately available, i.e. users don’t have to use data(). The addition of LazyData: true is handled automatically by usethis::use_data(). More detail is given in Chapter 8.\nEncoding describes the character encoding of files throughout your package. Our tooling will set this to Encoding: UTF-8 as this is the most common encoding in use today, and we are not aware of any reasons to use a different value.\nCollate controls the order in which R files are sourced. This only matters if your code has side-effects; most commonly because you’re using S4. If needed, Collate is typically generated by roxygen2 through use of the @include tag. See ?roxygen2::update_collate for details.\nVignetteBuilder lists any package that your package needs as a vignette engine. Our recommended vignette workflow is described in Section 18.1, which will list the knitr package in VignetteBuilder.\n\nSystemRequirements is where you describe dependencies external to R. This is a plain text field and does not, for example, actually install or check for anything, so you might need to include additional installation details in your README (Section 19.1). The most common usage is in the context of a package with compiled code, where SystemRequirements is used to declare the C++ standard, the need for GNU make, or some other external dependency. Examples:\nSystemRequirements: C++17\nSystemRequirements: GNU make\nSystemRequirements: C++11, GNU make\nSystemRequirements: TensorFlow (https://www.tensorflow.org/\n\n\nWe discourage the explicit use of the Date field, as it is extremely easy to forget to update it if you manage Date by hand. This field will be populated in the natural course of bundling the package, e.g. when submitting to CRAN, and we recommend that you just let that happen.\nThere are many other DESCRIPTION fields that are used less frequently. A complete list can be found in the “The DESCRIPTION file” section of Writing R Extensions."
  },
  {
    "objectID": "description.html#sec-description-custom-fields",
    "href": "description.html#sec-description-custom-fields",
    "title": "10  DESCRIPTION",
    "section": "\n10.8 Custom fields",
    "text": "10.8 Custom fields\nThere is also some flexibility to create your own fields to add additional metadata. In the narrowest sense, the only restriction is that you shouldn’t re-purpose the official field names used by R. You should also limit yourself to valid English words, so the field names aren’t flagged by the spell-check.\nIn practice, if you plan to submit to CRAN, we recommend that any custom field name should start with Config/. We’ll revisit this later when we explain how Config/Needs/website is used to record additional packages needed to build a package’s website (Section 12.7).\nYou might notice that create_package() writes two more fields we haven’t discussed yet, relating to the use of the roxygen2 package for documentation:\nRoxygen: list(markdown = TRUE)\nRoxygenNote: 7.2.1\nYou will learn more about these in Chapter 17. The use of these specific field names is basically an accident of history and, if it were re-done today, they would follow the Config/* pattern recommended above."
  },
  {
    "objectID": "dependencies-mindset-background.html#sec-dependencies-pros-cons",
    "href": "dependencies-mindset-background.html#sec-dependencies-pros-cons",
    "title": "11  Dependencies: Mindset and Background",
    "section": "\n11.1 When should you take a dependency?",
    "text": "11.1 When should you take a dependency?\nThis section is adapted from the “It Depends” blog post and talk authored by Jim Hester.\nSoftware dependencies are a double-edged sword. On one hand, they let you take advantage of others’ work, giving your software new capabilities and making its behaviour and interface more consistent with other packages. By using a pre-existing solution, you avoid re-implementing functionality, which eliminates many opportunities for you to introduce bugs. On the other hand, your dependencies will likely change over time, which could require you to make changes to your package, potentially increasing your maintenance burden. Your dependencies can also increase the time and disk space needed when users install your package. These downsides have led some to suggest a ‘dependency zero’ mindset. We feel that this is bad advice for most projects, and is likely to lead to lower functionality, increased maintenance, and new bugs.\n\n11.1.1 Dependencies are not equal\nOne problem with simply minimizing the absolute number of dependencies is that it treats all dependencies as equivalent, as if they all have the same costs and benefits (or even, infinite cost and no benefit). However, in reality, this is far from the truth. There are many axes upon which dependencies can differ, but some of the most important include:\n\nThe type of the dependency. Some dependencies come bundled with R itself (e.g. base, utils, stats) or are one of the ‘Recommended’ packages (e.g. Matrix, survival). These packages are very low cost to depend on, as they are (nearly) universally installed on all users’ systems, and mostly change only with new R versions. In contrast, there is a higher cost for a dependency that comes from, e.g., a non-CRAN repository, which requires users to configure additional repositories before installation.\nThe number of upstream dependencies, i.e. recursive dependencies. For example, the rlang package is intentionally managed as a low-level package and has no upstream dependencies apart from R itself. At the other extreme, there are packages on CRAN with ~250 recursive dependencies.\nAlready fulfilled dependencies. If your package depends on dplyr then taking a dependency on tibble does not change the dependency footprint, as dplyr itself already depends on tibble. Additionally, some of the most popular packages (e.g. ggplot2) will already be installed on the majority of users’ machines. So adding a ggplot2 dependency is unlikely to incur additional installation costs in most cases.\n\nThe burden of installing the package. Various factors make a package more costly to install, in terms of time, space, and human aggravation:\n\nTime to compile: Packages that contain C/C++ can take very different amounts of time to install depending on the complexity of the code. For example, the glue package takes ~5 seconds to compile on CRAN’s build machines, whereas the readr package takes ~100 seconds to install on the same machines.\nBinary package size: Users installing binary packages need to download them, so the size of the binary is relevant, particularly for those with slow internet connections. This also varies a great deal across packages. The smallest packages on CRAN are around 1 Kb in size, while the h2o package is 170 Mb, and there are Bioconductor binaries that are over 4 Gb!\nSystem requirements: Some packages require additional system dependencies in order to be used. For instance, the rjags package requires a matching installation of the JAGS library. Another example is rJava which requires a Java SDK and also has additional steps needed to configure R for the proper Java installation, which has caused installation issues for many people.\n\n\nMaintenance capacity. It is reasonable to have higher confidence in a package that is well-established and that is maintained by developers or teams with a long track record and that maintain many other packages. This increases the likelihood that the package will remain on CRAN without interruptions and that the maintainer has an intentional approach to the software life cycle (Chapter 22).\nFunctionality. Some packages implement a critical piece of functionality that is used across many packages. In the tidyverse, broadly defined, the rlang, tidyselect, vctrs, and tibble packages are all examples of this. By using these packages for tricky tasks like non-standard evaluation or manipulation of vectors and data frames, package authors can avoid re-implementing basic functionality. It’s easy to think “how hard can it be to write my own X?” when you are focused on the Happy Path1. But a huge part of the value brought by packages like vctrs or tibble is letting someone else worry about edge cases and error handling2 . There is also value in having shared behaviour with other packages, e.g. the tidyverse rules for name repair or recycling.\n\nThe specifics above hopefully make it clear that package dependencies are not equal.\n\n11.1.2 Prefer a holistic, balanced, and quantitative approach\nInstead of striving for a minimal number of dependencies, we recommend a more holistic, balanced, and quantitative approach.\nA holistic approach looks at the project as a whole and asks “who is the primary audience?”. If the audience is other package authors, then a leaner package with fewer dependencies may be more appropriate. If, instead, the target user is a data scientist or statistician, they will likely already have many popular dependencies installed and would benefit from a more feature-full package.\nA balanced approach understands that adding (or removing) dependencies comes with trade-offs. Adding a dependency gives you additional features, bug fixes, and real-world testing, at the cost of increased installation time, disk space and maintenance, if the dependency has breaking changes. In some cases it makes sense to increase dependencies for a package, even if an implementation already exists. For instance, base R has a number of different implementations of non-standard evaluation with varying semantics across its functions. The same used to be true of tidyverse packages as well, but now they all depend on the implementations in the tidyselect and rlang packages. Users benefit from the improved consistency of this feature and individual package developers can let the maintainers of tidyselect and rlang worry about the technical details.\nIn contrast, removing a dependency lowers installation time, disk space, and avoids potential breaking changes. However, it means your package will have fewer features or that you must re-implement them yourself. That, in turn, takes development time and introduces new bugs. One advantage of using an existing solution is that you’ll get the benefit of all the bugs that have already been discovered and fixed. Especially if the dependency is relied on by many other packages, this is a gift that keeps on giving.\nSimilar to optimizing performance, if you are worried about the burden of dependencies, it makes sense to address those concerns in a specific and quantitative way. The experimental itdepends package was created for the talk and blog post this section is based on. It is still a useful source of concrete ideas (and code) for analyzing how heavy a dependency is. The pak package also has several functions that are useful for dependency analysis:\n\npak::pkg_deps_tree(\"tibble\")\n#> tibble 3.1.8 ✨\n#> ├─fansi 1.0.3 ✨\n#> ├─lifecycle 1.0.3 ✨\n#> │ ├─cli 3.4.1 ✨ ⬇ (1.28 MB)\n#> │ ├─glue 1.6.2 ✨\n#> │ └─rlang 1.0.6 ✨ ⬇ (1.81 MB)\n#> ├─magrittr 2.0.3 ✨\n#> ├─pillar 1.8.1 ✨ ⬇ (673.95 kB)\n#> │ ├─cli\n#> │ ├─fansi\n#> │ ├─glue\n#> │ ├─lifecycle\n#> │ ├─rlang\n#> │ ├─utf8 1.2.2 ✨\n#> │ └─vctrs 0.5.1 ✨ ⬇ (1.82 MB)\n#> │   ├─cli\n#> │   ├─glue\n#> │   ├─lifecycle\n#> │   └─rlang\n#> ├─pkgconfig 2.0.3 ✨\n#> ├─rlang\n#> └─vctrs\n#>\n#> Key:  ✨ new |  ⬇ download\n\npak::pkg_deps_explain(\"tibble\", \"rlang\")\n#> tibble -> lifecycle -> rlang\n#> tibble -> pillar -> lifecycle -> rlang\n#> tibble -> pillar -> rlang\n#> tibble -> pillar -> vctrs -> lifecycle -> rlang\n#> tibble -> pillar -> vctrs -> rlang\n#> tibble -> rlang\n#> tibble -> vctrs -> lifecycle -> rlang\n#> tibble -> vctrs -> rlang\n\n\n11.1.3 Dependency thoughts specific to the tidyverse\nThe packages maintained by the tidyverse team play different roles in the ecosystem and are managed accordingly. For example, the tidyverse and devtools packages are essentially meta-packages that exist for the convenience of an end-user. Consequently, it is recommended that other packages should not depend on tidyverse3 or devtools (Section 3.2), i.e. these two packages should almost never appear in Imports. Instead, a package maintainer should identify and depend on the specific package that actually implements the desired functionality.\nIn the previous section, we talked about different ways to gauge the weight of a dependency. Both the tidyverse and devtools can be seen as heavy due to the very high number of recursive dependencies:\n\nn_hard_deps <- function(pkg) {\n  deps <- tools::package_dependencies(pkg, recursive = TRUE)\n  sapply(deps, length)\n}\n\nn_hard_deps(c(\"tidyverse\", \"devtools\"))\n#> tidyverse  devtools \n#>       114       101\n\nIn contrast, several packages are specifically conceived as low-level packages that implement features that should work and feel the same across the whole ecosystem. At the time of writing, this includes:\n\nrlang, to support tidy eval and throw errors\ncli and glue, for creating a rich user interface (which includes errors)\nwithr, for managing state responsibly\nlifecycle, for managing the life cycle of functions and arguments\n\nThese are basically regarded as free dependencies and can be added to DESCRIPTION via usethis::use_tidy_dependencies() (which also does a few more things). It should come as no surprise that these packages have a very small dependency footprint.\n\ntools::package_dependencies(c(\"rlang\", \"cli\", \"glue\", \"withr\", \"lifecycle\"))\n#> $rlang\n#> [1] \"utils\"\n#> \n#> $cli\n#> [1] \"utils\"\n#> \n#> $glue\n#> [1] \"methods\"\n#> \n#> $withr\n#> [1] \"graphics\"  \"grDevices\" \"stats\"    \n#> \n#> $lifecycle\n#> [1] \"cli\"   \"glue\"  \"rlang\"\n\n\n\n\n\n\n\nSubmitting to CRAN\n\n\n\nUnder certain configurations, including those used for incoming CRAN submissions, R CMD check issues a NOTE if there are 20 or more “non-default” packages in Imports:\nN  checking package dependencies (1.5s)\n   Imports includes 29 non-default packages.\n   Importing from so many packages makes the package vulnerable to any of\n   them becoming unavailable.  Move as many as possible to Suggests and\n   use conditionally.\nOur best advice is to try hard to comply, as it should be rather rare to need so many dependencies and it’s best to eliminate any NOTE that you can. Of course, there are exceptions to every rule and perhaps your package is one them. In that case, you may need to argue your case. It is certainly true that many CRAN packages violate this threshold.\n\n\n\n11.1.4 Whether to Import or Suggest\nThe withr package is a good case study for deciding whether to list a dependency in Imports or Suggests. Withr is very useful for writing tests that clean up after themselves. Such usage is compatible with listing withr in Suggests, since regular users don’t need to run the tests. But sometimes a package might also use withr in its own functions, perhaps to offer its own with_*() and local_*() functions. In that case, withr should be listed in Imports.\nImports and Suggests differ in the strength and nature of dependency:\n\n\nImports: packages listed here must be present for your package to work. Any time your package is installed, those packages will also be installed, if not already present. devtools::load_all() also checks that all packages in Imports are installed.\nIt’s worth pointing out that adding a package to Imports ensures it will be installed and that is all it does. It has nothing to do with actually importing functions from that package. See Section 12.4 for more about how to use a package in Imports.\n\n\nSuggests: your package can use these packages, but doesn’t require them. You might use suggested packages for example datasets, to run tests, build vignettes, or maybe there’s only one function that needs the package.\nPackages listed in Suggests are not automatically installed along with your package. This means that you can’t assume that your users have installed all the suggested packages, but you can assume that developers have. See Section 12.5 for how to check whether a suggested package is installed.\n\n\nSuggests isn’t terribly relevant for packages where the user base is approximately equal to the development team or for packages that are used in a very predictable context. In that case, it’s reasonable to just use Imports for everything. Using Suggests is mostly a courtesy to external users or to accommodate very lean installations. It can free users from downloading rarely needed packages (especially those that are tricky to install) and lets them get started with your package as quickly as possible."
  },
  {
    "objectID": "dependencies-mindset-background.html#sec-dependencies-namespace",
    "href": "dependencies-mindset-background.html#sec-dependencies-namespace",
    "title": "11  Dependencies: Mindset and Background",
    "section": "\n11.2 Namespace",
    "text": "11.2 Namespace\nSo far, we’ve explained the mechanics of declaring a dependency in DESCRIPTION (Section 10.6) and how to analyze the costs and benefits of dependencies (Section 11.1). Before we explain how to use your dependencies in various parts of your package in Chapter 12, we need to establish the concepts of a package namespace and the search path.\n\n11.2.1 Motivation\nAs the name suggests, namespaces provide “spaces” for “names”. They provide a context for looking up the value of an object associated with a name.\nWithout knowing it, you’ve probably already used namespaces. Have you ever used the :: operator? It disambiguates functions with the same name. For example, both the lubridate and here packages provide a here() function. If you attach lubridate, then here, here() will refer to the here version, because the last package attached wins. But if you attach the packages in the opposite order, here() will refer to the lubridate version.\n\nlibrary(lubridate)    |  library(here)\nlibrary(here)         |  library(lubridate)\n\nhere() # here::here() |  here() # lubridate::here()\n\nThis can be confusing. Instead, you can qualify the function call with a specific namespace: lubridate::here() and here::here(). Then the order in which the packages are attached won’t matter4.\n\nlubridate::here() # always gets lubridate::here()\nhere::here()      # always gets here::here()\n\nAs you will see in Section 12.4, the package::function() calling style is also our default recommendation for how to use your dependencies in the code below R/, because it eliminates all ambiguity.\nBut, in the context of package code, the use of :: is not really our main line of defense against the confusion seen in the example above. In packages, we rely on namespaces to ensure that every package works the same way regardless of what packages are attached by the user.\nConsider the sd() function from the stats package that is part of base R:\n\nsd\n#> function (x, na.rm = FALSE) \n#> sqrt(var(if (is.vector(x) || is.factor(x)) x else as.double(x), \n#>     na.rm = na.rm))\n#> <bytecode: 0x557ef44ab258>\n#> <environment: namespace:stats>\n\nIt’s defined in terms of another function, var(), also from the stats package. So what happens if we override var() with our own definition? Does it break sd()?\n\nvar <- function(x) -5\nvar(1:5)\n#> [1] -5\n\nsd(1:5)\n#> [1] 1.58\n\nSurprisingly, it does not! That’s because when sd() looks for an object called var(), it looks first in the stats package namespace, so it finds stats::var(), not the var() we created in the global environment. It would be chaos if functions like sd() could be broken by a user redefining var() or by attaching a package that overrides var(). The package namespace system is what saves us from this fate.\n\n11.2.2 The NAMESPACE file\nThe NAMESPACE file plays a key role in defining your package’s namespace. Here are selected lines from the NAMESPACE file in the testthat package:\n# Generated by roxygen2: do not edit by hand\n\nS3method(compare,character)\nS3method(print,testthat_results)\nexport(compare)\nexport(expect_equal)\nimport(rlang)\nimportFrom(brio,readLines)\nuseDynLib(testthat, .registration = TRUE)\nThe first line announces that this file is not written by hand, but rather is generated by the roxygen2 package. We’ll return to this topic soon, after we discuss the remaining lines.\nYou can see that the NAMESPACE file looks a bit like R code (but it is not). Each line contains a directive: S3method(), export(), importFrom(), and so on. Each directive describes an R object, and says whether it’s exported from this package to be used by others, or it’s imported from another package to be used internally.\nThese directives are the most important in our development approach, in order of frequency:\n\n\nexport(): export a function (including S3 and S4 generics).\n\nS3method(): export an S3 method.\n\nimportFrom(): import selected object from another namespace (including S4 generics).\n\nimport(): import all objects from another package’s namespace.\n\nuseDynLib(): registers routines from a DLL (this is specific to packages with compiled code).\n\nThere are other directives that we won’t cover here, because they are explicitly discouraged or they just rarely come up in our development work.\n\n\nexportPattern(): exports all functions that match a pattern. We feel it’s safer to always use explicit exports and we avoid the use of this directive.\n\nexportClasses(), exportMethods(), importClassesFrom(), importMethodsFrom(): export and import S4 classes and methods. We only work in the S4 system when necessary for compatibility with another package, i.e. we generally don’t implement methods or classes that we own with S4. Therefore the S4 coverage in this book is very minimal.\n\nIn the devtools workflow, the NAMESPACE file is not written by hand! Instead, we prefer to generate NAMESPACE with the roxygen2 package, using specific tags located in a roxygen comment above each function’s definition in the R/*.R files (Section 12.3). We will have much more to say about roxygen comments and the roxygen2 package when we discuss package documentation in Chapter 17. For now, we just lay out the reasons we prefer this method of generating the NAMESPACE file:\n\nNamespace tags are integrated into the source code, so when you read the code it’s easier to see what’s being exported and imported and why.\nRoxygen2 abstracts away some of the details of NAMESPACE. You only need to learn one tag, @export, and roxygen2 will figure out which specific directive to use, based on whether the associated object is a regular function, S3 method, S4 method, or S4 class.\nRoxygen2 keeps NAMESPACE tidy. No matter how many times @importFrom foo bar appears in your roxygen comments, you’ll only get one importFrom(foo, bar) in your NAMESPACE. Roxygen2 also keeps NAMESPACE organised in a principled order, sorting first by the directive type and then alphabetically. Roxygen2 takes away the burden of writing NAMESPACE, while also trying to keep the file as readable as possible. This organization also makes Git diffs much more informative.\n\nNote that you can choose to use roxygen2 to generate just NAMESPACE, just man/*.Rd (Chapter 17), or both (as is our practice). If you don’t use any namespace related tags, roxygen2 won’t touch NAMESPACE. If you don’t use any documentation related tags, roxygen2 won’t touch man/."
  },
  {
    "objectID": "dependencies-mindset-background.html#sec-dependencies-search",
    "href": "dependencies-mindset-background.html#sec-dependencies-search",
    "title": "11  Dependencies: Mindset and Background",
    "section": "\n11.3 Search path",
    "text": "11.3 Search path\nTo understand why namespaces are important, you need a solid understanding of search paths. To call a function, R first has to find it. This search unfolds differently for user code than for package code and that is because of the namespace system.\n\n11.3.1 Function lookup for user code\nThe first place R looks for an object is the global environment. If R doesn’t find it there, it looks in the search path, the list of all the packages you have attached. You can see this list by running search(). For example, here’s the search path for the code in this book:\n\nsearch()\n#>  [1] \".GlobalEnv\"        \"tools:quarto\"      \"tools:quarto\"     \n#>  [4] \"package:stats\"     \"package:graphics\"  \"package:grDevices\"\n#>  [7] \"package:utils\"     \"package:datasets\"  \"package:methods\"  \n#> [10] \"Autoloads\"         \"package:base\"\n\nThis has a specific form (see Figure 11.1):\n\nThe global environment.\nThe packages that have been attached, e.g. via library(), from most-recently attached to least.\n\nAutoloads, a special environment that uses delayed bindings to save memory by only loading package objects (like big datasets) when needed.\nThe base environment, by which we mean the package environment of the base package.\n\n\n\n\n\nFigure 11.1: Typical state of the search path.\n\n\n\n\nEach element in the search path has the next element as its parent, i.e. this is a chain of environments that is searched in order. In the diagram, this relationship is shown as a small blue circle with an arrow that points to the parent. The first environment (the global environment) and the last two (Autoloads and the base environment) are special and maintain their position.\nBut the middle section of attached packages is more dynamic. When a new package is attached, it is inserted right after and becomes the parent of the global environment. When you attach another package with library(), it changes the search path, as show in Figure 11.2:\n\n\n\n\nFigure 11.2: A newly attached package is inserted into the search path.\n\n\n\n\nThe main gotcha around how the user’s search path works is the scenario we explored in Section 11.2.1, where two packages (lubridate and here) offer competing functions by the same name (here()). It should be very clear now why a user’s call to here() can produce a different result, depending on the order in which they attached the two packages.\nThis sort of confusion would be even more damaging if it applied to package code, but luckily it does not. Now we can explain how the namespace system designs this problem away.\n\n11.3.2 Function lookup inside a package\nIn Section 11.2.1, we proved that a user’s definition of a function named var() does not break stats::sd(). Somehow, to our immense relief, stats::sd() finds stats::var() when it should. How does that work?\nThis section is somewhat technical and you can absolutely develop a package with a well-behaved namespace without fully understanding these details. Consider this optional reading that you can consult when and if you’re interested. You can learn even more in Advanced R, especially in the chapter on environments, from which we have adapted some of this material.\nEvery function in a package is associated with a pair of environments: the package environment, which is what appears in the user’s search path, and the namespace environment.\n\nThe package environment is the external interface to the package. It’s how a regular R user finds a function in an attached package or with ::. Its parent is determined by search path, i.e. the order in which packages have been attached. The package environment only exposes exported objects.\nThe namespace environment is the internal interface of the package. It includes all objects in the package, both exported and non-exported. This ensures that every function can find every other function in the package. Every binding in the package environment also exists in the namespace environment, but not vice versa.\n\nFigure 11.3 diagram depicts the sd() function as a rectangle with a rounded end. The arrows from package:stats and namespace:stats show that sd() is bound in both. But the relationship is not symmetric. The black circle with an arrow pointing back to namespace:stats indicates where sd() will look for objects that it needs: in the namespace environment, not the package environment.\n\n\n\n\nFigure 11.3: An exported function is bound in the package environment and in the namespace, but only binds the namespace.\n\n\n\n\n\nThe package environment controls how users find the function; the namespace controls how the function finds its variables.\n\nEvery namespace environment has the same set of ancestors, as depicted in Figure 11.4:\n\nEach namespace has an imports environment that can contain bindings to functions used by the package that are defined in another package. The imports environment is controlled by the package developer with the NAMESPACE file. Specifically, directives such as importFrom() and imports() populate this environment.\nExplicitly importing every base function would be tiresome, so the parent of the imports environment is the base namespace. The base namespace contains the same bindings as the base environment, but it has a different parent.\nThe parent of the base namespace is the global environment. This means that if a binding isn’t defined in the imports environment the package will look for it in the usual way. This is usually a bad idea (because it makes code depend on other loaded packages), so R CMD check automatically warns about such code. It is needed primarily for historical reasons, particularly due to how S3 method dispatch works.\n\n\n\n\n\nFigure 11.4: The namespace environment has the imports environment as parent, which inherits from the namespace environment of the base package and, ultimately, the global environment.\n\n\n\n\nFinally, we can put it all together in this last diagram, Figure 11.5. This shows the user’s search path, along the bottom, and the internal stats search path, along the top.\n\n\n\n\nFigure 11.5: For user code, objects are found using the search path, whereas package code uses the namespace.\n\n\n\n\nA user (or some package they are using) is free to define a function named var(). But when that user calls sd(), it will always call stats::var() because sd() searches in a sequence of environments determined by the stats package, not by the user. This is how the namespace system ensures that package code always works the same way, regardless of what’s been defined in the global environment or what’s been attached."
  },
  {
    "objectID": "dependencies-mindset-background.html#sec-dependencies-attach-vs-load",
    "href": "dependencies-mindset-background.html#sec-dependencies-attach-vs-load",
    "title": "11  Dependencies: Mindset and Background",
    "section": "\n11.4 Attaching versus loading",
    "text": "11.4 Attaching versus loading\nIt’s common to hear something like “we use library(somepackage) to load somepackage”. But technically library() attaches a package to the search path. This casual abuse of terminology is often harmless and can even be beneficial in some settings. But sometimes it’s important to be precise and pedantic and this is one of those times. Package developers need to know the difference between attaching and loading a package and when to care about this difference.\nIf a package is installed,\n\nLoading will load code, data, and any DLLs; register S3 and S4 methods; and run the .onLoad() function. After loading, the package is available in memory, but because it’s not in the search path, you won’t be able to access its components without using ::. Confusingly, :: will also load a package automatically if it isn’t already loaded.\nAttaching puts the package in the search path (Section 11.3.1). You can’t attach a package without first loading it, so both library() (or require()) load then attach the package. This also runs the .onAttach() function.\n\nThere are four functions that make a package available, shown in Table 11.1. They differ based on whether they load or attach, and what happens if the package is not found (i.e., throws an error or returns FALSE).\n\n\nTable 11.1: Functions that load or attach a package.\n\n\n\n\n\n\n\nThrows error\nReturns FALSE\n\n\n\n\nLoad\nloadNamespace(\"x\")\nrequireNamespace(\"x\", quietly = TRUE)\n\n\nAttach\nlibrary(x)\nrequire(x, quietly = TRUE)\n\n\n\n\nOf the four, these two functions are by far the most useful:\n\nUse library(x) in, e.g., a data analysis script or a vignette. It will throw an error if the package is not installed, and will terminate the script. You want to attach the package to save typing. Never use library() in package code below R/ or tests/.\nUse requireNamespace(\"x\", quietly = TRUE) inside a package if you want to specify different behaviour depending on whether or not a suggested package is installed. Note that this also loads the package. We give examples in Section 12.5.1.\n\nloadNamespace() is somewhat esoteric and is really only needed for internal R code.\nrequire(pkg) is almost never a good idea5 and, we suspect, may come from people projecting certain hopes and dreams onto the function name. Ironically, require(pkg) does not actually require success in attaching pkg and your function or script will soldier on even in the case of failure. This, in turn, often leads to a very puzzling error much later. If you want to “attach or fail”, use library(). If you want to check whether pkg is available and proceed accordingly, use requireNamespace(\"pkg\", quietly = TRUE).\nOne reasonable use of require() is in an example that uses a package your package Suggests, which is further discussed in Section 12.5.3.\nThe .onLoad() and .onAttach() functions mentioned above are two of several hooks that allow you to run specific code when your package is loaded or attached (or, even, detached or unloaded). Most packages don’t need this, but these hooks are useful in certain situations. See Section 7.5.4 for some use cases for .onLoad() and .onAttach().\n\n11.4.1 Whether to Import or Depend\nWe are now in a position to lay out the difference between between Depends and Imports in the DESCRIPTION. Listing a package in either Depends or Imports ensures that it’s installed when needed. The main difference is that a package you list in Imports will just be loaded when you use it, whereas a package you list in Dependswill be attached when your package is attached.\nUnless there is a good reason otherwise, you should always list packages in Imports not Depends. That’s because a good package is self-contained, and minimises changes to the global landscape, including the search path.6\nUsers of devtools are actually regularly exposed to the fact that devtools Depends on usethis:\n\nlibrary(devtools)\n#> Loading required package: usethis\n\nsearch()\n#>  [1] \".GlobalEnv\"        \"package:devtools\"  \"package:usethis\"  \n#>  ...\n\nThis choice is motivated by backwards compatibility. When devtools was split into several smaller packages (Section 3.2), many of the user-facing functions moved to usethis. Putting usethis in Depends was a pragmatic choice to insulate users from keeping track of which function ended up where.\nA more classic example of Depends is how the censored package depends on the parsnip and survival packages. Parsnip provides a unified interface for fitting models, and censored is an extension package for survival analysis. Censored is not useful without parsnip and survival, so it makes sense to list them in Depends."
  },
  {
    "objectID": "dependencies-in-practice.html#confusion-about-imports",
    "href": "dependencies-in-practice.html#confusion-about-imports",
    "title": "12  Dependencies: In Practice",
    "section": "\n12.1 Confusion about Imports\n",
    "text": "12.1 Confusion about Imports\n\nLet’s make this crystal clear:\n\nListing a package in Imports in DESCRIPTION does not “import” that package.\n\nIt is natural to assume that listing a package in Imports actually “imports” the package, but this is just an unfortunate choice of name for the Imports field. The Imports field makes sure that the packages listed there are installed when your package is installed. It does not make those functions available to you, e.g. below R/, or to your user.\nIt is neither automatic nor necessarily advisable that a package listed in Imports also appears in NAMESPACE via imports() or importFrom(). It is common for a package to be listed in Imports in DESCRIPTION, but not in NAMESPACE. The converse is not true. Every package mentioned in NAMESPACE must also be present in the Imports or Depends fields."
  },
  {
    "objectID": "dependencies-in-practice.html#conventions-for-this-chapter",
    "href": "dependencies-in-practice.html#conventions-for-this-chapter",
    "title": "12  Dependencies: In Practice",
    "section": "\n12.2 Conventions for this chapter",
    "text": "12.2 Conventions for this chapter\nSometimes our examples can feature real functions from real packages. But if we need to talk about a generic package or function, here are the conventions we use below:\n\npkg: the name of your hypothetical package\naaapkg or bbbpkg: the name of a hypothetical package your package depends on\naaa_fun(): the name of a function exported by aaapkg"
  },
  {
    "objectID": "dependencies-in-practice.html#sec-dependencies-NAMESPACE-workflow",
    "href": "dependencies-in-practice.html#sec-dependencies-NAMESPACE-workflow",
    "title": "12  Dependencies: In Practice",
    "section": "\n12.3 NAMESPACE Workflow",
    "text": "12.3 NAMESPACE Workflow\nIn the sections below, we give practical instructions on how (and when) to import functions from another package into yours and how to export functions from your package. The file that keeps track of all this is the NAMESPACE file (more details in Section 11.2.2).\nIn the devtools workflow and this book, we generate the NAMESPACE file from special comments in the R/*.R files. Since the package that ultimately does this work is roxygen2, these are called “roxygen comments”. These roxygen comments are also the basis for your package’s help topics, which is covered in Section 17.1.1.\nThe NAMESPACE file starts out with a single commented-out line explaining the situation (and hopefully discouraging any manual edits):\n# Generated by roxygen2: do not edit by hand\nAs you incorporate roxygen tags to export and import functions, you need to re-generate the NAMESPACE file periodically. Here is the general workflow for regenerating NAMESPACE (and your documentation):\n\n\nAdd namespace-related tags to the roxygen comments in your R/*.R files. This is an artificial example, but it gives you the basic idea:\n\n#' @importFrom aaapkg aaa_fun\n#' @import bbbpkg\n#' @export\nfoo <- function(x, y, z) {\n  ...\n}\n\n\n\nRun devtools::document() (or press Ctrl/Cmd + Shift + D in RStudio) to “document” your package. By default, two things happen:\n\nThe help topics in the man/*.Rd files are updated (covered in Chapter 17).\n\nThe NAMESPACE file is re-generated. In our example, the NAMESPACE file would look like:\n# Generated by roxygen2: do not edit by hand\n\nexport(foo)\nimport(bbbpkg)\nimportFrom(aaapkg,aaa_fun)\n\n\n\n\nRoxygen2 is quite smart and will insert the appropriate directive in NAMESPACE, i.e. it can usually determine whether to use export() or S3method().\n\n\n\n\n\n\nRStudio\n\n\n\nPress Ctrl/Cmd + Shift + D, to generate your package’s NAMESPACE (and man/*.Rd files). This is also available via Document in the Build menu and pane."
  },
  {
    "objectID": "dependencies-in-practice.html#sec-dependencies-in-imports",
    "href": "dependencies-in-practice.html#sec-dependencies-in-imports",
    "title": "12  Dependencies: In Practice",
    "section": "\n12.4 Package is listed in Imports\n",
    "text": "12.4 Package is listed in Imports\n\nConsider a dependency that is listed in DESCRIPTION in Imports:\nImports:\n    aaapkg\nThe code inside your package can assume that aaapkg is installed whenever pkg is installed.\n\n12.4.1 In code below R/\nOur recommended default is to call external functions using the package::function() syntax:\n\nsomefunction <- function(...) {\n  ...\n  x <- aaapkg::aaa_fun(...)\n  ...\n}\n\nSpecifically, we recommend that you default to not importing anything from aaapkg into your namespace. This makes it very easy to identify which functions live outside of your package, which is especially useful when you read your code in the future. This also eliminates any concerns about name conflicts between aaapkg and your package.\nOf course there are reasons to make exceptions to this rule and to import something from another package into yours:\n\nAn operator: You can’t call an operator from another package via ::, so you must import it. Example: the null-coalescing operator %||% from rlang or the original pipe %>% from magrittr.\nA function that you use a lot: If importing a function makes your code much more readable, that’s a good enough reason to import it. This literally reduces the number of characters required to call the external function. This can be especially handy when generating user-facing messages, because it makes it more likely that lines in the source correspond to lines in the output.\nA function that you call in a tight loop: There is a minor performance penalty associated with ::. It’s on the order of 100ns, so it will only matter if you call the function millions of times.\n\nA handy function for your interactive workflow is usethis::use_import_from():\n\nusethis::use_import_from(\"glue\", \"glue_collapse\")\n\nThe call above writes this roxygen tag into the source code of your package:\n\n#' @importFrom glue glue_collapse\n\nWhere should this roxygen tag go? There are two reasonable locations:\n\nAs close as possible to the usage of the external function. With this mindset, you would place @importFrom in the roxygen comment above the function in your package where you use the external function. If this is your style, you’ll have to do it by hand. We have found that this feels natural at first, but starts to break down as you use more external functions in more places.\n\nIn a central location. This approach keeps all @importFrom tags together, in a dedicated section of the package-level documentation file (which can be created with usethis::use_package_doc(), Section 17.7). This is what use_import_from() implements. So, in R/pkg-package.R, you’ll end up with something like this:\n\n# The following block is used by usethis to automatically manage\n# roxygen namespace tags. Modify with care!\n## usethis namespace: start\n#' @importFrom glue glue_collapse\n## usethis namespace: end\nNULL\n#> NULL\n\n\n\nRecall that devtools::document() processes your roxygen comments (Section 12.3), which writes help topics to man/*.Rd and, relevant to our current goal, generates the NAMESPACE file. If you use use_import_from(), it does this for you and also calls load_all(), making the newly imported function available in your current session.\nThe roxygen tag above causes this directive to appear in the NAMESPACE file:\nimportFrom(glue, glue_collapse)\nNow you can use the imported function directly in your code:\n\nsomefunction <- function(...) {\n  ...\n  x <- glue_collapse(...)\n  ...\n}\n\nSometimes you make such heavy use of so many functions from another package that you want to import its entire namespace. This should be relatively rare. In the tidyverse, the package we most commonly treat this way is rlang, which functions almost like a base package for us.\nHere is the roxygen tag that imports all of rlang. This should appear somewhere in R/*.R, such as the dedicated space described above for collecting all of your namespace import tags.\n\n#' @import rlang\n\nAfter calling devtools::document(), this roxygen tag causes this directive to appear in the NAMESPACE file:\nimport(rlang)\nThis is the least recommended solution because it can make your code harder to read (you can’t tell where a function is coming from), and if you @import many packages, it increases the chance of function name conflicts. Save this for very special situations.\n\n12.4.1.1 How to not use a package in Imports\nSometimes you have a package listed in Imports, but you don’t actually use it inside your package or, at least, R doesn’t think you use it. That leads to a NOTE from R CMD check:\n* checking dependencies in R code ... NOTE\nNamespace in Imports field not imported from: ‘aaapkg’\n  All declared Imports should be used.\nThis can happen if you need to list an indirect dependency in Imports, perhaps to state a minimum version for it. The tidyverse meta-package has this problem on a large scale, since it exists mostly to install a bundle of packages at specific versions. Another scenario is when your package uses a dependency in such a way that requires another package that is only suggested by the direct dependency1. There are various situations where it’s not obvious that your package truly needs every package listed in Imports, but in fact it does.\nHow can you get rid of this NOTE?\nOur recommendation is to put a namespace-qualified reference (not a call) to an object in aaapkg in some file below R/, such as a .R file associated with package-wide setup:\n\nignore_unused_imports <- function() {\n  aaapkg::aaa_fun\n}\n\nYou don’t need to call ignore_unused_imports() anywhere. You shouldn’t export it. You don’t have to actually exercise aaapkg::aaa_fun(). What’s important is to access something in aaapkg’s namespace with ::.\nAn alternative approach you might be tempted to use is to import aaapkg::aaa_fun() into your package’s namespace, probably with the roxygen tag @importFrom aaapkg aaa_fun. This does suppress the NOTE, but it also does more. This causes aaapkg to be loaded whenever your package is loaded. In contrast, if you use the approach we recommend, the aaapkg will only be loaded if your user does something actually requires it. This rarely matters in practice, but it’s always nice to minimize or delay the loading of additional packages.\n\n12.4.2 In test code\nRefer to external functions in your tests just as you refer to them in the code below R/. Usually this means you should use aaapkg::aaa_fun(). But if you have imported a particular function, either specifically or as part of an entire namespace, you can just call it directly in your test code.\nIt’s generally a bad idea to use library(aaapkg) to attach one of your dependencies somewhere in your tests, because it makes the search path in your tests different from how your package actually works. This is covered in more detail in Section 15.2.5.\n\n12.4.3 In examples and vignettes\nIf you use a package that appears in Imports in one of your examples or vignettes, you’ll need to either attach the package with library(aaapkg) or use a aaapkg::aaa_fun()-style call. You can assume that aaapkg is available, because that’s what Imports guarantees. Read more in Section 17.5.4 and Section 18.4."
  },
  {
    "objectID": "dependencies-in-practice.html#sec-dependencies-in-suggests",
    "href": "dependencies-in-practice.html#sec-dependencies-in-suggests",
    "title": "12  Dependencies: In Practice",
    "section": "\n12.5 Package is listed in Suggests\n",
    "text": "12.5 Package is listed in Suggests\n\nConsider a dependency that is listed in DESCRIPTION in Suggests:\nSuggests:\n    aaapkg\nYou can NOT assume that every user has installed aaapkg (but you can assume that a developer has). Whether a user has aaapkg will depend on how they installed your package. Most of the functions that are used to install packages support a dependencies argument that controls whether to install just the hard dependencies or to take a more expansive approach, which includes suggested packages:\n\ninstall.packages(dependencies =)\nremotes::install_github(dependencies =)\npak::pkg_install(dependencies =)\n\nBroadly speaking, the default is to not install packages in Suggests.\n\n12.5.1 In code below R/\n\nInside a function in your own package, check for the availability of a suggested package with requireNamespace(\"aaapkg\", quietly = TRUE). There are two basic scenarios: the dependency is absolutely required or your package offers some sort of fallback behaviour.\n\n# the suggested package is required \nmy_fun <- function(a, b) {\n  if (!requireNamespace(\"aaapkg\", quietly = TRUE)) {\n    stop(\n      \"Package \\\"aaapkg\\\" must be installed to use this function.\",\n      call. = FALSE\n    )\n  }\n  # code that includes calls such as aaapkg::aaa_fun()\n}\n\n# the suggested package is optional; a fallback method is available\nmy_fun <- function(a, b) {\n  if (requireNamespace(\"aaapkg\", quietly = TRUE)) {\n    aaapkg::aaa_fun()\n  } else {\n    g()\n  }\n}\n\nThe rlang package has some useful functions for checking package availability: rlang::check_installed() and rlang::is_installed(). Here’s how the checks around a suggested package could look if you use rlang:\n\n# the suggested package is required \nmy_fun <- function(a, b) {\n  rlang::check_installed(\"aaapkg\", reason = \"to use `aaa_fun()`\")\n  # code that includes calls such as aaapkg::aaa_fun()\n}\n\n# the suggested package is optional; a fallback method is available\nmy_fun <- function(a, b) {\n  if (rlang::is_installed(\"aaapkg\")) {\n    aaapkg::aaa_fun()\n  } else {\n    g()\n  }\n}\n\nThese rlang functions have handy features for programming, such as vectorization over pkg, classed errors with a data payload, and, for check_installed(), an offer to install the needed package in an interactive session.\n\n12.5.2 In test code\nThe tidyverse team generally writes tests as if all suggested packages are available. That is, we use them unconditionally in the tests.\nThe motivation for this posture is self-consistency and pragmatism. The key package needed to run tests is testthat and it appears in Suggests, not in Imports or Depends. Therefore, if the tests are actually executing, that implies that an expansive notion of package dependencies has been applied.\nAlso, empirically, in every important scenario of running R CMD check, the suggested packages are installed. This is generally true for CRAN and we ensure that it’s true in our own automated checks. However, it’s important to note that other package maintainers take a different stance and choose to protect all usage of suggested packages in their tests and vignettes.\nSometimes even we make an exception and guard the use of a suggested package in a test. Here’s a test from ggplot2, which uses testthat::skip_if_not_installed() to skip execution if the suggested sf package is not available.\n\ntest_that(\"basic plot builds without error\", {\n  skip_if_not_installed(\"sf\")\n\n  nc_tiny_coords <- matrix(\n    c(-81.473, -81.741, -81.67, -81.345, -81.266, -81.24, -81.473,\n      36.234, 36.392, 36.59, 36.573, 36.437, 36.365, 36.234),\n    ncol = 2\n  )\n\n  nc <- sf::st_as_sf(\n    data_frame(\n      NAME = \"ashe\",\n      geometry = sf::st_sfc(sf::st_polygon(list(nc_tiny_coords)), crs = 4326)\n    )\n  )\n\n  expect_doppelganger(\"sf-polygons\", ggplot(nc) + geom_sf() + coord_sf())\n})\n\nWhat might justify the use of skip_if_not_installed()? In this case, the sf package can be nontrivial to install and it is conceivable that a contributor would want to run the remaining tests, even if sf is not available.\nFinally, note that testthat::skip_if_not_installed(pkg, minimum_version = \"x.y.z\") can be used to conditionally skip a test based on the version of the other package.\n\n12.5.3 In examples and vignettes\nAnother common place to use a suggested package is in an example and here we often guard with require() or requireNamespace(). This example is from ggplot2::coord_map(). ggplot2 lists the maps package in Suggests.\n\n#' @examples\n#' if (require(\"maps\")) {\n#'   nz <- map_data(\"nz\")\n#'   # Prepare a map of NZ\n#'   nzmap <- ggplot(nz, aes(x = long, y = lat, group = group)) +\n#'     geom_polygon(fill = \"white\", colour = \"black\")\n#'  \n#'   # Plot it in cartesian coordinates\n#'   nzmap\n#' }\n\nAn example is basically the only place where we would use require() inside a package. Read more in Section 11.4.\nOur stance regarding the use of suggested packages in vignettes is similar to that for tests. The key packages needed to build vignettes (rmarkdown and knitr) are listed in Suggests. Therefore, if the vignettes are being built, it’s reasonable to assume that all of the suggested packages are available. We typically use suggested packages unconditionally inside vignettes.\nBut if you choose to use suggested packages conditionally in your vignettes, the knitr chunk option eval is very useful for achieving this. See Section 18.4 for more."
  },
  {
    "objectID": "dependencies-in-practice.html#sec-dependencies-in-depends",
    "href": "dependencies-in-practice.html#sec-dependencies-in-depends",
    "title": "12  Dependencies: In Practice",
    "section": "\n12.6 Package is listed in Depends\n",
    "text": "12.6 Package is listed in Depends\n\nConsider a dependency that is listed in DESCRIPTION in Depends:\nDepends:\n    aaapkg\nThis situation has a lot in common with a package listed in Imports. The code inside your package can assume that aaapkg is installed on the system. The only difference is that aaapkg will be attached whenever your package is.\n\n12.6.1 In code below R/ and in test code\nYour options are exactly the same as using functions from a package listed in Imports:\n\nUse the aaapkg::aaa_fun() syntax.\nImport an individual function with the @importFrom aaapkg aaa_fun roxygen tag and call aaa_fun() directly.\nImport the entire aaapkg namespace with the @import aaapkg roxygen tag and call any function directly.\n\nThe main difference between this situation and a dependency listed in Imports is that it’s much more common to import the entire namespace of a package listed in Depends. This often makes sense, due to the special dependency relationship that motivated listing it in Depends in the first place.\n\n12.6.2 In examples and vignettes\nThis is the most obvious difference with a dependency in Depends versus Imports. Since your package is attached when your examples are executed, so is the package listed in Depends. You don’t have to attach it explicitly with library(aaapkg).\nThe ggforce package Depends on ggplot2 and the examples for ggforce::geom_mark_rect() use functions like ggplot2::ggplot() and ggplot2::geom_point() without any explicit call to library(ggplot2):\n\nggplot(iris, aes(Petal.Length, Petal.Width)) +\n  geom_mark_rect(aes(fill = Species, filter = Species != 'versicolor')) +\n  geom_point()\n# example code continues ...\n\nThe first line of code executed in one of your vignettes is probably library(pkg), which attaches your package and, as a side effect, attaches any dependency listed in Depends. You do not need to explicitly attach the dependency before using it. The censored package Depends on the survival package and the code in vignette(\"examples\", package = \"censored\") starts out like so:\n\nlibrary(tidymodels)\nlibrary(censored)\n#> Loading required package: survival\n\n# vignette code continues ..."
  },
  {
    "objectID": "dependencies-in-practice.html#sec-dependencies-nonstandard",
    "href": "dependencies-in-practice.html#sec-dependencies-nonstandard",
    "title": "12  Dependencies: In Practice",
    "section": "\n12.7 Package is a nonstandard dependency",
    "text": "12.7 Package is a nonstandard dependency\nIn packages developed with devtools, you may see DESCRIPTION files that use a couple other nonstandard fields for package dependencies specific to development tasks.\n\n12.7.1 Depending on the development version of a package\nThe Remotes field can be used when you need to install a dependency from a nonstandard place, i.e. from somewhere besides CRAN or Bioconductor. One common example of this is when you’re developing against a development version of one of your dependencies. During this time, you’ll want to install the dependency from its development repository, which is often GitHub. The way to specify various remote sources is described in a devtools vignette and in a pak help topic.\nThe dependency and any minimum version requirement still need to be declared in the normal way in, e.g., Imports. usethis::use_dev_package() helps to make the necessary changes in DESCRIPTION. If your package temporarily relies on a development version of aaapkg, the affected DESCRIPTION fields might evolve like this:\n\nStable -->               Dev -->                       Stable again\n----------------------   ---------------------------   ----------------------\nPackage: pkg             Package: pkg                  Package: pkg\nVersion: 1.0.0           Version: 1.0.0.9000           Version: 1.1.0\nImports:                 Imports:                      Imports: \n    aaapkg (>= 2.1.3)       aaapkg (>= 2.1.3.9000)       aaapkg (>= 2.2.0)\n                         Remotes:   \n                             jane/aaapkg \n\n\n\n\n\n\nCRAN\n\n\n\nIt’s important to note that you should not submit your package to CRAN in the intermediate state, meaning with a Remotes field and with a dependency required at a version that’s not available from CRAN or Bioconductor. For CRAN packages, this can only be a temporary development state, eventually resolved when the dependency updates on CRAN and you can bump your minimum version accordingly.\n\n\n\n12.7.2 Config/Needs/* field\nYou may also see devtools-developed packages with packages listed in DESCRIPTION fields in the form of Config/Needs/*, which we described in Section 10.8.\n\nThe use of Config/Needs/* is not directly related to devtools. It’s more accurate to say that it’s associated with continuous integration workflows made available to the community at https://github.com/r-lib/actions/ and exposed via functions such as usethis::use_github_actions(). A Config/Needs/* field tells the setup-r-dependencies GitHub Action about extra packages that need to be installed.\nConfig/Needs/website is the most common and it provides a place to specify packages that aren’t a formal dependency, but that must be present in order to build the package’s website (Chapter 20). The readxl package is a good example. It has a non-vignette article on workflows that shows readxl working in concert with other tidyverse packages, such as readr and purrr. But it doesn’t make sense for readxl to have a formal dependency on readr or purrr or (even worse) the tidyverse!\nOn the left is what readxl has in the Config/Needs/website field of DESCRIPTION to indicate that the tidyverse is needed in order to build the website, which is also formatted with styling that lives in the tidyverse/template GitHub repo. On the right is the corresponding excerpt from the configuration of the workflow that builds and deploys the website.\nin DESCRIPTION                  in .github/workflows/pkgdown.yaml\n--------------------------      ---------------------------------\nConfig/Needs/website:           - uses: r-lib/actions/setup-r-dependencies@v2\n    tidyverse,                    with:\n    tidyverse/tidytemplate          extra-packages: pkgdown\n                                    needs: website\nPackage websites and continuous integration are discussed more in Chapter 20 and Section 21.2, respectively.\nThe Config/Needs/* convention is handy because it allows a developer to use DESCRIPTION as their definitive record of package dependencies, while maintaining a clean distinction between true runtime dependencies versus those that are only needed for specialized development tasks."
  },
  {
    "objectID": "dependencies-in-practice.html#exports",
    "href": "dependencies-in-practice.html#exports",
    "title": "12  Dependencies: In Practice",
    "section": "\n12.8 Exports",
    "text": "12.8 Exports\nFor a function to be usable outside of your package, you must export it. When you create a new package with usethis::create_package(), nothing is exported at first, even once you add some functions. You can still experiment interactively with load_all(), since that loads all functions, not just those that are exported. But if you install and attach the package with library(pkg) in a fresh R session, you’ll notice that no functions are available.\n\n12.8.1 What to export\nExport functions that you want other people to use. Exported functions must be documented, and you must be cautious when changing their interface — other people are using them! Generally, it’s better to export too little than too much. It’s easy to start exporting something that you previously did not; it’s hard to stop exporting a function because it might break existing code. Always err on the side of caution, and simplicity. It’s easier to give people more functionality than it is to take away stuff they’re used to.\nWe believe that packages that have a wide audience should strive to do one thing and do it well. All functions in a package should be related to a single problem (or a set of closely related problems). Any functions not related to that purpose should not be exported. For example, most of our packages have a utils.R file (Section 7.1) that contains small utility functions that are useful internally, but aren’t part of the core purpose of those packages. We don’t export such functions. There are at least two reasons for this:\n\nFreedom to be less robust and less general. A utility for internal use doesn’t have to be implemented in the same way as a function used by others. You just need to cover your own use case.\nRegrettable reverse dependencies. You don’t want people depending on your package for functionality and functions that are unrelated to its core purpose.\n\nThat said, if you’re creating a package for yourself, it’s far less important to be this disciplined. Because you know what’s in your package, it’s fine to have a local “miscellany” package that contains a hodgepodge of functions that you find useful. But it is probably not a good idea to release such a package for wider use.\nSometimes your package has a function that could be of interest to other developers extending your package, but not to typical users. In this case, you want to export the function, but also to give it a very low profile in terms of public documentation. This can be achieved by combining the roxygen tags @export and @keywords internal. The internal keyword keeps the function from appearing in the package index, but the associated help topic still exists and the function still appears among those exported in the NAMESPACE file.\n\n12.8.2 Re-exporting\nSometimes you want to make something available to users of your package that is actually provided by one of your dependencies. When devtools was split into several smaller packages (Section 3.2), many of the user-facing functions moved elsewhere. For usethis, the chosen solution was to list it in Depends (Section 11.4.1), but that is not a good general solution. Instead, devtools now re-exports certain functions that actually live in a different package.\nHere is a blueprint for re-exporting an object from another package, using the session_info() function as our example:\n\n\nList the package that hosts the re-exported object in Imports in DESCRIPTION.2 In this case, the session_info() function is exported by the sessioninfo package.\nImports:\n    sessioninfo\n\n\nIn one of your R/*.R files, have a reference to the target function, preceded by roxygen tags for both importing and exporting.\n\n#' @export\n#' @importFrom sessioninfo session_info\nsessioninfo::session_info\n\n\n\nThat’s it! Next time you re-generate NAMESPACE, these two lines will be there (typically interspersed with other exports and imports):\n...\nexport(session_info)\n...\nimportFrom(sessioninfo,session_info)\n...\nAnd this explains how library(devtools) makes session_info() available in the current session. This will also lead to the creation of the man/reexports.Rd file, which finesses the requirement that your package must document all of its exported functions. This help topic lists all re-exported objects and links to their primary documentation."
  },
  {
    "objectID": "dependencies-in-practice.html#imports-and-exports-related-to-s3",
    "href": "dependencies-in-practice.html#imports-and-exports-related-to-s3",
    "title": "12  Dependencies: In Practice",
    "section": "\n12.9 Imports and exports related to S3",
    "text": "12.9 Imports and exports related to S3\nR has multiple object-oriented programming (OOP) systems:\n\nS3 is currently the most important for us and is the one we that is addressed in various parts of this book. The S3 chapter of Advanced R is a good place to learn more about S3 conceptually and the vctrs package is worth studying for practical knowledge.\nS4 is very important within certain R communities, most notably within the Bioconductor project. We only use S4 when it’s necessary for compatibility with other packages. If you want to learn more, the S4 chapter of Advanced R is a good starting point and has recommendations for additional resources.\nR6 is used in many tidyverse packages (broadly defined), but is out of scope for this book. Good places to learn more include the R6 package website, the R6 chapter of Advanced R, and the roxygen2 documentation related to R6.\n\nIn terms of namespace issues around S3 classes, the main things to consider are generic functions and their class-specific implementations known as methods. If your package “owns” an S3 class, it makes sense to export a user-friendly constructor function. This is often just a regular function and there is no special S3 angle.\nIf your package “owns” an S3 generic and you want others to be able to use it, you should export the generic. For example, the dplyr package exports the generic function dplyr::count() and also implements and exports a specific method, count.data.frame():\n\n#' ... all the usual documentation for count() ...\n#' @export\ncount <- function(x, ..., wt = NULL, sort = FALSE, name = NULL) {\n  UseMethod(\"count\")\n}\n\n#' @export\ncount.data.frame <- function(x, ..., wt = NULL, sort = FALSE, name = NULL, .drop = group_by_drop_default(x)) { ... }\n\nThe corresponding lines in dplyr’s NAMESPACE file look like this:\n...\nS3method(count,data.frame)\n...\nexport(count)\n...\nNow imagine that your package implements a method for count() for a class you “own” (not data.frame). A good example is the dbplyr package, which implements count() for the tbl_lazy class. An add-on package that implements an S3 generic for a new class should list the generic-providing package in Imports, import the generic into its namespace, and export its S3 method. Here’s part of dbplyr’s DESCRIPTION file:\nImports: \n    ...,\n    dplyr,\n    ...\nIn dbplyr/R/verb-count.R, we have:\n\n#' @importFrom dplyr count\n#' @export\ncount.tbl_lazy <- function(x, ..., wt = NULL, sort = FALSE, name = NULL) { ... }\n\nIn NAMESPACE, we have:\nS3method(count,tbl_lazy)\n...\nimportFrom(dplyr,count)\nDbplyr also provides methods for various generics provided by the base package, such as dim() and names(). In this case, there is no need to import those generics, but it’s still necessary to export the methods. In dbplyr/R/tbl_lazy.R, we have:\n\n#' @export\ndim.tbl_lazy <- function(x) {\n  c(NA, length(op_vars(x$lazy_query)))\n}\n\n#' @export\nnames.tbl_lazy <- function(x) {\n  colnames(x)\n}\n\nIn NAMESPACE, this produces:\nS3method(dim,tbl_lazy)\n...\nS3method(names,tbl_lazy)\nThe last and trickiest case is when your package offers a method for a generic “owned” by a package you’ve listed in Suggests. The basic idea is that you want to register the availability of your S3 method conditionally, when your package is being loaded. If the suggested package is present, your S3 method should be registered, but otherwise it should not.\nWe’ll illustrate this with an example. Within the tidyverse, the glue package is managed as a low-level package that should have minimal dependencies (Section 11.1.3). Glue functions generally return a character vector that also has the \"glue\" S3 class.\n\nlibrary(glue)\nname <- \"Betty\"\n(ret <- glue('My name is {name}.'))\n#> My name is Betty.\nclass(ret)\n#> [1] \"glue\"      \"character\"\n\nThe motivation for this is that it allows glue to offer special methods for print(), the + operator, and subsetting via [ and [[. One downside, though, is that this class attribute complicates string comparisons:\n\nidentical(ret, \"My name is Betty.\")\n#> [1] FALSE\nall.equal(ret, \"My name is Betty.\")\n#> [1] \"Attributes: < Modes: list, NULL >\"                   \n#> [2] \"Attributes: < Lengths: 1, 0 >\"                       \n#> [3] \"Attributes: < names for target but not for current >\"\n#> [4] \"Attributes: < current is not list-like >\"            \n#> [5] \"target is glue, current is character\"\n\nTherefore, for testing, it is helpful if glue offers a method for testthat::compare(), which explains why this expectation succeeds:\n\ntestthat::expect_equal(ret, \"My name is Betty.\")\n\nBut glue can’t list testthat in Imports! It must go in Suggests. The solution is to register the method conditionally when glue is loaded. Here is a redacted version of glue’s .onLoad() function, where you’ll see that it conditionally registers some other methods as well:\n\n.onLoad <- function(...) {\n  s3_register(\"testthat::compare\", \"glue\")\n  s3_register(\"waldo::compare_proxy\", \"glue\")\n  s3_register(\"vctrs::vec_ptype2\", \"glue.glue\")\n  ...\n  invisible()\n}\n\nThe s3_register() function comes from the vctrs package. If you don’t have an organic need to depend on vctrs, it is common (and encouraged) to simply inline the s3_register() source into your own package. You can’t always copy code from other people’s packages and paste it into yours, but you can in this case. This usage is specifically allowed by the license of the source code of s3_register(). This provides a great segue into Chapter 13, which is all about licensing."
  },
  {
    "objectID": "license.html#big-picture",
    "href": "license.html#big-picture",
    "title": "13  Licensing",
    "section": "\n13.1 Big picture",
    "text": "13.1 Big picture\nTo understand the author’s wishes, it’s useful to understand the two major camps of open source licenses:\n\nPermissive licenses are very easy going. Code with a permissive license can be freely copied, modified, and published, and the only restriction is that the license must be preserved. The MIT and Apache licenses are the most common modern permissive licenses; older permissive licenses include the various forms of the BSD license.\nCopyleft licenses are stricter. The most common copyleft license is the GPL which allows you to freely copy and modify the code for personal use, but if you publish modified versions or bundle with other code, the modified version or complete bundle must also be licensed with the GPL.\n\nWhen you look across all programming languages, permissive licenses are the most common. For example, a 2015 survey of GitHub repositories found that ~55% used a permissive license and ~20% used a copyleft license. The R community is rather different: as of 2022, my analysis2 found that ~70% of CRAN packages use a copyleft license and ~20% use a permissive license. This means licensing your R package requires a little more care than for other languages."
  },
  {
    "objectID": "license.html#code-you-write",
    "href": "license.html#code-you-write",
    "title": "13  Licensing",
    "section": "\n13.2 Code you write",
    "text": "13.2 Code you write\nWe’ll start by talking about code that you write, and how to license it to make clear how you want people to treat it. It’s important to use a license because if you don’t, the default copyright laws apply, which means that no one is allowed to make a copy of your code without your express permission.\nIn brief:\n\nIf you want a permissive license so people can use your code with minimal restrictions, choose the MIT license with use_mit_license().\nIf you want a copyleft license so that all derivatives and bundles of your code are also open source, choose the GPLv3 license with use_gpl_license().\nIf your package primarily contains data, not code, and you want minimal restrictions, choose the CC0 license with use_cc0_license(). Or if you want to require attribution when your data is used, choose the CC BY license by calling use_ccby_license().\nIf you don’t want to make your code open source call use_proprietary_license(). Such packages can not be distributed by CRAN.\n\nWe’ll come back to more details and present a few other licenses in Section 13.2.2.\n\n13.2.1 Key files\nThere are three key files used to record your licensing decision:\n\n\nEvery license sets the License field in the DESCRIPTION. This contains the name of the license in a standard form so that R CMD check and CRAN can automatically verify it. It comes in four main forms:\n\nA name and version specification, e.g. GPL (>= 2), or Apache License (== 2.0).\nA standard abbreviation, e.g. GPL-2, LGPL-2.1, Artistic-2.0.\nA name of a license “template” and a file containing specific variables. The most common case is MIT + file LICENSE, where the LICENSE file needs to contain two fields: the year and copyright holder.\nPointer to the full text of a non-standard license, file LICENSE.\n\nMore complicated licensing structures are possible but outside the scope of this text. See the Licensing section of “Writing R extensions” for details.\n\nAs described above, the LICENSE file is used in one of two ways. Some licenses are templates that require additional details to be complete in the LICENSE file. The LICENSE file can also contain the full text of non-standard and non-open source licenses. You are not permitted to include the full text of standard licenses.\nLICENSE.md includes a copy of the full text of the license. All open source licenses require a copy of the license to be included, but CRAN does not permit you to include a copy of standard licenses in your package, so we also use .Rbuildignore to make sure this file is not sent to CRAN.\n\nThere is one other file that we’ll come back to in Section 13.4.2: LICENSE.note. This is used when you have bundled code written by other people, and parts of your package have more permissive licenses than the whole.\n\n13.2.2 More licenses for code\nI gave you the absolute minimum you need to know above. But it’s worth mentioning a few more important licenses roughly ordered from most permissive to least permissive:\n\nuse_apache_license(): the Apache License is similar to the MIT license but it also includes an explicit patent grant. Patents are another component of intellectual property distinct from copyrights, and some organisations also care about protection from patent claims.\nuse_lgpl_license(): the LGPL is a little weaker than the GPL, allowing you to bundle LPGL code using any license for the larger work.\nuse_gpl_license(): We’ve discussed the GPL already, but there’s one important wrinkle to note — the GPL has two major versions, GPLv2 and GPLv3, and they’re not compatible (i.e. you can’t bundle GPLv2 and GPLv3 code in the same project). To avoid this problem it’s generally recommended to license your package as GPL >=2 or GPL >= 3 so that future versions of the GPL license also apply to your code. This is what use_gpl_license() does by default.\nuse_agpl_license(): The AGPL defines distribution to include providing a service over a network, so that if you use AGPL code to provide a web service, all bundled code must also be open-sourced. Because this is a considerably broader claim than the GPL, many companies expressly forbid the use of AGPL software.\n\nThere are many other licenses available. To get a high-level view of the open source licensing space, and the details of individual licenses, I highly recommend https://choosealicense.com, which I’ve used in the links above. For more details about licensing R packages, I recommend Licensing R by Colin Fay. The primary downside of choosing a license not in the list above is that fewer R users will understand what it means, and it will make it harder for them to use your code.\n\n13.2.3 Licenses for data\nAll these licenses are designed specifically to apply to source code, so if you’re releasing a package that primarily contains data, you should use a different type of license. We recommend one of two Creative Commons licenses:\n\nIf you want to make the data as freely available as possible, you use the CC0 license with use_cc0_license(). This is a permissive license that’s equivalent to the MIT license, but applies to data, not code.3\nIf you want to require attribution when someone else uses your data, you can use the CC-BY license, with use_ccby_license().\n\n13.2.4 Relicensing\nChanging your license after the fact is hard because it requires the permission of all copyright holders, and unless you have taken special steps (more on that below) this will include everyone who has contributed a non-trivial amount of code.\nIf you do need to re-license a package, we recommend the following steps:\n\nCheck the Authors@R field in the DESCRIPTION to confirm that the package doesn’t contain bundled code (which we’ll talk about in Section 13.4).\nFind all contributors by looking at the Git history or the contributors display on GitHub.\nOptionally, inspect the specific contributions and remove people who only contributed typo fixes and similar4.\nAsk every contributor if they’re OK with changing the license. If every contributor is on GitHub, the easiest way to do this is to create an issue where you list all contributors and ask them to confirm that they’re OK with the change.\nOnce all copyright holders have approved, make the change by calling the appropriate license function.\n\nYou can read about how the tidyverse followed this process to unify on the MIT license at https://www.tidyverse.org/blog/2021/12/relicensing-packages/."
  },
  {
    "objectID": "license.html#sec-code-given-to-you",
    "href": "license.html#sec-code-given-to-you",
    "title": "13  Licensing",
    "section": "\n13.3 Code given to you",
    "text": "13.3 Code given to you\nMany packages include code not written by the author. There are two main ways this happens: other people might choose to contribute to your package using a pull request or similar, or you might find some code and choose to bundle it. This section will discuss code that others give to you, and the next section will discuss code that you bundle.\nWhen someone contributes code to your package using a pull request or similar, you can assume that the author is happy for their code to use your license. This is explicit in the GitHub terms of service, but is generally considered to be true regardless of how the code is contributed5.\nHowever, the author retains copyright of their code, which means that you can’t change the license without their permission (more on that shortly). If you want to retain the ability to change the license, you need an explicit “contributor license agreement” or CLA, where the author explicitly reassigns the copyright. This is most important for dual open-source/commercial projects because it easily allows for dual licensing where the code is made available to the world with a copyleft license, and to paying customers with a different, more permissive, license.\nIt’s also important to acknowledge the contribution, and it’s good practice to be generous with thanks and attribution. In the tidyverse, we ask that all code contributors include a bullet in NEWS.md with their GitHub username, and we thank all contributors in release announcements. We only add core developers6 to the DESCRIPTION file; but some projects choose to add all contributors no matter how small."
  },
  {
    "objectID": "license.html#sec-code-you-bundle",
    "href": "license.html#sec-code-you-bundle",
    "title": "13  Licensing",
    "section": "\n13.4 Code you bundle",
    "text": "13.4 Code you bundle\nThere are three common reasons that you might choose to bundle code written by someone else:\n\nYou’re including someone else’s CSS or JS library in order to create a useful and attractive web page or HTML widgets. Shiny is a great example of a package that does this extensively.\nYou’re providing an R wrapper for a simple C or C++ library. (For complex C/C++ libraries, you don’t usually bundle the code in your package, but instead link to a copy installed elsewhere on the system).\nYou’ve copied a small amount of R code from another package to avoid taking a dependency. Generally, taking a dependency on another package is the right thing to do because you don’t need to worry about licensing, and you’ll automatically get bug fixes. But sometimes you only need a very small amount of code from a big package, and copying and pasting it into your package is the right thing to do.\n\n\n13.4.1 License compatibility\nBefore you bundle someone else’s code into your package, you need to first check that the bundled license is compatible with your license. When distributing code, you can add additional restrictions, but you can not remove restrictions, which means that license compatibility is not symmetric. For example, you can bundle MIT licensed code in a GPL licensed package, but you can not bundle GPL licensed code in an MIT licensed package.\nThere are five main cases to consider:\n\nIf your license and their license are the same: it’s OK to bundle.\nIf their license is MIT or BSD, it’s OK to bundle.\nIf their code has a copyleft license and your code has a permissive license, you can’t bundle their code. You’ll need to consider an alternative approach, either looking for code with a more permissive license, or putting the external code in a separate package.\nIf the code comes from Stack Overflow, it’s licensed7 with the Creative Common CC BY-SA license, which is only compatible with GPLv38 . This means that you need to take extra care when using Stack Overflow code in open source packages . Learn more at https://empirical-software.engineering/blog/so-snippets-in-gh-projects.\nOtherwise, you’ll need to do a little research. Wikipedia has a useful diagram and Google is your friend. It’s important to note that different versions of the same license are not necessarily compatible, e.g. GPLv2 and GPLv3 are not compatible.\n\nIf your package isn’t open source, things are more complicated. Permissive licenses are still easy, and copyleft licenses generally don’t restrict use as long as you don’t distribute the package outside your company. But this is a complex issue and opinions differ, and should check with your legal department first.\n\n13.4.2 How to include\nOnce you’ve determined that the licenses are compatible, you can bring the code in your package. When doing so, you need to preserve all existing license and copyright statements, and make it as easy as possible for future readers to understand the licensing situation:\n\nIf you’re including a fragment of another project, generally best to put in its own file and ensure that file has copyright statements and license description at the top.\nIf you’re including multiple files, put in a directory, and put a LICENSE file in that directory.\n\nYou also need to include some standard metadata in Authors@R. You should use role = \"cph\" to declare that the author is a copyright holder, with a comment describing what they’re the author of.\nIf you’re submitting to CRAN and the bundled code has a different (but compatible) license, you also need to include a LICENSE.note file that describes the overall license of the package, and the specific licenses of each individual component. For example, the diffviewer package bundles six javascript libraries all of which use a permissive license. The DESCRIPTION lists all copyright holders, and the LICENSE.note describes their licenses. (Other packages use other techniques, but I think this is the simplest approach that will fly with CRAN.)"
  },
  {
    "objectID": "license.html#code-you-use",
    "href": "license.html#code-you-use",
    "title": "13  Licensing",
    "section": "\n13.5 Code you use",
    "text": "13.5 Code you use\n\nObviously all the R code you write uses R, and R is licensed with the GPL. Does that mean your R code must always be GPL licensed? No, and the R Foundation made this clear in 2009. Similarly, it’s our personal opinion that the license of your package doesn’t need to be compatible with the licenses of R packages that you merely use by calling their exported R functions (i.e. via Suggests or Imports).\nThings are different in other languages, like C, because creating a C executable almost invariably ends up copying some component of the code you use into the executable. This can also come up if your R package has compiled code and you link to (using the LinkingTo in your DESCRIPTION): you’ll need to do more investigation to make sure your license is compatible. However, if you’re just linking to R itself, you are generally free to license as you wish because R headers are licensed with the Lesser GPL.\nOf course, any user of your package will have to download all the packages that your package depends on (as well as R itself), so will still have to comply with the terms of those licenses."
  },
  {
    "objectID": "testing-basics.html#why-is-formal-testing-worth-the-trouble",
    "href": "testing-basics.html#why-is-formal-testing-worth-the-trouble",
    "title": "14  Testing basics",
    "section": "\n14.1 Why is formal testing worth the trouble?",
    "text": "14.1 Why is formal testing worth the trouble?\nUp until now, your workflow probably looks like this:\n\nWrite a function.\nLoad it with devtools::load_all(), maybe via Ctrl/Cmd + Shift + L.\nExperiment with it in the console to see if it works.\nRinse and repeat.\n\nWhile you are testing your code in this workflow, you’re only doing it informally. The problem with this approach is that when you come back to this code in 3 months time to add a new feature, you’ve probably forgotten some of the informal tests you ran the first time around. This makes it very easy to break code that used to work.\nMany of us embrace automated testing when we realize we’re re-fixing a bug for the second or fifth time. While writing code or fixing bugs, we might perform some interactive tests to make sure the code we’re working on does what we want. But it’s easy to forget all the different use cases you need to check, if you don’t have a system for storing and re-running the tests. This is a common practice among R programmers. The problem is not that you don’t test your code, it’s that you don’t automate your tests.\nIn this chapter you’ll learn how to transition from informal ad hoc testing, done interactively in the console, to automated testing (also known as unit testing). While turning casual interactive tests into formal tests requires a little more work up front, it pays off in four ways:\n\n\nFewer bugs. Because you’re explicit about how your code should behave, you will have fewer bugs. The reason why is a bit like the reason double entry book-keeping works: because you describe the behaviour of your code in two places, both in your code and in your tests, you are able to check one against the other.\nWith informal testing, it’s tempting to just explore typical and authentic usage, similar to writing examples. However, when writing formal tests, it’s natural to adopt a more adversarial mindset and to anticipate how unexpected inputs could break your code.\nIf you always introduce new tests when you add a new feature or function, you’ll prevent many bugs from being created in the first place, because you will proactively address pesky edge cases. Tests also keep you from (re-)breaking one feature, when you’re tinkering with another.\n\nBetter code structure. Code that is well designed tends to be easy to test and you can turn this to your advantage. If you are struggling to write tests, consider if the problem is actually the design of your function(s). The process of writing tests is a great way to get free, private, and personalized feedback on how well-factored your code is. If you integrate testing into your development workflow (versus planning to slap tests on “later”), you’ll subject yourself to constant pressure to break complicated operations into separate functions that work in isolation. Functions that are easier to test are usually easier to understand and re-combine in new ways.\nCall to action. When we start to fix a bug, we first like to convert it into a (failing) test. This is wonderfully effective at making your goal very concrete: make this test pass. This is basically a special case of a general methodology known as test driven development.\nRobust code. If you know that all the major functionality of your package is well covered by the tests, you can confidently make big changes without worrying about accidentally breaking something. This provides a great reality check when you think you’ve discovered some brilliant new way to simplify your package. Sometimes such “simplifications” fail to account for some important use case and your tests will save you from yourself."
  },
  {
    "objectID": "testing-basics.html#introducing-testthat",
    "href": "testing-basics.html#introducing-testthat",
    "title": "14  Testing basics",
    "section": "\n14.2 Introducing testthat",
    "text": "14.2 Introducing testthat\nThis chapter describes how to test your R package using the testthat package: https://testthat.r-lib.org\nIf you’re familiar with frameworks for unit testing in other languages, you should note that there are some fundamental differences with testthat. This is because R is, at heart, more a functional programming language than an object-oriented programming language. For instance, because R’s main object-oriented systems (S3 and S4) are based on generic functions (i.e., methods belong to functions not classes), testing approaches built around objects and methods don’t make much sense.\ntestthat 3.0.0 (released 2020-10-31) introduced the idea of an edition of testthat, specifically the third edition of testthat, which we refer to as testthat 3e. An edition is a bundle of behaviors that you have to explicitly choose to use, allowing us to make otherwise backward incompatible changes. This is particularly important for testthat since it has a very large number of packages that use it (almost 5,000 at last count). To use testthat 3e, you must have a version of testthat >= 3.0.0 and explicitly opt-in to the third edition behaviors. This allows testthat to continue to evolve and improve without breaking historical packages that are in a rather passive maintenance phase. You can learn more in the testthat 3e article and the blog post Upgrading to testthat edition 3.\nWe recommend testthat 3e for all new packages and we recommend updating existing, actively maintained packages to use testthat 3e. Unless we say otherwise, this chapter describes testthat 3e."
  },
  {
    "objectID": "testing-basics.html#sec-tests-mechanics-workflow",
    "href": "testing-basics.html#sec-tests-mechanics-workflow",
    "title": "14  Testing basics",
    "section": "\n14.3 Test mechanics and workflow",
    "text": "14.3 Test mechanics and workflow\n\n14.3.1 Initial setup\nTo setup your package to use testthat, run:\n\nusethis::use_testthat(3)\n\nThis will:\n\nCreate a tests/testthat/ directory.\n\nAdd testthat to the Suggests field in the DESCRIPTION and specify testthat 3e in the Config/testthat/edition field. The affected DESCRIPTION fields might look like:\nSuggests: testthat (>= 3.0.0)\nConfig/testthat/edition: 3\n\n\nCreate a file tests/testthat.R that runs all your tests when R CMD check runs (Section 5.5). For a package named “pkg”, the contents of this file will be something like:\n\nlibrary(testthat)\nlibrary(pkg)\n\ntest_check(\"pkg\")\n\n\n\nThis initial setup is usually something you do once per package. However, even in a package that already uses testthat, it is safe to run use_testthat(3), when you’re ready to opt-in to testthat 3e.\nDo not edit tests/testthat.R! It is run during R CMD check (and, therefore, devtools::check()), but is not used in most other test-running scenarios (such as devtools::test() or devtools::test_active_file()). If you want to do something that affects all of your tests, there is almost always a better way than modifying the boilerplate tests/testthat.R script. This chapter details many different ways to make objects and logic available during testing.\n\n14.3.2 Create a test\nAs you define functions in your package, in the files below R/, you add the corresponding tests to .R files in tests/testthat/. We strongly recommend that the organisation of test files match the organisation of R/ files, discussed in Section 7.1: The foofy() function (and its friends and helpers) should be defined in R/foofy.R and their tests should live in tests/testthat/test-foofy.R.\nR                                     tests/testthat\n└── foofy.R                           └── test-foofy.R\n    foofy <- function(...) {...}          test_that(\"foofy does this\", {...})\n                                          test_that(\"foofy does that\", {...})\nEven if you have different conventions for file organisation and naming, note that testthat tests must live in files below tests/testthat/ and these file names must begin with test. The test file name is displayed in testthat output, which provides helpful context1.\n\nusethis offers a helpful pair of functions for creating or toggling between files:\n\nusethis::use_r()\nusethis::use_test()\n\nEither one can be called with a file (base) name, in order to create a file de novo and open it for editing:\n\nuse_r(\"foofy\")    # creates and opens R/foofy.R\nuse_test(\"blarg\") # creates and opens tests/testthat/test-blarg.R\n\nThe use_r() / use_test() duo has some convenience features that make them “just work” in many common situations:\n\nWhen determining the target file, they can deal with the presence or absence of the .R extension and the test- prefix.\n\nEquivalent: use_r(\"foofy.R\"), use_r(\"foofy\")\n\nEquivalent: use_test(\"test-blarg.R\"), use_test(\"blarg.R\"), use_test(\"blarg\")\n\n\n\nIf the target file already exists, it is opened for editing. Otherwise, the target is created and then opened for editing.\n\n\n\n\n\n\n\nRStudio\n\n\n\nIf R/foofy.R is the active file in your source editor, you can even call use_test() with no arguments! The target test file can be inferred: if you’re editing R/foofy.R, you probably want to work on the companion test file, tests/testthat/test-foofy.R. If it doesn’t exist yet, it is created and, either way, the test file is opened for editing. This all works the other way around also. If you’re editing tests/testthat/test-foofy.R, a call to use_r() (optionally, creates and) opens R/foofy.R.\n\n\nBottom line: use_r() / use_test() are handy for initially creating these file pairs and, later, for shifting your attention from one to the other.\nWhen use_test() creates a new test file, it inserts an example test:\n\ntest_that(\"multiplication works\", {\n  expect_equal(2 * 2, 4)\n})\n\nYou will replace this with your own description and logic, but it’s a nice reminder of the basic form:\n\nA test file holds one or more test_that() tests.\nEach test describes what it’s testing: e.g. “multiplication works”.\nEach test has one or more expectations: e.g. expect_equal(2 * 2, 4).\n\nBelow we go into much more detail about how to test your own functions.\n\n14.3.3 Run tests\nDepending on where you are in the development cycle, you’ll run your tests at various scales. When you are rapidly iterating on a function, you might work at the level of individual tests. As the code settles down, you’ll run entire test files and eventually the entire test suite.\nMicro-iteration: This is the interactive phase where you initiate and refine a function and its tests in tandem. Here you will run devtools::load_all() often, and then execute individual expectations or whole tests interactively in the console. Note that load_all() attaches testthat, so it puts you in the perfect position to test drive your functions and to execute individual tests and expectations.\n\n# tweak the foofy() function and re-load it\ndevtools::load_all()\n\n# interactively explore and refine expectations and tests\nexpect_equal(foofy(...), EXPECTED_FOOFY_OUTPUT)\n\ntestthat(\"foofy does good things\", {...})\n\nMezzo-iteration: As one file’s-worth of functions and their associated tests start to shape up, you will want to execute the entire file of associated tests, perhaps with testthat::test_file():\n\n\ntestthat::test_file(\"tests/testthat/test-foofy.R\")\n\n\n\n\n\n\n\nRStudio\n\n\n\nIn RStudio, you have a couple shortcuts for running a single test file.\nIf the target test file is the active file, you can use the “Run Tests” button in the upper right corner of the source editor.\nThere is also a useful function, devtools::test_active_file(). It infers the target test file from the active file and, similar to how use_r() and use_test() work, it works regardless of whether the active file is a test file or a companion R/*.R file. You can invoke this via “Run a test file” in the Addins menu. However, for heavy users (like us!), we recommend binding this to a keyboard shortcut; we use Ctrl/Cmd + T.\n\n\nMacro-iteration: As you near the completion of a new feature or bug fix, you will want to run the entire test suite.\nMost frequently, you’ll do this with devtools::test():\n\ndevtools::test()\n\nThen eventually, as part of R CMD check with devtools::check():\n\ndevtools::check()\n\n\n\n\n\n\n\nRStudio\n\n\n\ndevtools::test() is mapped to Ctrl/Cmd + Shift + T. devtools::check() is mapped to Ctrl/Cmd + Shift + E.\n\n\n\nThe output of devtools::test() looks like this:\ndevtools::test()\nℹ Loading usethis\nℹ Testing usethis\n✓ | F W S  OK | Context\n✓ |         1 | addin [0.1s]\n✓ |         6 | badge [0.5s]\n   ...\n✓ |        27 | github-actions [4.9s]\n   ...\n✓ |        44 | write [0.6s]\n\n══ Results ═════════════════════════════════════════════════════════════════\nDuration: 31.3 s\n\n── Skipped tests  ──────────────────────────────────────────────────────────\n• Not on GitHub Actions, Travis, or Appveyor (3)\n\n[ FAIL 1 | WARN 0 | SKIP 3 | PASS 728 ]\nTest failure is reported like this:\nFailure (test-release.R:108:3): get_release_data() works if no file found\nres$Version (`actual`) not equal to \"0.0.0.9000\" (`expected`).\n\n`actual`:   \"0.0.0.1234\"\n`expected`: \"0.0.0.9000\"\nEach failure gives a description of the test (e.g., “get_release_data() works if no file found”), its location (e.g., “test-release.R:108:3”), and the reason for the failure (e.g., “res$Version (actual) not equal to”0.0.0.9000” (expected)“).\nThe idea is that you’ll modify your code (either the functions defined below R/ or the tests in tests/testthat/) until all tests are passing."
  },
  {
    "objectID": "testing-basics.html#test-organisation",
    "href": "testing-basics.html#test-organisation",
    "title": "14  Testing basics",
    "section": "\n14.4 Test organisation",
    "text": "14.4 Test organisation\nA test file lives in tests/testthat/. Its name must start with test. We will inspect and execute a test file from the stringr package.\n\nBut first, for the purposes of rendering this book, we must attach stringr and testthat. Note that in real-life test-running situations, this is taken care of by your package development tooling:\n\nDuring interactive development, devtools::load_all() makes testthat and the package-under-development available (both its exported and unexported functions).\nDuring arms-length test execution, this is taken care of by devtools::test_active_file(), devtools::test(), and tests/testthat.R.\n\n\n\n\n\n\n\nImportant\n\n\n\nYour test files should not include these library() calls. We also explicitly request testthat edition 3, but in a real package this will be declared in DESCRIPTION.\n\nlibrary(testthat)\nlibrary(stringr)\nlocal_edition(3)\n\n\n\nHere are the contents of tests/testthat/test-dup.r from stringr:\n\ntest_that(\"basic duplication works\", {\n  expect_equal(str_dup(\"a\", 3), \"aaa\")\n  expect_equal(str_dup(\"abc\", 2), \"abcabc\")\n  expect_equal(str_dup(c(\"a\", \"b\"), 2), c(\"aa\", \"bb\"))\n  expect_equal(str_dup(c(\"a\", \"b\"), c(2, 3)), c(\"aa\", \"bbb\"))\n})\n#> Test passed 🥳\n\ntest_that(\"0 duplicates equals empty string\", {\n  expect_equal(str_dup(\"a\", 0), \"\")\n  expect_equal(str_dup(c(\"a\", \"b\"), 0), rep(\"\", 2))\n})\n#> Test passed 🎉\n\ntest_that(\"uses tidyverse recycling rules\", {\n  expect_error(str_dup(1:2, 1:3), class = \"vctrs_error_incompatible_size\")\n})\n#> Test passed 🌈\n\nThis file shows a typical mix of tests:\n\n“basic duplication works” tests typical usage of str_dup().\n“0 duplicates equals empty string” probes a specific edge case.\n“uses tidyverse recycling rules” checks that malformed input results in a specific kind of error.\n\nTests are organised hierarchically: expectations are grouped into tests which are organised in files:\n\nA file holds multiple related tests. In this example, the file tests/testthat/test-dup.r has all of the tests for the code in R/dup.r.\n\nA test groups together multiple expectations to test the output from a simple function, a range of possibilities for a single parameter from a more complicated function, or tightly related functionality from across multiple functions. This is why they are sometimes called unit tests. Each test should cover a single unit of functionality. A test is created with test_that(desc, code).\nIt’s common to write the description (desc) to create something that reads naturally, e.g. test_that(\"basic duplication works\", { ... }). A test failure report includes this description, which is why you want a concise statement of the test’s purpose, e.g. a specific behaviour.\n\nAn expectation is the atom of testing. It describes the expected result of a computation: Does it have the right value and right class? Does it produce an error when it should? An expectation automates visual checking of results in the console. Expectations are functions that start with expect_.\n\nYou want to arrange things such that, when a test fails, you’ll know what’s wrong and where in your code to look for the problem. This motivates all our recommendations regarding file organisation, file naming, and the test description. Finally, try to avoid putting too many expectations in one test - it’s better to have more smaller tests than fewer larger tests."
  },
  {
    "objectID": "testing-basics.html#expectations",
    "href": "testing-basics.html#expectations",
    "title": "14  Testing basics",
    "section": "\n14.5 Expectations",
    "text": "14.5 Expectations\nAn expectation is the finest level of testing. It makes a binary assertion about whether or not an object has the properties you expect. This object is usually the return value from a function in your package.\nAll expectations have a similar structure:\n\nThey start with expect_.\nThey have two main arguments: the first is the actual result, the second is what you expect.\nIf the actual and expected results don’t agree, testthat throws an error.\nSome expectations have additional arguments that control the finer points of comparing an actual and expected result.\n\nWhile you’ll normally put expectations inside tests inside files, you can also run them directly. This makes it easy to explore expectations interactively. There are more than 40 expectations in the testthat package, which can be explored in testthat’s reference index. We’re only going to cover the most important expectations here.\n\n14.5.1 Testing for equality\nexpect_equal() checks for equality, with some reasonable amount of numeric tolerance:\n\nexpect_equal(10, 10)\nexpect_equal(10, 10L)\nexpect_equal(10, 10 + 1e-7)\nexpect_equal(10, 11)\n#> Error: 10 (`actual`) not equal to 11 (`expected`).\n#> \n#>   `actual`: 10\n#> `expected`: 11\n\nIf you want to test for exact equivalence, use expect_identical().\n\nexpect_equal(10, 10 + 1e-7)\nexpect_identical(10, 10 + 1e-7)\n#> Error: 10 (`actual`) not identical to 10 + 1e-07 (`expected`).\n#> \n#>   `actual`: 10.0000000\n#> `expected`: 10.0000001\n\nexpect_equal(2, 2L)\nexpect_identical(2, 2L)\n#> Error: 2 (`actual`) not identical to 2L (`expected`).\n#> \n#> `actual` is a double vector (2)\n#> `expected` is an integer vector (2)\n\n\n14.5.2 Testing errors\nUse expect_error() to check whether an expression throws an error. It’s the most important expectation in a trio that also includes expect_warning() and expect_message(). We’re going to emphasize errors here, but most of this also applies to warnings and messages.\nUsually you care about two things when testing an error:\n\nDoes the code fail? Specifically, does it fail for the right reason?\nDoes the accompanying message make sense to the human who needs to deal with the error?\n\nThe entry-level solution is to expect a specific type of condition:\n\n1 / \"a\"\n#> Error in 1/\"a\": non-numeric argument to binary operator\nexpect_error(1 / \"a\") \n\nlog(-1)\n#> Warning in log(-1): NaNs produced\n#> [1] NaN\nexpect_warning(log(-1))\n\nThis is a bit dangerous, though, especially when testing an error. There are lots of ways for code to fail! Consider the following test:\n\nexpect_error(str_duq(1:2, 1:3))\n\nThis expectation is intended to test the recycling behaviour of str_dup(). But, due to a typo, it tests behaviour of a non-existent function, str_duq(). The code throws an error and, therefore, the test above passes, but for the wrong reason. Due to the typo, the actual error thrown is about not being able to find the str_duq() function:\n\nstr_duq(1:2, 1:3)\n#> Error in str_duq(1:2, 1:3): could not find function \"str_duq\"\n\nHistorically, the best defense against this was to assert that the condition message matches a certain regular expression, via the second argument, regexp.\n\nexpect_error(1 / \"a\", \"non-numeric argument\")\nexpect_warning(log(-1), \"NaNs produced\")\n\nThis does, in fact, force our typo problem to the surface:\n\nexpect_error(str_duq(1:2, 1:3), \"recycle\")\n#> Error in str_duq(1:2, 1:3): could not find function \"str_duq\"\n\nRecent developments in both base R and rlang make it increasingly likely that conditions are signaled with a class, which provides a better basis for creating precise expectations. That is exactly what you’ve already seen in this stringr example. This is what the class argument is for:\n\n# fails, error has wrong class\nexpect_error(str_duq(1:2, 1:3), class = \"vctrs_error_incompatible_size\")\n#> Error in str_duq(1:2, 1:3): could not find function \"str_duq\"\n\n# passes, error has expected class\nexpect_error(str_dup(1:2, 1:3), class = \"vctrs_error_incompatible_size\")\n\n\nIf you have the choice, express your expectation in terms of the condition’s class, instead of its message. Often this is under your control, i.e. if your package signals the condition. If the condition originates from base R or another package, proceed with caution. This is often a good reminder to re-consider the wisdom of testing a condition that is not fully under your control in the first place.\nTo check for the absence of an error, warning, or message, pass NA to the regexp argument:\n\nexpect_error(1 / 2, NA)\n\nOf course, this is functionally equivalent to simply executing 1 / 2 inside a test, but some developers find the explicit expectation expressive.\nIf you genuinely care about the condition’s message, testthat 3e’s snapshot tests are the best approach, which we describe next.\n\n14.5.3 Snapshot tests\nSometimes it’s difficult or awkward to describe an expected result with code. Snapshot tests are a great solution to this problem and this is one of the main innovations in testthat 3e. The basic idea is that you record the expected result in a separate, human-readable file. Going forward, testthat alerts you when a newly computed result differs from the previously recorded snapshot. Snapshot tests are particularly suited to monitoring your package’s user interface, such as its informational messages and errors. Other use cases include testing images or other complicated objects.\nWe’ll illustrate snapshot tests using the waldo package. Under the hood, testthat 3e uses waldo to do the heavy lifting of “actual vs. expected” comparisons, so it’s good for you to know a bit about waldo anyway. One of waldo’s main design goals is to present differences in a clear and actionable manner, as opposed to a frustrating declaration that “this differs from that and I know exactly how, but I won’t tell you”. Therefore, the formatting of output from waldo::compare() is very intentional and is well-suited to a snapshot test. The binary outcome of TRUE (actual == expected) vs. FALSE (actual != expected) is fairly easy to check and could get its own test. Here we’re concerned with writing a test to ensure that differences are reported to the user in the intended way.\nwaldo uses a few different layouts for showing diffs, depending on various conditions. Here we deliberately constrain the width, in order to trigger a side-by-side layout.2 (We’ll talk more about the withr package below.)\n\nwithr::with_options(\n  list(width = 20),\n  waldo::compare(c(\"X\", letters), c(letters, \"X\"))\n)\n#>     old | new    \n#> [1] \"X\" -        \n#> [2] \"a\" | \"a\" [1]\n#> [3] \"b\" | \"b\" [2]\n#> [4] \"c\" | \"c\" [3]\n#> \n#>      old | new     \n#> [25] \"x\" | \"x\" [24]\n#> [26] \"y\" | \"y\" [25]\n#> [27] \"z\" | \"z\" [26]\n#>          - \"X\" [27]\n\nThe two primary inputs differ at two locations: once at the start and once at the end. This layout presents both of these, with some surrounding context, which helps the reader orient themselves.\nHere’s how this would look as a snapshot test:\n\n\ntest_that(\"side-by-side diffs work\", {\n  withr::local_options(width = 20)\n  expect_snapshot(\n    waldo::compare(c(\"X\", letters), c(letters, \"X\"))\n  )\n})\n\nIf you execute expect_snapshot() or a test containing expect_snapshot() interactively, you’ll see this:\nCan't compare snapshot to reference when testing interactively\nℹ Run `devtools::test()` or `testthat::test_file()` to see changes\nfollowed by a preview of the snapshot output.\nThis reminds you that snapshot tests only function when executed non-interactively, i.e. while running an entire test file or the entire test suite. This applies both to recording snapshots and to checking them.\nThe first time this test is executed via devtools::test() or similar, you’ll see something like this (assume the test is in tests/testthat/test-diff.R):\n── Warning (test-diff.R:63:3): side-by-side diffs work ─────────────────────\nAdding new snapshot:\nCode\n  waldo::compare(c(\n    \"X\", letters), c(\n    letters, \"X\"))\nOutput\n      old | new    \n  [1] \"X\" -        \n  [2] \"a\" | \"a\" [1]\n  [3] \"b\" | \"b\" [2]\n  [4] \"c\" | \"c\" [3]\n  \n       old | new     \n  [25] \"x\" | \"x\" [24]\n  [26] \"y\" | \"y\" [25]\n  [27] \"z\" | \"z\" [26]\n           - \"X\" [27]\nThere is always a warning upon initial snapshot creation. The snapshot is added to tests/testthat/_snaps/diff.md, under the heading “side-by-side diffs work”, which comes from the test’s description. The snapshot looks exactly like what a user sees interactively in the console, which is the experience we want to check for. The snapshot file is also very readable, which is pleasant for the package developer. This readability extends to snapshot changes, i.e. when examining Git diffs and reviewing pull requests on GitHub, which helps you keep tabs on your user interface. Going forward, as long as your package continues to re-capitulate the expected snapshot, this test will pass.\nIf you’ve written a lot of conventional unit tests, you can appreciate how well-suited snapshot tests are for this use case. If we were forced to inline the expected output in the test file, there would be a great deal of quoting, escaping, and newline management. Ironically, with conventional expectations, the output you expect your user to see tends to get obscured by a heavy layer of syntactical noise.\nWhat about when a snapshot test fails? Let’s imagine a hypothetical internal change where the default labels switch from “old” and “new” to “OLD” and “NEW”. Here’s how this snapshot test would react:\n── Failure (test-diff.R:63:3): side-by-side diffs work──────────────────────────\nSnapshot of code has changed:\nold[3:15] vs new[3:15]\n  \"    \\\"X\\\", letters), c(\"\n  \"    letters, \\\"X\\\"))\"\n  \"Output\"\n- \"      old | new    \"\n+ \"      OLD | NEW    \"\n  \"  [1] \\\"X\\\" -        \"\n  \"  [2] \\\"a\\\" | \\\"a\\\" [1]\"\n  \"  [3] \\\"b\\\" | \\\"b\\\" [2]\"\n  \"  [4] \\\"c\\\" | \\\"c\\\" [3]\"\n  \"  \"\n- \"       old | new     \"\n+ \"       OLD | NEW     \"\nand 3 more ...\n\n* Run `snapshot_accept('diff')` to accept the change\n* Run `snapshot_review('diff')` to interactively review the change\nThis diff is presented more effectively in most real-world usage, e.g. in the console, by a Git client, or via a Shiny app (see below). But even this plain text version highlights the changes quite clearly. Each of the two loci of change is indicated with a pair of lines marked with - and +, showing how the snapshot has changed.\nYou can call testthat::snapshot_review('diff') to review changes locally in a Shiny app, which lets you skip or accept individual snapshots. Or, if all changes are intentional and expected, you can go straight to testthat::snapshot_accept('diff'). Once you’ve re-synchronized your actual output and the snapshots on file, your tests will pass once again. In real life, snapshot tests are a great way to stay informed about changes to your package’s user interface, due to your own internal changes or due to changes in your dependencies or even R itself.\nexpect_snapshot() has a few arguments worth knowing about:\n\ncran = FALSE: By default, snapshot tests are skipped if it looks like the tests are running on CRAN’s servers. This reflects the typical intent of snapshot tests, which is to proactively monitor user interface, but not to check for correctness, which presumably is the job of other unit tests which are not skipped. In typical usage, a snapshot change is something the developer will want to know about, but it does not signal an actual defect.\n\nerror = FALSE: By default, snapshot code is not allowed to throw an error. See expect_error(), described above, for one approach to testing errors. But sometimes you want to assess “Does this error message make sense to a human?” and having it laid out in context in a snapshot is a great way to see it with fresh eyes. Specify error = TRUE in this case:\n\nexpect_snapshot(error = TRUE,\n  str_dup(1:2, 1:3)\n)\n\n\ntransform: Sometimes a snapshot contains volatile, insignificant elements, such as a temporary filepath or a timestamp. The transform argument accepts a function, presumably written by you, to remove or replace such changeable text. Another use of transform is to scrub sensitive information from the snapshot.\nvariant: Sometimes snapshots reflect the ambient conditions, such as the operating system or the version of R or one of your dependencies, and you need a different snapshot for each variant. This is an experimental and somewhat advanced feature, so if you can arrange things to use a single snapshot, you probably should.\n\nIn typical usage, testthat will take care of managing the snapshot files below tests/testthat/_snaps/. This happens in the normal course of you running your tests and, perhaps, calling testthat::snapshot_accept().\n\n14.5.4 Shortcuts for other common patterns\nWe conclude this section with a few more expectations that come up frequently. But remember that testthat has many more pre-built expectations than we can demonstrate here.\nSeveral expectations can be described as “shortcuts”, i.e. they streamline a pattern that comes up often enough to deserve its own wrapper.\n\n\nexpect_match(object, regexp, ...) is a shortcut that wraps grepl(pattern = regexp, x = object, ...). It matches a character vector input against a regular expression regexp. The optional all argument controls whether all elements or just one element needs to match. Read the expect_match() documentation to see how additional arguments, like ignore.case = FALSE or fixed = TRUE, can be passed down to grepl().\n\nstring <- \"Testing is fun!\"\n\nexpect_match(string, \"Testing\") \n\n# Fails, match is case-sensitive\nexpect_match(string, \"testing\")\n#> Error: `string` does not match \"testing\".\n#> Actual value: \"Testing is fun!\"\n\n# Passes because additional arguments are passed to grepl():\nexpect_match(string, \"testing\", ignore.case = TRUE)\n\n\nexpect_length(object, n) is a shortcut for expect_equal(length(object), n).\nexpect_setequal(x, y) tests that every element of x occurs in y, and that every element of y occurs in x. But it won’t fail if x and y happen to have their elements in a different order.\n\nexpect_s3_class() and expect_s4_class() check that an object inherit()s from a specified class. expect_type()checks the typeof() an object.\n\nmodel <- lm(mpg ~ wt, data = mtcars)\nexpect_s3_class(model, \"lm\")\nexpect_s3_class(model, \"glm\")\n#> Error: `model` inherits from 'lm' not 'glm'.\n\n\n\nexpect_true() and expect_false() are useful catchalls if none of the other expectations does what you need."
  },
  {
    "objectID": "testing-design.html#what-to-test",
    "href": "testing-design.html#what-to-test",
    "title": "15  Designing your test suite",
    "section": "\n15.1 What to test",
    "text": "15.1 What to test\n\nWhenever you are tempted to type something into a print statement or a debugger expression, write it as a test instead. — Martin Fowler\n\nThere is a fine balance to writing tests. Each test that you write makes your code less likely to change inadvertently; but it also can make it harder to change your code on purpose. It’s hard to give good general advice about writing tests, but you might find these points helpful:\n\nFocus on testing the external interface to your functions - if you test the internal interface, then it’s harder to change the implementation in the future because as well as modifying the code, you’ll also need to update all the tests.\nStrive to test each behaviour in one and only one test. Then if that behaviour later changes you only need to update a single test.\nAvoid testing simple code that you’re confident will work. Instead focus your time on code that you’re not sure about, is fragile, or has complicated interdependencies. That said, I often find I make the most mistakes when I falsely assume that the problem is simple and doesn’t need any tests.\nAlways write a test when you discover a bug. You may find it helpful to adopt the test-first philosophy. There you always start by writing the tests, and then write the code that makes them pass. This reflects an important problem solving strategy: start by establishing your success criteria, how you know if you’ve solved the problem.\n\n\n15.1.1 Test coverage\nAnother concrete way to direct your test writing efforts is to examine your test coverage. The covr package (https://covr.r-lib.org) can be used to determine which lines of your package’s source code are (or are not!) executed when the test suite is run. This is most often presented as a percentage. Generally speaking, the higher the better.\nIn some technical sense, 100% test coverage is the goal, however, this is rarely achieved in practice and that’s often OK. Going from 90% or 99% coverage to 100% is not always the best use of your development time and energy. In many cases, that last 10% or 1% often requires some awkward gymnastics to cover. Sometimes this forces you to introduce mocking or some other new complexity. Don’t sacrifice the maintainability of your test suite in the name of covering some weird edge case that hasn’t yet proven to be a problem. Also remember that not every line of code or every function is equally likely to harbor bugs. Focus your testing energy on code that is tricky, based on your expert opinion and any empirical evidence you’ve accumulated about bug hot spots.\nWe use covr regularly, in two different ways:\n\nLocal, interactive use. We mostly use devtools::test_coverage_active_file() and devtools::test_coverage(), for exploring the coverage of an individual file or the whole package, respectively.\nAutomatic, remote use via GitHub Actions (GHA). We cover continuous integration and GHA more thoroughly elsewhere, but we will at least mention here that usethis::use_github_action(\"test-coverage\") configures a GHA workflow that constantly monitors your test coverage. Test coverage can be an especially helpful metric when evaluating a pull request (either your own or from an external contributor). A proposed change that is well-covered by tests is less risky to merge."
  },
  {
    "objectID": "testing-design.html#sec-testing-design-principles",
    "href": "testing-design.html#sec-testing-design-principles",
    "title": "15  Designing your test suite",
    "section": "\n15.2 High-level principles for testing",
    "text": "15.2 High-level principles for testing\nIn later sections, we offer concrete strategies for how to handle common testing dilemmas in R. Here we lay out the high-level principles that underpin these recommendations:\n\nA test should ideally be self-sufficient and self-contained.\nThe interactive workflow is important, because you will mostly interact with your tests when they are failing.\nIt’s more important that test code be obvious than, e.g., as DRY as possible.\nHowever, the interactive workflow shouldn’t “leak” into and undermine the test suite.\n\nWriting good tests for a code base often feels more challenging than writing the code in the first place. This can come as a bit of a shock when you’re new to package development and you might be concerned that you’re doing it wrong. Don’t worry, you’re not! Testing presents many unique challenges and maneuvers, which tend to get much less air time in programming communities than strategies for writing the “main code”, i.e. the stuff below R/. As a result, it requires more deliberate effort to develop your skills and taste around testing.\nMany of the packages maintained by our team violate some of the advice you’ll find here. There are (at least) two reasons for that:\n\ntestthat has been evolving for more than twelve years and this chapter reflects the cumulative lessons learned from that experience. The tests in many packages have been in place for a long time and reflect typical practices from different eras and different maintainers.\nThese aren’t hard and fast rules, but are, rather, guidelines. There will always be specific situations where it makes sense to bend the rule.\n\nThis chapter can’t address all possible testing situations, but hopefully these guidelines will help your future decision-making.\n\n15.2.1 Self-sufficient tests\n\nAll tests should strive to be hermetic: a test should contain all of the information necessary to set up, execute, and tear down its environment. Tests should assume as little as possible about the outside environment ….\nFrom the book Software Engineering at Google, Chapter 11\n\nRecall this advice found in Section 7.5, which covers your package’s “main code”, i.e. everything below R/:\n\nThe .R files below R/ should consist almost entirely of function definitions. Any other top-level code is suspicious and should be carefully reviewed for possible conversion into a function.\n\nWe have analogous advice for your test files:\n\nThe test-*.R files below tests/testthat/ should consist almost entirely of calls to test_that(). Any other top-level code is suspicious and should be carefully considered for relocation into calls to test_that() or to other files that get special treatment inside an R package or from testthat.\n\nEliminating (or at least minimizing) top-level code outside of test_that() will have the beneficial effect of making your tests more hermetic. This is basically the testing analogue of the general programming advice that it’s wise to avoid unstructured sharing of state.\nLogic at the top-level of a test file has an awkward scope: Objects or functions defined here have what you might call “test file scope”, if the definitions appear before the first call to test_that(). If top-level code is interleaved between test_that() calls, you can even create “partial test file scope”.\nWhile writing tests, it can feel convenient to rely on these file-scoped objects, especially early in the life of a test suite, e.g. when each test file fits on one screen. But we find that implicitly relying on objects in a test’s parent environment tends to make a test suite harder to understand and maintain over time.\nConsider a test file with top-level code sprinkled around it, outside of test_that():\n\ndat <- data.frame(x = c(\"a\", \"b\", \"c\"), y = c(1, 2, 3))\n\nskip_if(today_is_a_monday())\n\ntest_that(\"foofy() does this\", {\n  expect_equal(foofy(dat), ...)\n})\n\ndat2 <- data.frame(x = c(\"x\", \"y\", \"z\"), y = c(4, 5, 6))\n\nskip_on_os(\"windows\")\n\ntest_that(\"foofy2() does that\", {\n  expect_snapshot(foofy2(dat, dat2)\n})\n\nWe recommend relocating file-scoped logic to either a narrower scope or to a broader scope. Here’s what it would look like to use a narrow scope, i.e. to inline everything inside test_that() calls:\n\ntest_that(\"foofy() does this\", {\n  skip_if(today_is_a_monday())\n  \n  dat <- data.frame(x = c(\"a\", \"b\", \"c\"), y = c(1, 2, 3))\n  \n  expect_equal(foofy(dat), ...)\n})\n\ntest_that(\"foofy() does that\", {\n  skip_if(today_is_a_monday())\n  skip_on_os(\"windows\")\n  \n  dat <- data.frame(x = c(\"a\", \"b\", \"c\"), y = c(1, 2, 3))\n  dat2 <- data.frame(x = c(\"x\", \"y\", \"z\"), y = c(4, 5, 6))\n  \n  expect_snapshot(foofy(dat, dat2)\n})\n\nBelow we will discuss techniques for moving file-scoped logic to a broader scope.\n\n15.2.2 Self-contained tests\nEach test_that() test has its own execution environment, which makes it somewhat self-contained. For example, an R object you create inside a test does not exist after the test exits:\n\nexists(\"thingy\")\n#> [1] FALSE\n\ntest_that(\"thingy exists\", {\n  thingy <- \"thingy\"\n  expect_true(exists(thingy))\n})\n#> Test passed 🌈\n\nexists(\"thingy\")\n#> [1] FALSE\n\nThe thingy object lives and dies entirely within the confines of test_that(). However, testthat doesn’t know how to cleanup after actions that affect other aspects of the R landscape:\n\nThe filesystem: creating and deleting files, changing the working directory, etc.\nThe search path: library(), attach().\nGlobal options, like options() and par(), and environment variables.\n\nWatch how calls like library(), options(), and Sys.setenv() have a persistent effect after a test, even when they are executed inside test_that():\n\ngrep(\"jsonlite\", search(), value = TRUE)\n#> character(0)\ngetOption(\"opt_whatever\")\n#> NULL\nSys.getenv(\"envvar_whatever\")\n#> [1] \"\"\n\ntest_that(\"landscape changes leak outside the test\", {\n  library(jsonlite)\n  options(opt_whatever = \"whatever\")\n  Sys.setenv(envvar_whatever = \"whatever\")\n  \n  expect_match(search(), \"jsonlite\", all = FALSE)\n  expect_equal(getOption(\"opt_whatever\"), \"whatever\")\n  expect_equal(Sys.getenv(\"envvar_whatever\"), \"whatever\")\n})\n#> Test passed 🎊\n\ngrep(\"jsonlite\", search(), value = TRUE)\n#> [1] \"package:jsonlite\"\ngetOption(\"opt_whatever\")\n#> [1] \"whatever\"\nSys.getenv(\"envvar_whatever\")\n#> [1] \"whatever\"\n\nThese changes to the landscape even persist beyond the current test file, i.e. they carry over into all subsequent test files.\nIf it’s easy to avoid making such changes in your test code, that is the best strategy! But if it’s unavoidable, then you have to make sure that you clean up after yourself. This mindset is very similar to one we advocated for in Section 7.5, when discussing how to design well-mannered functions.\nWe like to use the withr package (https://withr.r-lib.org) to make temporary changes in global state, because it automatically captures the initial state and arranges the eventual restoration. You’ve already seen an example of its usage, when we explored snapshot tests:\n\ntest_that(\"side-by-side diffs work\", {\n  withr::local_options(width = 20) # <-- (°_°) look here!\n  expect_snapshot(\n    waldo::compare(c(\"X\", letters), c(letters, \"X\"))\n  )\n})\n\nThis test requires the display width to be set at 20 columns, which is considerably less than the default width. withr::local_options(width = 20) sets the width option to 20 and, at the end of the test, restores the option to its original value. withr is also pleasant to use during interactive development: deferred actions are still captured on the global environment and can be executed explicitly via withr::deferred_run() or implicitly by restarting R.\nWe recommend including withr in Suggests, if you’re only going to use it in your tests, or in Imports, if you also use it below R/. Call withr functions as we do above, e.g. like withr::local_whatever(), in either case. See Section 11.4.1 and Section 12.5.2 for more.\n\n\n\n\n\n\nTip\n\n\n\nThe easiest way to add a package to DESCRIPTION is with, e.g., usethis::use_package(\"withr\", type = \"Suggests\"). For tidyverse packages, withr is considered a “free dependency”, i.e. the tidyverse uses withr so extensively that we don’t hesitate to use it whenever it would be useful.\n\n\nwithr has a large set of pre-implemented local_*() / with_*() functions that should handle most of your testing needs, so check there before you write your own. If nothing exists that meets your need, withr::defer() is the general way to schedule some action at the end of a test.1\nHere’s how we would fix the problems in the previous example using withr: Behind the scenes, we reversed the landscape changes, so we can try this again.\n\ngrep(\"jsonlite\", search(), value = TRUE)\n#> character(0)\ngetOption(\"opt_whatever\")\n#> NULL\nSys.getenv(\"envvar_whatever\")\n#> [1] \"\"\n\ntest_that(\"withr makes landscape changes local to a test\", {\n  withr::local_package(\"jsonlite\")\n  withr::local_options(opt_whatever = \"whatever\")\n  withr::local_envvar(envvar_whatever = \"whatever\")\n  \n  expect_match(search(), \"jsonlite\", all = FALSE)\n  expect_equal(getOption(\"opt_whatever\"), \"whatever\")\n  expect_equal(Sys.getenv(\"envvar_whatever\"), \"whatever\")\n})\n#> Test passed 😸\n\ngrep(\"jsonlite\", search(), value = TRUE)\n#> character(0)\ngetOption(\"opt_whatever\")\n#> NULL\nSys.getenv(\"envvar_whatever\")\n#> [1] \"\"\n\ntestthat leans heavily on withr to make test execution environments as reproducible and self-contained as possible. In testthat 3e, testthat::local_reproducible_output() is implicitly part of each test_that() test.\n\ntest_that(\"something specific happens\", {\n  local_reproducible_output() # <-- this happens implicitly\n  \n  # your test code, which might be sensitive to ambient conditions, such as\n  # display width or the number of supported colors\n})\n\nlocal_reproducible_output() temporarily sets various options and environment variables to values favorable for testing, e.g. it suppresses colored output, turns off fancy quotes, sets the console width, and sets LC_COLLATE = \"C\". Usually, you can just passively enjoy the benefits of local_reproducible_output(). But you may want to call it explicitly when replicating test results interactively or if you want to override the default settings in a specific test.\n\n15.2.3 Plan for test failure\nWe regret to inform you that most of the quality time you spend with your tests will be when they are inexplicably failing.\n\nIn its purest form, automating testing consists of three activities: writing tests, running tests, and reacting to test failures….\nRemember that tests are often revisited only when something breaks. When you are called to fix a broken test that you have never seen before, you will be thankful someone took the time to make it easy to understand. Code is read far more than it is written, so make sure you write the test you’d like to read!\nFrom the book Software Engineering at Google, Chapter 11\n\nMost of us don’t work on a code base the size of Google. But even in a team of one, tests that you wrote six months ago might as well have been written by someone else. Especially when they are failing.\nWhen we do reverse dependency checks, often involving hundreds or thousands of CRAN packages, we have to inspect test failures to determine if changes in our packages are to blame. As a result, we regularly engage with failing tests in other people’s packages, which leaves us with lots of opinions about practices that create unnecessary testing pain.\nTest troubleshooting nirvana looks like this: In a fresh R session, you can do devtools::load_all() and immediately run an individual test or walk through it line-by-line. There is no need to hunt around for setup code that has to be run manually first, that is found elsewhere in the test file or perhaps in a different file altogether. Test-related code that lives in an unconventional location causes extra self-inflicted pain when you least need it.\nConsider this extreme and abstract example of a test that is difficult to troubleshoot due to implicit dependencies on free-range code:\n\n# dozens or hundreds of lines of top-level code, interspersed with other tests,\n# which you must read and selectively execute\n\ntest_that(\"f() works\", {\n  x <- function_from_some_dependency(object_with_unknown_origin)\n  expect_equal(f(x), 2.5)\n})\n\nThis test is much easier to drop in on if dependencies are invoked in the normal way, i.e. via ::, and test objects are created inline:\n\n# dozens or hundreds of lines of self-sufficient, self-contained tests,\n# all of which you can safely ignore!\n\ntest_that(\"f() works\", {\n  useful_thing <- ...\n  x <- somePkg::someFunction(useful_thing)\n  expect_equal(f(x), 2.5)\n})\n\nThis test is self-sufficient. The code inside { ... } explicitly creates any necessary objects or conditions and makes explicit calls to any helper functions. This test doesn’t rely on objects or dependencies that happen to be be ambiently available.\nSelf-sufficient, self-contained tests are a win-win: It is literally safer to design tests this way and it also makes tests much easier for humans to troubleshoot later.\n\n15.2.4 Repetition is OK\nOne obvious consequence of our suggestion to minimize code with “file scope” is that your tests will probably have some repetition. And that’s OK! We’re going to make the controversial recommendation that you tolerate a fair amount of duplication in test code, i.e. you can relax some of your DRY (“don’t repeat yourself”) tendencies.\n\nKeep the reader in your test function. Good production code is well-factored; good test code is obvious. … think about what will make the problem obvious when a test fails.\nFrom the blog post Why Good Developers Write Bad Unit Tests\n\nHere’s a toy example to make things concrete.\n\ntest_that(\"multiplication works\", {\n  useful_thing <- 3\n  expect_equal(2 * useful_thing, 6)\n})\n#> Test passed 😸\n\ntest_that(\"subtraction works\", {\n  useful_thing <- 3\n  expect_equal(5 - useful_thing, 2)\n})\n#> Test passed 🥇\n\nIn real life, useful_thing is usually a more complicated object that somehow feels burdensome to instantiate. Notice how useful_thing <- 3 appears in more than once place. Conventional wisdom says we should DRY this code out. It’s tempting to just move useful_thing’s definition outside of the tests:\n\nuseful_thing <- 3\n\ntest_that(\"multiplication works\", {\n  expect_equal(2 * useful_thing, 6)\n})\n#> Test passed 😀\n\ntest_that(\"subtraction works\", {\n  expect_equal(5 - useful_thing, 2)\n})\n#> Test passed 🌈\n\nBut we really do think the first form, with the repetition, is often the better choice.\nAt this point, many readers might be thinking “but the code I might have to repeat is much longer than 1 line!”. Below we describe the use of test fixtures. This can often reduce complicated situations back to something that resembles this simple example.\n\n15.2.5 Remove tension between interactive and automated testing\nYour test code will be executed in two different settings:\n\nInteractive test development and maintenance, which includes tasks like:\n\nInitial test creation\nModifying tests to adapt to change\nDebugging test failure\n\n\nAutomated test runs, which is accomplished with functions such as:\n\nSingle file: devtools::test_active_file(), testthat::test_file()\n\nWhole package: devtools::test(), devtools::check()\n\n\n\n\nAutomated testing of your whole package is what takes priority. This is ultimately the whole point of your tests. However, the interactive experience is clearly important for the humans doing this work. Therefore it’s important to find a pleasant workflow, but also to ensure that you don’t rig anything for interactive convenience that actually compromises the health of the test suite.\nThese two modes of test-running should not be in conflict with each other. If you perceive tension between these two modes, this can indicate that you’re not taking full advantage of some of testthat’s features and the way it’s designed to work with devtools::load_all().\nWhen working on your tests, use load_all(), just like you do when working below R/. By default, load_all() does all of these things:\n\nSimulates re-building, re-installing, and re-loading your package.\nMakes everything in your package’s namespace available, including unexported functions and objects and anything you’ve imported from another package.\nAttaches testthat, i.e. does library(testthat).\nRuns test helper files, i.e. executes test/testthat/helper.R (more on that below).\n\nThis eliminates the need for any library() calls below tests/testthat/, for the vast majority of R packages. Any instance of library(testthat) is clearly no longer necessary. Likewise, any instance of attaching one of your dependencies via library(somePkg) is unnecessary. In your tests, if you need to call functions from somePkg, do it just as you do below R/. If you have imported the function into your namespace, use fun(). If you have not, use somePkg::fun(). It’s fair to say that library(somePkg) in the tests should be about as rare as taking a dependency via Depends, i.e. there is almost always a better alternative.\nUnnecessary calls to library(somePkg) in test files have a real downside, because they actually change the R landscape. library() alters the search path. This means the circumstances under which you are testing may not necessarily reflect the circumstances under which your package will be used. This makes it easier to create subtle test bugs, which you will have to unravel in the future.\nOne other function that should almost never appear below tests/testhat/ is source(). There are several special files with an official role in testthat workflows (see below), not to mention the entire R package machinery, that provide better ways to make functions, objects, and other logic available in your tests."
  },
  {
    "objectID": "testing-design.html#sec-tests-files-overview",
    "href": "testing-design.html#sec-tests-files-overview",
    "title": "15  Designing your test suite",
    "section": "\n15.3 Files relevant to testing",
    "text": "15.3 Files relevant to testing\nHere we review which package files are especially relevant to testing and, more generally, best practices for interacting with the file system from your tests.\n\n15.3.1 Hiding in plain sight: files below R/\n\nThe most important functions you’ll need to access from your tests are clearly those in your package! Here we’re talking about everything that’s defined below R/. The functions and other objects defined by your package are always available when testing, regardless of whether they are exported or not. For interactive work, devtools::load_all() takes care of this. During automated testing, this is taken care of internally by testthat.\nThis implies that test helpers can absolutely be defined below R/ and used freely in your tests. It might make sense to gather such helpers in a clearly marked file, such as one of these:\n.                              \n├── ...\n└── R\n    ├── ...\n    ├── test-helpers.R\n    ├── test-utils.R\n    ├── testthat.R\n    ├── utils-testing.R\n    └── ...\nFor example, the dbplyr package uses R/testthat.R to define a couple of helpers to facilitate comparisons and expectations involving tbl objects, which is used to represent database tables.\n\ncompare_tbl <- function(x, y, label = NULL, expected.label = NULL) {\n  testthat::expect_equal(\n    arrange(collect(x), dplyr::across(everything())),\n    arrange(collect(y), dplyr::across(everything())),\n    label = label,\n    expected.label = expected.label\n  )\n}\n\nexpect_equal_tbls <- function(results, ref = NULL, ...) {\n  # code that gets things ready ...\n\n  for (i in seq_along(rest)) {\n    compare_tbl(\n      rest[[i]], ref,\n      label = names(rest)[[i]],\n      expected.label = ref_name\n    )\n  }\n\n  invisible(TRUE)\n}\n\n\n15.3.2 tests/testthat.R\n\nRecall the initial testthat setup described in Section 14.3: The standard tests/testthat.R file looks like this:\n\nlibrary(testthat)\nlibrary(pkg)\n\ntest_check(\"pkg\")\n\nWe repeat the advice to not edit tests/testthat.R. It is run during R CMD check (and, therefore, devtools::check()), but is not used in most other test-running scenarios (such as devtools::test() or devtools::test_active_file() or during interactive development). Do not attach your dependencies here with library(). Call them in your tests in the same manner as you do below R/ (Section 12.4.2, Section 12.5.2).\n\n15.3.3 Testthat helper files\nAnother type of file that is always executed by load_all() and at the beginning of automated testing is a helper file, defined as any file below tests/testthat/ that begins with helper. Helper files are a mighty weapon in the battle to eliminate code floating around at the top-level of test files. Helper files are a prime example of what we mean when we recommend moving such code into a broader scope. Objects or functions defined in a helper file are available to all of your tests.\nIf you have just one such file, you should probably name it helper.R. If you organize your helpers into multiple files, you could include a suffix with additional info. Here are examples of how such files might look:\n.                              \n├── ...\n└── tests\n    ├── testthat\n    │   ├── helper.R\n    │   ├── helper-blah.R\n    │   ├── helper-foo.R    \n    │   ├── test-foofy.R\n    │   └── (more test files)\n    └── testthat.R\nMany developers use helper files to define custom test helper functions, which we describe in detail in Chapter 16. Compared to defining helpers below R/, some people find that tests/testthat/helper.R makes it more clear that these utilities are specifically for testing the package. This location also feels more natural if your helpers rely on testthat functions. For example, usethis and vroom both have fairly extensive tests/testthat/helper.R files that define many custom test helpers. Here are two very simple usethis helpers that check that the currently active project (usually an ephemeral test project) has a specific file or folder:\n\nexpect_proj_file <- function(...) expect_true(file_exists(proj_path(...)))\nexpect_proj_dir <- function(...) expect_true(dir_exists(proj_path(...)))\n\nA helper file is also a good location for setup code that is needed for its side effects. This is a case where tests/testthat/helper.R is clearly more appropriate than a file below R/. For example, in an API-wrapping package, helper.R is a good place to (attempt to) authenticate with the testing credentials2.\n\n15.3.4 Testthat setup files\nTestthat has one more special file type: setup files, defined as any file below test/testthat/ that begins with setup. Here’s an example of how that might look:\n.                              \n├── ...\n└── tests\n    ├── testthat\n    │   ├── helper.R\n    │   ├── setup.R\n    │   ├── test-foofy.R\n    │   └── (more test files)\n    └── testthat.R\nA setup file is handled almost exactly like a helper file, but with two big differences:\n\nSetup files are not executed by devtools::load_all().\nSetup files often contain the corresponding teardown code.\n\nSetup files are good for global test setup that is tailored for test execution in non-interactive or remote environments. For example, you might turn off behaviour that’s aimed at an interactive user, such as messaging or writing to the clipboard.\nIf any of your setup should be reversed after test execution, you should also include the necessary teardown code in setup.R3. We recommend maintaining teardown code alongside the setup code, in setup.R, because this makes it easier to ensure they stay in sync. The artificial environment teardown_env() exists as a magical handle to use in withr::defer() and withr::local_*() / withr::with_*().\nHere’s a setup.R example from the reprex package, where we turn off clipboard and HTML preview functionality during testing:\n\nop <- options(reprex.clipboard = FALSE, reprex.html_preview = FALSE)\n\nwithr::defer(options(op), teardown_env())\n\nSince we are just modifying options here, we can be even more concise and use the pre-built function withr::local_options() and pass teardown_env() as the .local_envir:\n\nwithr::local_options(\n  list(reprex.clipboard = FALSE, reprex.html_preview = FALSE),\n  .local_envir = teardown_env()\n)\n\n\n15.3.5 Files ignored by testthat\ntestthat only automatically executes files where these are both true:\n\nFile is a direct child of tests/testthat/\n\nFile name starts with one of the specific strings:\n\nhelper\nsetup\ntest\n\n\n\nIt is fine to have other files or directories in tests/testthat/, but testthat won’t automatically do anything with them (other than the _snaps directory, which holds snapshots).\n\n15.3.6 Storing test data\nMany packages contain files that hold test data. Where should these be stored? The best location is somewhere below tests/testthat/, often in a subdirectory, to keep things neat. Below is an example, where useful_thing1.rds and useful_thing2.rds hold objects used in the test files.\n.\n├── ...\n└── tests\n    ├── testthat\n    │   ├── fixtures\n    │   │   ├── make-useful-things.R\n    │   │   ├── useful_thing1.rds\n    │   │   └── useful_thing2.rds\n    │   ├── helper.R\n    │   ├── setup.R\n    │   └── (all the test files)\n    └── testthat.R\nThen, in your tests, use testthat::test_path() to build a robust filepath to such files.\n\ntest_that(\"foofy() does this\", {\n  useful_thing <- readRDS(test_path(\"fixtures\", \"useful_thing1.rds\"))\n  # ...\n})\n\ntestthat::test_path() is extremely handy, because it produces the correct path in the two important modes of test execution:\n\nInteractive test development and maintenance, where working directory is presumably set to the top-level of the package.\nAutomated testing, where working directory is usually set to something below tests/.\n\n15.3.7 Where to write files during testing\nIf it’s easy to avoid writing files from your tests, that is definitely the best plan. But there are many times when you really must write files.\nYou should only write files inside the session temp directory. Do not write into your package’s tests/ directory. Do not write into the current working directory. Do not write into the user’s home directory. Even though you are writing into the session temp directory, you should still clean up after yourself, i.e. delete any files you’ve written.\nMost package developers don’t want to hear this, because it sounds like a hassle. But it’s not that burdensome once you get familiar with a few techniques and build some new habits. A high level of file system discipline also eliminates various testing bugs and will absolutely make your CRAN life run more smoothly.\nThis test is from roxygen2 and demonstrates everything we recommend:\n\ntest_that(\"can read from file name with utf-8 path\", {\n  path <- withr::local_tempfile(\n    pattern = \"Universit\\u00e0-\",\n    lines = c(\"#' @include foo.R\", NULL)\n  )\n  expect_equal(find_includes(path), \"foo.R\")\n})\n\nwithr::local_tempfile() creates a file within the session temp directory whose lifetime is tied to the “local” environment – in this case, the execution environment of an individual test. It is a wrapper around base::tempfile() and passes, e.g., the pattern argument through, so you have some control over the file name. You can optionally provide lines to populate the file with at creation time or you can write to the file in all the usual ways in subsequent steps. Finally, with no special effort on your part, the temporary file will automatically be deleted at the end of the test.\nSometimes you need even more control over the file name. In that case, you can use withr::local_tempdir() to create a self-deleting temporary directory and write intentionally-named files inside this directory."
  },
  {
    "objectID": "testing-advanced.html#test-fixtures",
    "href": "testing-advanced.html#test-fixtures",
    "title": "16  Advanced testing techniques",
    "section": "\n16.1 Test fixtures",
    "text": "16.1 Test fixtures\nWhen it’s not practical to make your test entirely self-sufficient, prefer making the necessary object, logic, or conditions available in a structured, explicit way. There’s a pre-existing term for this in software engineering: a test fixture.\n\nA test fixture is something used to consistently test some item, device, or piece of software. — Wikipedia\n\nThe main idea is that we need to make it as easy and obvious as possible to arrange the world into a state that is conducive for testing. We describe several specific solutions to this problem:\n\nPut repeated code in a constructor-type helper function. Memoise it, if construction is demonstrably slow.\nIf the repeated code has side effects, write a custom local_*() function to do what’s needed and clean up afterwards.\nIf the above approaches are too slow or awkward and the thing you need is fairly stable, save it as a static file and load it.\n\n\n16.1.1 Create useful_things with a helper function\nIs it fiddly to create a useful_thing? Does it take several lines of code, but not much time or memory? In that case, write a helper function to create a useful_thing on-demand:\n\nnew_useful_thing <- function() {\n  # your fiddly code to create a useful_thing goes here\n}\n\nand call that helper in the affected tests:\n\ntest_that(\"foofy() does this\", {\n  useful_thing1 <- new_useful_thing()\n  expect_equal(foofy(useful_thing1, x = \"this\"), EXPECTED_FOOFY_OUTPUT)\n})\n\ntest_that(\"foofy() does that\", {\n  useful_thing2 <- new_useful_thing()\n  expect_equal(foofy(useful_thing2, x = \"that\"), EXPECTED_FOOFY_OUTPUT)\n})\n\nWhere should the new_useful_thing() helper be defined? This comes back to what we outlined in Section 15.3. Test helpers can be defined below R/, just like any other internal utility in your package. Another popular location is in a test helper file, e.g. tests/testthat/helper.R. A key feature of both options is that the helpers are made available to you during interactive maintenance via devtools::load_all().\nIf it’s fiddly AND costly to create a useful_thing, your helper function could even use memoisation to avoid unnecessary re-computation. Once you have a helper like new_useful_thing(), you often discover that it has uses beyond testing, e.g. behind-the-scenes in a vignette. Sometimes you even realize you should just define it below R/ and export and document it, so you can use it freely in documentation and tests.\n\n16.1.2 Create (and destroy) a “local” useful_thing\n\nSo far, our example of a useful_thing was a regular R object, which is cleaned-up automatically at the end of each test. What if the creation of a useful_thing has a side effect on the local file system, on a remote resource, R session options, environment variables, or the like? Then your helper function should create a useful_thing and clean up afterwards. Instead of a simple new_useful_thing() constructor, you’ll write a customized function in the style of withr’s local_*() functions:\n\nlocal_useful_thing <- function(..., env = parent.frame()) {\n  # your fiddly code to create a useful_thing goes here\n  withr::defer(\n    # your fiddly code to clean up after a useful_thing goes here\n    envir = env\n  )\n}\n\nUse it in your tests like this:\n\ntest_that(\"foofy() does this\", {\n  useful_thing1 <- local_useful_thing()\n  expect_equal(foofy(useful_thing1, x = \"this\"), EXPECTED_FOOFY_OUTPUT)\n})\n\ntest_that(\"foofy() does that\", {\n  useful_thing2 <- local_useful_thing()\n  expect_equal(foofy(useful_thing2, x = \"that\"), EXPECTED_FOOFY_OUTPUT)\n})\n\nWhere should the local_useful_thing() helper be defined? All the advice given above for new_useful_thing() applies: define it below R/ or in a test helper file.\nTo learn more about writing custom helpers like local_useful_thing(), see the testthat vignette on test fixtures.\n\n16.1.3 Store a concrete useful_thing persistently\nIf a useful_thing is costly to create, in terms of time or memory, maybe you don’t actually need to re-create it for each test run. You could make the useful_thing once, store it as a static test fixture, and load it in the tests that need it. Here’s a sketch of how this could look:\n\ntest_that(\"foofy() does this\", {\n  useful_thing1 <- readRDS(test_path(\"fixtures\", \"useful_thing1.rds\"))\n  expect_equal(foofy(useful_thing1, x = \"this\"), EXPECTED_FOOFY_OUTPUT)\n})\n\ntest_that(\"foofy() does that\", {\n  useful_thing2 <- readRDS(test_path(\"fixtures\", \"useful_thing2.rds\"))\n  expect_equal(foofy(useful_thing2, x = \"that\"), EXPECTED_FOOFY_OUTPUT)\n})\n\nNow we can revisit a file listing from earlier, which addressed exactly this scenario:\n.\n├── ...\n└── tests\n    ├── testthat\n    │   ├── fixtures\n    │   │   ├── make-useful-things.R\n    │   │   ├── useful_thing1.rds\n    │   │   └── useful_thing2.rds\n    │   ├── helper.R\n    │   ├── setup.R\n    │   └── (all the test files)\n    └── testthat.R\nThis shows static test files stored in tests/testthat/fixtures/, but also notice the companion R script, make-useful-things.R. From data analysis, we all know there is no such thing as a script that is run only once. Refinement and iteration is inevitable. This also holds true for test objects like useful_thing1.rds. We highly recommend saving the R code used to create your test objects, so that they can be re-created as needed."
  },
  {
    "objectID": "testing-advanced.html#building-your-own-testing-tools",
    "href": "testing-advanced.html#building-your-own-testing-tools",
    "title": "16  Advanced testing techniques",
    "section": "\n16.2 Building your own testing tools",
    "text": "16.2 Building your own testing tools\nLet’s return to the topic of duplication in your test code. We’ve encouraged you to have a higher tolerance for repetition in test code, in the name of making your tests obvious. But there’s still a limit to how much repetition to tolerate. We’ve covered techniques such as loading static objects with test_path(), writing a constructor like new_useful_thing(), or implementing a test fixture like local_useful_thing(). There are even more types of test helpers that can be useful in certain situations.\n\n16.2.1 Helper defined inside a test\nConsider this test for the str_trunc() function in stringr:\n\n# from stringr (hypothetically)\ntest_that(\"truncations work for all sides\", {\n  expect_equal(\n    str_trunc(\"This string is moderately long\", width = 20, side = \"right\"),\n    \"This string is mo...\"\n  )\n  expect_equal(\n    str_trunc(\"This string is moderately long\", width = 20, side = \"left\"),\n    \"...s moderately long\"\n  )\n  expect_equal(\n    str_trunc(\"This string is moderately long\", width = 20, side = \"center\"),\n    \"This stri...ely long\"\n  )\n})\n\nThere’s a lot of repetition here, which increases the chance of copy / paste errors and generally makes your eyes glaze over. Sometimes it’s nice to create a hyper-local helper, inside the test. Here’s how the test actually looks in stringr\n\n# from stringr (actually)\ntest_that(\"truncations work for all sides\", {\n\n  trunc <- function(direction) str_trunc(\n    \"This string is moderately long\",\n    direction,\n    width = 20\n  )\n\n  expect_equal(trunc(\"right\"),   \"This string is mo...\")\n  expect_equal(trunc(\"left\"),    \"...s moderately long\")\n  expect_equal(trunc(\"center\"),  \"This stri...ely long\")\n})\n\nA hyper-local helper like trunc() is particularly useful when it allows you to fit all the important business for each expectation on one line. Then your expectations can be read almost like a table of actual vs. expected, for a set of related use cases. Above, it’s very easy to watch the result change as we truncate the input from the right, left, and in the center.\nNote that this technique should be used in extreme moderation. A helper like trunc() is yet another place where you can introduce a bug, so it’s best to keep such helpers extremely short and simple.\n\n16.2.2 Custom expectations\nIf a more complicated helper feels necessary, it’s a good time to reflect on why that is. If it’s fussy to get into position to test a function, that could be a sign that it’s also fussy to use that function. Do you need to refactor it? If the function seems sound, then you probably need to use a more formal helper, defined outside of any individual test, as described earlier.\nOne specific type of helper you might want to create is a custom expectation. Here are two very simple ones from usethis:\n\nexpect_usethis_error <- function(...) {\n  expect_error(..., class = \"usethis_error\")\n}\n\nexpect_proj_file <- function(...) {\n  expect_true(file_exists(proj_path(...)))\n}\n\nexpect_usethis_error() checks that an error has the \"usethis_error\" class. expect_proj_file() is a simple wrapper around file_exists() that searches for the file in the current project. These are very simple functions, but the sheer amount of repetition and the expressiveness of their names makes them feel justified.\nIt is somewhat involved to make a proper custom expectation, i.e. one that behaves like the expectations built into testthat. We refer you to the Custom expectations vignette if you wish to learn more about that.\nFinally, it can be handy to know that testthat makes specific information available when it’s running:\n\n\nThe environment variable TESTTHAT is set to \"true\". testthat::is_testing() is a shortcut:\n\nis_testing <- function() {\n  Sys.getenv(\"TESTTHAT\")\n}\n\n\n\nThe package-under-test is available as the environment variable TESTTHAT_PKG and testthat::testing_package() is a shortcut:\n\ntesting_package <- function() {\n  Sys.getenv(\"TESTTHAT_PKG\")\n}\n\n\n\nIn some situations, you may want to exploit this information without taking a run-time dependency on testthat. In that case, just inline the source of these functions directly into your package."
  },
  {
    "objectID": "testing-advanced.html#when-testing-gets-hard",
    "href": "testing-advanced.html#when-testing-gets-hard",
    "title": "16  Advanced testing techniques",
    "section": "\n16.3 When testing gets hard",
    "text": "16.3 When testing gets hard\nDespite all the techniques we’ve covered so far, there remain situations where it still feels very difficult to write tests. In this section, we review more ways to deal with challenging situations:\n\nSkipping a test in certain situations\nMocking an external service\nDealing with secrets\n\n\n16.3.1 Skipping a test\nSometimes it’s impossible to perform a test - you may not have an internet connection or you may not have access to the necessary credentials. Unfortunately, another likely reason follows from this simple rule: the more platforms you use to test your code, the more likely it is that you won’t be able to run all of your tests, all of the time. In short, there are times when, instead of getting a failure, you just want to skip a test.\n\n16.3.1.1 testthat::skip()\n\nHere we use testthat::skip() to write a hypothetical custom skipper, skip_if_no_api():\n\nskip_if_no_api() <- function() {\n  if (api_unavailable()) {\n    skip(\"API not available\")\n  }\n}\n\ntest_that(\"foo api returns bar when given baz\", {\n  skip_if_no_api()\n  ...\n})\n\nskip_if_no_api() is a yet another example of a test helper and the advice already given about where to define it applies here too.\nskip()s and the associated reasons are reported inline as tests are executed and are also indicated clearly in the summary:\n\ndevtools::test()\n#> ℹ Loading abcde\n#> ℹ Testing abcde\n#> ✔ | F W S  OK | Context\n#> ✔ |         2 | blarg\n#> ✔ |     1   2 | foofy\n#> ────────────────────────────────────────────────────────────────────────────────\n#> Skip (test-foofy.R:6:3): foo api returns bar when given baz\n#> Reason: API not available\n#> ────────────────────────────────────────────────────────────────────────────────\n#> ✔ |         0 | yo                                                              \n#> ══ Results ═════════════════════════════════════════════════════════════════════\n#> ── Skipped tests  ──────────────────────────────────────────────────────────────\n#> • API not available (1)\n#> \n#> [ FAIL 0 | WARN 0 | SKIP 1 | PASS 4 ]\n#> \n#> 🥳\n\nSomething like skip_if_no_api() is likely to appear many times in your test suite. This is another occasion where it is tempting to DRY things out, by hoisting the skip() to the top-level of the file. However, we still lean towards calling skip_if_no_api() in each test where it’s needed.\n\n# we prefer this:\ntest_that(\"foo api returns bar when given baz\", {\n  skip_if_no_api()\n  ...\n})\n\ntest_that(\"foo api returns an errors when given qux\", {\n  skip_if_no_api()\n  ...\n})\n\n# over this:\nskip_if_no_api()\n\ntest_that(\"foo api returns bar when given baz\", {...})\n\ntest_that(\"foo api returns an errors when given qux\", {...})\n\nWithin the realm of top-level code in test files, having a skip() at the very beginning of a test file is one of the more benign situations. But once a test file does not fit entirely on your screen, it creates an implicit yet easy-to-miss connection between the skip() and individual tests.\n\n16.3.1.2 Built-in skip() functions\nSimilar to testthat’s built-in expectations, there is a family of skip() functions that anticipate some common situations. These functions often relieve you of the need to write a custom skipper. Here are some examples of the most useful skip() functions:\n\ntest_that(\"foo api returns bar when given baz\", {\n  skip_if(api_unavailable(), \"API not available\")\n  ...\n})\ntest_that(\"foo api returns bar when given baz\", {\n  skip_if_not(api_available(), \"API not available\")\n  ...\n})\n\nskip_if_not_installed(\"sp\")\nskip_if_not_installed(\"stringi\", \"1.2.2\")\n\nskip_if_offline()\nskip_on_cran()\nskip_on_os(\"windows\")\n\n\n16.3.1.3 Dangers of skipping\nOne challenge with skips is that they are currently completely invisible in CI — if you automatically skip too many tests, it’s easy to fool yourself that all your tests are passing when in fact they’re just being skipped! In an ideal world, your CI/CD would make it easy to see how many tests are being skipped and how that changes over time.\n2022-06-01: Recent changes to GitHub Actions mean that we will likely have better test reporting before the second edition of this book is published. Stay tuned!\nIt’s a good practice to regularly dig into the R CMD check results, especially on CI, and make sure the skips are as you expect. But this tends to be something you have to learn through experience.\n\n16.3.2 Mocking\nThe practice known as mocking is when we replace something that’s complicated or unreliable or out of our control with something simpler, that’s fully within our control. Usually we are mocking an external service, such as a REST API, or a function that reports something about session state, such as whether the session is interactive.\nThe classic application of mocking is in the context of a package that wraps an external API. In order to test your functions, technically you need to make a live call to that API to get a response, which you then process. But what if that API requires authentication or what if it’s somewhat flaky and has occasional downtime? It can be more productive to just pretend to call the API but, instead, to test the code under your control by processing a pre-recorded response from the actual API.\nOur main advice about mocking is to avoid it if you can. This is not an indictment of mocking, but just a realistic assessment that mocking introduces new complexity that is not always justified by the payoffs.\nSince most R packages do not need full-fledged mocking, we do not cover it here. Instead we’ll point you to the packages that represent the state-of-the-art for mocking in R today:\n\nmockery: https://github.com/r-lib/mockery\n\nmockr: https://krlmlr.github.io/mockr/\n\nhttptest: https://enpiar.com/r/httptest/\n\nhttptest2: https://enpiar.com/httptest2/\n\nwebfakes: https://webfakes.r-lib.org\n\n\nNote also that, at the time of writing, it seems likely that the testthat package will re-introduce some mocking capabilities (after previously getting out of the mocking business once already). Version v3.1.7 has two new experimental functions, testthat::with_mocked_bindings() and testthat::local_mocked_bindings().\n\n16.3.3 Secrets\nAnother common challenge for packages that wrap an external service is the need to manage credentials. Specifically, it is likely that you will need to provide a set of test credentials to fully test your package.\nOur main advice here is to design your package so that large parts of it can be tested without live, authenticated access to the external service.\nOf course, you will still want to be able to test your package against the actual service that it wraps, in environments that support secure environment variables. Since this is also a very specialized topic, we won’t go into more detail here. Instead we refer you to the Wrapping APIs vignette in the httr2 package, which offers substantial support for secret management."
  },
  {
    "objectID": "testing-advanced.html#special-considerations-for-cran-packages",
    "href": "testing-advanced.html#special-considerations-for-cran-packages",
    "title": "16  Advanced testing techniques",
    "section": "\n16.4 Special considerations for CRAN packages",
    "text": "16.4 Special considerations for CRAN packages\nCRAN runs R CMD check on all contributed packages, both upon submission and on a regular basis after acceptance. This check includes, but is not limited to, your testthat tests. We discuss the general challenge of preparing your package to face all of CRAN’s check “flavors” in Section 23.4.1. Here we focus on CRAN-specific considerations for your test suite.\nWhen a package runs afoul of the CRAN Repository Policy (https://cran.r-project.org/web/packages/policies.html), the test suite is very often the culprit (although not always). If your package is destined for CRAN, this should influence how you write your tests and how (or whether) they will be run on CRAN.\n\n16.4.1 Skip a test\nIf a specific test simply isn’t appropriate to be run by CRAN, include skip_on_cran() at the very start.\n\ntest_that(\"some long-running thing works\", {\n  skip_on_cran()\n  # test code that can potentially take \"a while\" to run  \n})\n\nUnder the hood, skip_on_cran() consults the NOT_CRAN environment variable. Such a test will only run when NOT_CRAN has been explicitly defined as \"true\". This variable is set by devtools and testthat, allowing those tests to run in environments where you expect success (and where you can tolerate and troubleshoot occasional failure).\nIn particular, the GitHub Actions workflows that we recommend in Section 21.2.1 will run tests with NOT_CRAN = \"true\". For certain types of functionality, there is no practical way to test it on CRAN and your own checks, on GitHub Actions or an equivalent continuous integration service, are your best method of quality assurance.\nThere are even rare cases where it makes sense to maintain tests outside of your package altogether. The tidymodels team uses this strategy for integration-type tests of their whole ecosystem that would be impossible to host inside an individual CRAN package.\n\n16.4.2 Speed\nYour tests need to run relatively quickly - ideally, less than a minute, in total. Use skip_on_cran() in a test that is unavoidably long-running.\n\n16.4.3 Reproducibility\nBe careful about testing things that are likely to be variable on CRAN machines. It’s risky to test how long something takes (because CRAN machines are often heavily loaded) or to test parallel code (because CRAN runs multiple package tests in parallel, multiple cores will not always be available). Numerical precision can also vary across platforms, so use expect_equal() unless you have a specific reason for using expect_identical().\n\n16.4.4 Flaky tests\nDue to the scale at which CRAN checks packages, there is basically no latitude for a test that’s “just flaky”, i.e. sometimes fails for incidental reasons. CRAN does not process your package’s test results the way you do, where you can inspect each failure and exercise some human judgment about how concerning it is.\nIt’s probably a good idea to eliminate flaky tests, just for your own sake! But if you have valuable, well-written tests that are prone to occasional nuisance failure, definitely put skip_on_cran() at the start.\nThe classic example is any test that accesses a website or web API. Given that any web resource in the world will experience occasional downtime, it’s best to not let such tests run on CRAN. The CRAN Repository Policy says:\n\nPackages which use Internet resources should fail gracefully with an informative message if the resource is not available or has changed (and not give a check warning nor error).\n\nOften making such a failure “graceful” would run counter to the behaviour you actually want in practice, i.e. you would want your user to get an error if their request fails. This is why it is usually more practical to test such functionality elsewhere.\nRecall that snapshot tests (Chapter 14), by default, are also skipped on CRAN. You typically use such tests to monitor, e.g., how various informational messages look. Slight changes in message formatting are something you want to be alerted to, but do not indicate a major defect in your package. This is the motivation for the default skip_on_cran() behaviour of snapshot tests.\nFinally, flaky tests cause problems for the maintainers of your dependencies. When the packages you depend on are updated, CRAN runs R CMD check on all reverse dependencies, including your package. If your package has flaky tests, your package can be the reason another package does not clear CRAN’s incoming checks and can delay its release.\n\n16.4.5 Process and file system hygiene\nIn Section 15.3.7, we urged you to only write into the session temp directory and to clean up after yourself. This practice makes your test suite much more maintainable and predictable. For packages that are (or aspire to be) on CRAN, this is absolutely required per the CRAN repository policy:\n\nPackages should not write in the user’s home filespace (including clipboards), nor anywhere else on the file system apart from the R session’s temporary directory (or during installation in the location pointed to by TMPDIR: and such usage should be cleaned up)…. Limited exceptions may be allowed in interactive sessions if the package obtains confirmation from the user.\n\nSimilarly, you should make an effort to be hygienic with respect to any processes you launch:\n\nPackages should not start external software (such as PDF viewers or browsers) during examples or tests unless that specific instance of the software is explicitly closed afterwards.\n\nAccessing the clipboard is the perfect storm that potentially runs afoul of both of these guidelines, as the clipboard is considered part of the user’s home filespace and, on Linux, can launch an external process (e.g. xsel or xclip). Therefore it is best to turn off any clipboard functionality in your tests (and to ensure that, during authentic usage, your user is clearly opting-in to that)."
  },
  {
    "objectID": "man.html#roxygen2-basics",
    "href": "man.html#roxygen2-basics",
    "title": "17  Function documentation",
    "section": "\n17.1 roxygen2 basics",
    "text": "17.1 roxygen2 basics\nTo get started, we’ll work through the basic roxygen2 workflow and discuss the overall structure of roxygen2 comments, which are organised into blocks and tags. We also highlight the biggest wins of using markdown with roxygen2.\n\n17.1.1 The documentation workflow\nUnlike with testthat, there’s no obvious opening move to declare that you’re going to use roxygen2 for documentation. That’s because the use of roxygen2 is purely a matter of your development workflow. It has no effect on, e.g., how a package gets checked or built. We think the roxygen approach is the best way to generate your .Rd files, but officially R only cares about the files themselves, not how they came to be.\nYour documentation workflow truly begins when you start to add roxygen comments above your functions. Roxygen comment lines always start with #' , the usual # for a comment, followed immediately by a single quote ':\n\n#' Add together two numbers\n#' \n#' @param x A number.\n#' @param y A number.\n#' @returns A numeric vector.\n#' @examples\n#' add(1, 1)\n#' add(10, 1)\nadd <- function(x, y) {\n  x + y\n}\n\n\n\n\n\n\n\nRStudio\n\n\n\nUsually you write your function first, then its documentation. Once the function definition exists, put your cursor somewhere in it and do Code > Insert Roxygen Skeleton to get a great head start on the roxygen comment.\n\n\nOnce you have at least one roxygen comment, run devtools::document() to generate (or update) your package’s .Rd files3. Under the hood, this ultimately calls roxygen2::roxygenise(). The roxygen block above generates a man/add.Rd file that looks like this:\n% Generated by roxygen2: do not edit by hand\n% Please edit documentation in R/add.R\n\\name{add}\n\\alias{add}\n\\title{Add together two numbers}\n\\usage{\nadd(x, y)\n}\n\\arguments{\n\\item{x}{A number.}\n\n\\item{y}{A number.}\n}\n\\value{\nA numeric vector.\n}\n\\description{\nAdd together two numbers\n}\n\\examples{\nadd(1, 1)\nadd(10, 1)\n}\n\n\n\n\n\n\nRStudio\n\n\n\nYou can also run devtools::document() with the keyboard shortcut Ctrl/Cmd + Shift + D or via the Build menu or pane.\n\n\nIf you’ve used LaTeX before, this should look vaguely familiar since the .Rd format is loosely based on LaTeX. If you are interested in the .Rd format, you can read more in Writing R Extensions. But generally you’ll never need to look at .Rd files, except to commit them to your package’s Git repository.\nHow does this .Rd file correspond to the documentation you see in R? When you run ?add, help(\"add\"), or example(\"add\"), R looks for an .Rd file containing \\alias{add}. It then parses the file, converts it into HTML, and displays it. Figure 17.1 shows how this help topic would look in RStudio:\n\n\n\n\nFigure 17.1: Help topic rendered to HTML.\n\n\n\n\n\n\n\n\n\n\nR CMD check warning\n\n\n\nYou should document all exported functions and datasets. Otherwise, you’ll get this warning from R CMD check:\nW  checking for missing documentation entries (614ms)\n  Undocumented code objects:\n    ‘somefunction’\n  Undocumented data sets:\n    ‘somedata’\n  All user-level objects in a package should have documentation entries.\nConversely, you probably don’t want to document unexported functions. If you want to use roxygen comments for internal documentation, include the @noRd tag to suppress the creation of the .Rd file.\n\n\nThis is also a good time to explain something you may have noticed in your DESCRIPTION file:\nRoxygen: list(markdown = TRUE)\ndevtools/usethis includes this by default when initiating a DESCRIPTION file and it gives roxygen2 a heads-up that your package uses markdown syntax in its roxygen comments.4\nThe default help-seeking process looks inside installed packages, so to see your package’s documentation during development, devtools overrides the usual help functions with modified versions that know to consult the current source package. To activate these overrides, you’ll need to run devtools::load_all() at least once. If it feels like your edits to the roxygen comments aren’t having an effect, double check that you have actually regenerated the .Rd files with devtools::document() and that you’ve loaded your package. When you call ?function, you should see “Rendering development documentation …”.\nTo summarize, there are four steps in the basic roxygen2 workflow:\n\nAdd roxygen2 comments to your .R files.\nRun devtools::document() or press Ctrl/Cmd + Shift + D to convert roxygen2 comments to .Rd files.\nPreview documentation with ?function.\nRinse and repeat until the documentation looks the way you want.\n\n17.1.2 roxygen2 comments, blocks, and tags\nNow that you understand the basic workflow, we’ll go into more detail about the syntax. roxygen2 comments start with #' and all the roxygen2 comments preceding a function are collectively called a block. Blocks are broken up by tags, which look like @tagName tagValue, and the content of a tag extends from the end of the tag name to the start of the next tag5. A block can contain text before the first tag which is called the introduction. By default, each block generates a single documentation topic, i.e. a single .Rd file6 in the man/ directory .\nThroughout this chapter we’ll show you roxygen2 comments from real tidyverse packages, focusing on stringr, since the functions there tend to be fairly straightforward, leading to documentation that’s understandable with relatively little context. We attach stringr here so that its functions are hyperlinked in the rendered book (more on that in section Section 17.1.3).\n\nlibrary(stringr)\n\nHere’s a simple first example: the documentation for str_unique().\n\n#' Remove duplicated strings\n#'\n#' `str_unique()` removes duplicated values, with optional control over\n#' how duplication is measured.\n#'\n#' @param string Input vector. Either a character vector, or something\n#'  coercible to one.\n#' @param ... Other options used to control matching behavior between duplicate\n#'   strings. Passed on to [stringi::stri_opts_collator()].\n#' @returns A character vector, usually shorter than `string`.\n#' @seealso [unique()], [stringi::stri_unique()] which this function wraps.\n#' @examples\n#' str_unique(c(\"a\", \"b\", \"c\", \"b\", \"a\"))\n#'\n#' # Use ... to pass additional arguments to stri_unique()\n#' str_unique(c(\"motley\", \"mötley\", \"pinguino\", \"pingüino\"))\n#' str_unique(c(\"motley\", \"mötley\", \"pinguino\", \"pingüino\"), strength = 1)\n#' @export\nstr_unique <- function(string, ...) {\n  ...\n}\n\nHere the introduction includes the title (“Remove duplicated strings”) and a basic description of what the function does. The introduction is followed by five tags: two @params, one @returns, one @seealso, one @examples, and one @export.\nNote that the block has an intentional line length (typically the same as that used for the surrounding R code) and the second and subsequent lines of the long @param tag are indented, which makes the entire block easier to scan. You can get more roxygen2 style advice in the tidyverse style guide.\n\n\n\n\n\n\nRStudio\n\n\n\nIt can be aggravating to manually manage the line length of roxygen comments, so be sure to try out Code > Reflow Comment (Ctrl/Cmd+Shift+/).\n\n\nNote also that the order in which tags appear in your roxygen comments (or even in handwritten .Rd files) does not dictate the order in rendered documentation. The order of presentation is determined by tooling within base R.\nThe following sections go into more depth for the most important tags. We start with the introduction, which provides the title, description, and details. Then we cover the inputs (the function arguments), outputs (the return value), and examples. Next we discuss links and cross-references, then finish off with techniques for sharing documentation between topics.\n\n17.1.3 Key markdown features\nFor the most part, general markdown and R Markdown knowledge suffice for taking advantage of markdown in roxygen2. But there are a few pieces of syntax that are so important we want to highlight them here. You’ll see these in many of the examples in this chapter.\nBackticks for inline code: Use backticks to format a piece of text as code, i.e. in a fixed width font. Example:\n\n#' I like `thisfunction()`, because it's great.\n\nSquare brackets for an auto-linked function: Enclose text like somefunction() and somepackage::somefunction() in square brackets to get an automatic link to that function’s documentation. Be sure to include the trailing parentheses, because it’s good style and and it causes the function to be formatted as code, i.e. you don’t need to add backticks. Example:\n\n#' It's obvious that `thisfunction()` is better than [otherpkg::otherfunction()]\n#' or even our own [olderfunction()].\n\nVignettes: If you refer to a vignette with an inline call to vignette(\"some-topic\"), it serves a dual purpose. First, this is literally the R code you would execute to view a vignette locally. But wait there’s more! In many rendered contexts, this automatically becomes a hyperlink to that same vignette in roxygen2’s pkgdown website. Here we use that to link to some very relevant vignettes7:\n\nvignette(\"rd-formatting\", package = \"roxygen2\")\nvignette(\"reuse\", package = \"roxygen2\")\nvignette(\"linking\", package = \"pkgdown\")\n\nLists: Bullet lists break up the dreaded “wall of text” and can make your documentation easier to scan. You can use them in the description of the function or of an argument and also for the return value. It is not necessary to include a blank line before the list, but that is also allowed.\n\n#' Best features of `thisfunction()`:\n#' * Smells nice\n#' * Has good vibes"
  },
  {
    "objectID": "man.html#title-description-details",
    "href": "man.html#title-description-details",
    "title": "17  Function documentation",
    "section": "\n17.2 Title, description, details",
    "text": "17.2 Title, description, details\nThe introduction provides a title, description, and, optionally, details, for the function. While it’s possible to use explicit tags in the introduction, we usually rely on implicit tags when possible:\n\nThe title is taken from the first sentence. It should be written in sentence case, not end in a full stop, and be followed by a blank line. The title is shown in various function indexes (e.g. help(package = \"somepackage\")) and is what the user will usually see when browsing multiple functions.\nThe description is taken from the next paragraph. It’s shown at the top of documentation and should briefly describe the most important features of the function.\nAdditional details are anything after the description. Details are optional, but can be any length so are useful if you want to dig deep into some important aspect of the function. Note that, even though the details come right after the description in the introduction, they appear much later in rendered documentation.\n\nThe following sections describe each component in more detail, and then discuss a few useful related tags.\n\n17.2.1 Title\nWhen writing the title, it’s useful to think about how it will appear in the reference index. When a user skims the index, how will they know which functions will solve their current problem? This requires thinking about what your functions have in common (which doesn’t need to be repeated in every title) and what is unique to that function (which should be highlighted in the title).\nWhen we wrote this chapter, we found the function titles for stringr to be somewhat disappointing. But they provide a useful negative case study:\n\n\nstr_detect(): Detect the presence or absence of a pattern in a string\n\nstr_extract(): Extract matching patterns from a string\n\nstr_locate(): Locate the position of patterns in a string\n\nstr_match(): Extract matched groups from a string\n\nThere’s a lot of repetition (“pattern”, “from a string”) and the verb used for the function name is repeated in the title, so if you don’t understand the function already, the title seems unlikely to help much. Hopefully we’ll have improved those titles by the time you read this!\nIn contrast, these titles from dplyr are much better8:\n\n\nmutate(): Create, modify, and delete columns\n\nsummarise(): Summarise each group down to one row\n\nfilter(): Keep rows that match a condition\n\nselect(): Keep or drop columns using their names and types\n\narrange(): Order rows using column values\n\nHere we try to succinctly describe what the function does, making sure to describe whether it affects rows, columns, or groups. We do our best to use synonyms, instead of repeating the function name, to hopefully give folks another chance to understand the intent of the function.\n\n17.2.2 Description\nThe purpose of the description is to summarize the goal of the function, usually in a single paragraph. This can be challenging for simple functions, because it can feel like you’re just repeating the title of the function. Try to find a slightly different wording, if you can. It’s okay if this feels a little repetitive; it’s often useful for users to see the same thing expressed in two different ways. It’s a little extra work, but the extra effort is often worth it. Here’s the description for str_detect():\n\n#' Detect the presence/absence of a match\n#'\n#' `str_detect()` returns a logical vector with `TRUE` for each element of\n#' `string` that matches `pattern` and `FALSE` otherwise. It's equivalent to\n#' `grepl(pattern, string)`.\n\nIf you want more than one paragraph, you must use an explicit @description tag to prevent the second (and subsequent) paragraphs from being turned into the @details. Here’s a two-paragraph @description from str_view():\n\n#' View strings and matches\n#'\n#' @description\n#' `str_view()` is used to print the underlying representation of a string and\n#' to see how a `pattern` matches.\n#'\n#' Matches are surrounded by `<>` and unusual whitespace (i.e. all whitespace\n#' apart from `\" \"` and `\"\\n\"`) are surrounded by `{}` and escaped. Where\n#' possible, matches and unusual whitespace are coloured blue and `NA`s red.\n\nHere’s another example from str_like(), which has a bullet list in @description:\n\n#' Detect a pattern in the same way as `SQL`'s `LIKE` operator\n#'\n#' @description\n#' `str_like()` follows the conventions of the SQL `LIKE` operator:\n#'\n#' * Must match the entire string.\n#' * `_` matches a single character (like `.`).\n#' * `%` matches any number of characters (like `.*`).\n#' * `\\%` and `\\_` match literal `%` and `_`.\n#' * The match is case insensitive by default.\n\nBasically, if you’re going to include an empty line in your description, you’ll need to use an explicit @description tag.\nFinally, it’s often particularly hard to write a good description if you’ve just written the function, because the purpose often seems very obvious. Do your best, and then come back later, when you’ve forgotten exactly what the function does. Once you’ve re-derived what the function does, you’ll be able to write a better description.\n\n17.2.3 Details\nThe @details are just any additional details or explanation that you think your function needs. Most functions don’t need details, but some functions need a lot. If you have a lot of information to convey, it’s a good idea to use informative markdown headings to break the details up into manageable sections9. Here’s an example from dplyr::mutate(). We’ve elided some of the details to keep this example short, but you should still get a sense of how we used headings to break up the content in to skimmable chunks:\n\n#' Create, modify, and delete columns\n#'\n#' `mutate()` creates new columns that are functions of existing variables.\n#' It can also modify (if the name is the same as an existing\n#' column) and delete columns (by setting their value to `NULL`).\n#'\n#' @section Useful mutate functions:\n#'\n#' * [`+`], [`-`], [log()], etc., for their usual mathematical meanings\n#' \n#' ...\n#'\n#' @section Grouped tibbles:\n#'\n#' Because mutating expressions are computed within groups, they may\n#' yield different results on grouped tibbles. This will be the case\n#' as soon as an aggregating, lagging, or ranking function is\n#' involved. Compare this ungrouped mutate:\n#' \n#' ...\n\nThis is a good time to remind ourselves that, even though a heading like Useful mutate functions in the example above comes immediately after the description in the roxygen block, the content appears much later in the rendered documentation. The details (whether they use section headings or not) appear after the function usage, arguments, and return value."
  },
  {
    "objectID": "man.html#arguments",
    "href": "man.html#arguments",
    "title": "17  Function documentation",
    "section": "\n17.3 Arguments",
    "text": "17.3 Arguments\nFor most functions, the bulk of your work will go towards documenting how each argument affects the output of the function. For this purpose, you’ll use @param (short for parameter, a synonym of argument) followed by the argument name and a description of its action.\nThe highest priority is to provide a succinct summary of the allowed inputs and what the parameter does. For example, here’s how str_detect() documents string:\n\n#' @param string Input vector. Either a character vector, or something\n#'  coercible to one.\n\nAnd here are three of the arguments to str_flatten():\n\n#' @param collapse String to insert between each piece. Defaults to `\"\"`.\n#' @param last Optional string to use in place of the final separator.\n#' @param na.rm Remove missing values? If `FALSE` (the default), the result \n#'   will be `NA` if any element of `string` is `NA`.\n\nNote that @param collapse and @param na.rm describe their default arguments. This is often a good practice because the function usage (which shows the default values) and the argument description are often quite far apart in the rendered documentation. But there are downsides. The main one is that this duplication means you’ll need to make updates in two places if you change the default value; we believe this small amount of extra work is worth it to make the life of the user easier.\nIf an argument has a fixed set of possible parameters, you should list them. If they’re simple, you can just list them in a sentence, like in str_trim():\n\n#' @param side Side on which to remove whitespace: `\"left\"`, `\"right\"`, or\n#'   `\"both\"` (the default).\n\nIf they need more explanation, you might use a bulleted list, as in str_wrap():\n\n#' @param whitespace_only A boolean.\n#'   * `TRUE` (the default): wrapping will only occur at whitespace.\n#'   * `FALSE`: can break on any non-word character (e.g. `/`, `-`).\n\nThe documentation for most arguments will be relatively short, often one or two sentences. But you should take as much space as you need, and you’ll see some examples of multi-paragraph argument docs shortly.\n\n17.3.1 Multiple arguments\nIf the behavior of multiple arguments is tightly coupled, you can document them together by separating the names with commas (with no spaces). For example, x and y are interchangeable in str_equal(), so they’re documented together:\n\n#' @param x,y A pair of character vectors.\n\nIn str_sub(), start and end define the range of characters to replace. But instead of supplying both, you can use just start if you pass in a two-column matrix. So it makes sense to document them together:\n\n#' @param start,end A pair of integer vectors defining the range of characters\n#'   to extract (inclusive).\n#'\n#'   Alternatively, instead of a pair of vectors, you can pass a matrix to\n#'   `start`. The matrix should have two columns, either labelled `start`\n#'   and `end`, or `start` and `length`.\n\nIn str_wrap(), indent and exdent define the indentation for the first line and all subsequent lines, respectively:\n\n#' @param indent,exdent A non-negative integer giving the indent for the\n#'   first line (`indent`) and all subsequent lines (`exdent`).\n\n\n17.3.2 Inheriting arguments\nIf your package contains many closely related functions, it’s common for them to have arguments that share the same name and meaning. It would be both annoying and error prone to copy and paste the same @param documentation to every function, so roxygen2 provides @inheritParams which allows you to inherit argument documentation from another function, possibly even in another package.\nstringr uses @inheritParams extensively because most functions have string and pattern arguments. The detailed and definitive documentation belongs to str_detect():\n\n#' @param string Input vector. Either a character vector, or something\n#'  coercible to one.\n#' @param pattern Pattern to look for.\n#'\n#'   The default interpretation is a regular expression, as described in\n#'   `vignette(\"regular-expressions\")`. Use [regex()] for finer control of the\n#'   matching behaviour.\n#'\n#'   Match a fixed string (i.e. by comparing only bytes), using\n#'   [fixed()]. This is fast, but approximate. Generally,\n#'   for matching human text, you'll want [coll()] which\n#'   respects character matching rules for the specified locale.\n#'\n#'   Match character, word, line and sentence boundaries with\n#'   [boundary()]. An empty pattern, \"\", is equivalent to\n#'   `boundary(\"character\")`.\n\nThen the other stringr functions use @inheritParams str_detect to get this detailed documentation for string and pattern without having to duplicate that text.\n@inheritParams only inherits docs for arguments that the function actually uses and that aren’t already documented, so you can document some arguments locally and inherit others. str_match() uses this to inherit str_detect()’s standard documentation for the string argument, while providing its own specialized documentation for pattern:\n\n#' @inheritParams str_detect\n#' @param pattern Unlike other stringr functions, `str_match()` only supports\n#'   regular expressions, as described `vignette(\"regular-expressions\")`. \n#'   The pattern should contain at least one capturing group.\n\nNow that we’ve discussed default values and inheritance we can bring up one more dilemma. Sometimes there’s tension between giving detailed information on an argument (acceptable values, default value, how the argument is used, etc.) and making the documentation amenable to reuse in other functions (which might differ in some specifics). This can motivate you to assess whether it’s truly worth it for related functions to handle the same input in different ways or if standardization would be beneficial.\nYou can inherit documentation from a function in another package by using the standard :: notation, i.e. @inheritParams anotherpackage::function. This does introduce one small annoyance: now the documentation for your package is no longer self-contained and the version of anotherpackage can affect the generated docs. Beware of spurious diffs introduced by contributors who run devtools::document() with a different installed version of the inherited-from package."
  },
  {
    "objectID": "man.html#sec-man-returns",
    "href": "man.html#sec-man-returns",
    "title": "17  Function documentation",
    "section": "\n17.4 Return value",
    "text": "17.4 Return value\nA function’s output is as important as its inputs. Documenting the output is the job of the @returns10 tag. Here the priority is to describe the overall “shape” of the output, i.e. what sort of object it is, and its dimensions (if that makes sense). For example, if your function returns a vector you might describe its type and length, or if your function returns a data frame you might describe the names and types of the columns and the expected number of rows.\nThe @returns documentation for functions in stringr is straightforward because almost all functions return some type of vector with the same length as one of the inputs. For example, here’s how str_like() describes its output:\n\n#' @returns A logical vector the same length as `string`.\n\nA more complicated case is the joint documentation for str_locate() and str_locate_all()11. str_locate() returns an integer matrix, and str_locate_all() returns a list of matrices, so the text needs to describe what determines the rows and columns.\n\n#' @returns\n#' * `str_locate()` returns an integer matrix with two columns and\n#'   one row for each element of `string`. The first column, `start`,\n#'   gives the position at the start of the match, and the second column, `end`,\n#'   gives the position of the end.\n#'\n#'* `str_locate_all()` returns a list of integer matrices with the same\n#'   length as `string`/`pattern`. The matrices have columns `start` and `end`\n#'   as above, and one row for each match.\n#' @seealso\n#'   [str_extract()] for a convenient way of extracting matches,\n#'   [stringi::stri_locate()] for the underlying implementation.\n\nIn other cases it can be easier to figure out what to highlight by thinking about the set of functions and how they differ. For example, most dplyr functions return a data frame, so just saying @returns A data frame is not very useful. Instead, we tried to identify exactly what makes each function different. We decided it makes sense to describe each function in terms of how it affects the rows, the columns, the groups, and the attributes. For example, this describes the return value of dplyr::filter():\n\n#' @returns\n#' An object of the same type as `.data`. The output has the following properties:\n#'\n#' * Rows are a subset of the input, but appear in the same order.\n#' * Columns are not modified.\n#' * The number of groups may be reduced (if `.preserve` is not `TRUE`).\n#' * Data frame attributes are preserved.\n\n@returns is also a good place to describe any important warnings or errors that the user might see. For example readr::read_csv() mentions what happens if there are any parsing problems:\n\n#' @returns A [tibble()]. If there are parsing problems, a warning will alert you.\n#'   You can retrieve the full details by calling [problems()] on your dataset.\n\n\n\n\n\n\n\nSubmitting to CRAN\n\n\n\nFor your initial CRAN submission, all functions must document their return value. While this may not be scrutinized in subsequent submissions, it’s still a good practice. There’s currently no way to check that you’ve documented the return value of every function (we’re working on it) which is why you’ll notice some tidyverse functions lack output documentation. But we certainly aspire to provide this information across the board."
  },
  {
    "objectID": "man.html#sec-man-examples",
    "href": "man.html#sec-man-examples",
    "title": "17  Function documentation",
    "section": "\n17.5 Examples",
    "text": "17.5 Examples\nDescribing what a function does is great, but showing how it works is even better. That’s the role of the @examples tag, which uses executable R code to demonstrate what a function can do. Unlike other parts of the documentation where we’ve focused mainly on what you should write, here we’ll briefly give some content advice and then focus mainly on the mechanics.\nThe main dilemma with examples is that you must jointly satisfy two requirements:\n\nYour example code should be readable and realistic. Examples are documentation that you provide for the benefit of the user, i.e. a real human, working interactively, trying to get their actual work done with your package.\nYour example code must run without error and with no side effects in many non-interactive contexts over which you have limited or no control, such as when CRAN runs R CMD check or when your package website is built via GitHub Actions.\n\nIt turns out that there is often tension between these goals and you’ll need to find a way to make your examples as useful as you can for users, while also satisfying the requirements of CRAN (if that’s your goal) or other automated infrastructure.\nThe mechanics of examples are complex because they must never error and they’re executed in four different situations:\n\nInteractively using the example() function.\nDuring R CMD check on your computer, or another computer you control (e.g. in GitHub Actions).\nDuring R CMD check run by CRAN.\nWhen your pkgdown website is being built, often via GitHub Actions or similar.\n\nAfter discussing what to put in your examples, we’ll talk about keeping your examples self-contained, how to display errors if needed, handling dependencies, running examples conditionally, and alternatives to the @examples tag for including example code.\n\n\n\n\n\n\nRStudio\n\n\n\nWhen preparing .R scripts or .Rmd / .qmd reports, it’s handy to use Ctrl/Cmd + Enter or the Run button to send a line of R code to the console for execution. Happily, you can use the same workflow for executing and developing the @examples in your roxygen comments. Remember to do devtools::load_all() often, to stay synced with the package source.\n\n\n\n17.5.1 Contents\nUse examples to first show the basic operation of the function, then to highlight any particularly important properties. For example, str_detect() starts by showing a few simple variations and then highlights a feature that’s easy to miss: as well as passing a vector of strings and one pattern, you can also pass one string and vector of patterns.\n\n#' @examples\n#' fruit <- c(\"apple\", \"banana\", \"pear\", \"pineapple\")\n#' str_detect(fruit, \"a\")\n#' str_detect(fruit, \"^a\")\n#' str_detect(fruit, \"a$\")\n#' \n#' # Also vectorised over pattern\n#' str_detect(\"aecfg\", letters)\n\nTry to stay focused on the most important features without getting into the weeds of every last edge case: if you make the examples too long, it becomes hard for the user to find the key application that they’re looking for. If you find yourself writing very long examples, it may be a sign that you should write a vignette instead.\nThere aren’t any formal ways to break up your examples into sections but you can use sectioning comments that use many --- to create a visual breakdown. Here’s an example from tidyr::chop():\n\n#' @examples\n#' # Chop ----------------------------------------------------------------------\n#' df <- tibble(x = c(1, 1, 1, 2, 2, 3), y = 1:6, z = 6:1)\n#' # Note that we get one row of output for each unique combination of\n#' # non-chopped variables\n#' df %>% chop(c(y, z))\n#' # cf nest\n#' df %>% nest(data = c(y, z))\n#'\n#' # Unchop --------------------------------------------------------------------\n#' df <- tibble(x = 1:4, y = list(integer(), 1L, 1:2, 1:3))\n#' df %>% unchop(y)\n#' df %>% unchop(y, keep_empty = TRUE)\n\nStrive to keep the examples focused on the specific function that you’re documenting. If you can make the point with a familiar built-in dataset, like mtcars, do so. If you find yourself needing to do a bunch of setup to create a dataset or object to use in the example, it may be a sign that you need to create a package dataset or even a helper function. See Chapter 8, Section 8.3.2, and Section 16.1.1 for ideas. Making it easy to write (and read) examples will greatly improve the quality of your documentation.\nAlso, remember that examples are not tests. Examples should be focused on the authentic and typical usage you’ve designed for and that you want to encourage. The test suite is the more appropriate place to exhaustively exercise all of the arguments and to explore weird, pathological edge cases.\n\n17.5.2 Leave the world as you found it\nYour examples should be self-contained. For example, this means:\n\nIf you modify options(), reset them at the end of the example.\nIf you create a file, create it somewhere in tempdir(), and make sure to delete it at the end of the example.\nDon’t change the working directory.\nDon’t write to the clipboard (unless a user is present to provide some form of consent).\n\nThis has a lot of overlap with our recommendations for tests (see section Section 15.2.2) and even for the R functions in your package (see section Section 7.5). However, due to the way that examples are run during R CMD check the tools available for making examples self-contained are much more limited. Unfortunately, you can’t use the withr package or even on.exit() to schedule clean up, like restoring options or deleting a file. Instead, you’ll need to do it by hand. If you can avoid doing something that must then be undone, that is the best way to go and this is especially true for examples.\nThese constraints are often in tension with good documentation, if you’re trying to document a function that somehow changes the state of the world. For example, you have to “show your work”, i.e. all of your code, which means that your users will see all of the setup and teardown, even it is not typical for authentic usage. If you’re finding it hard to follow the rules, this might be another sign to switch to a vignette (section Chapter 18).\n\n\n\n\n\n\nSubmitting to CRAN\n\n\n\nMany of these constraints are also mentioned in the CRAN repository policy, which you must adhere to when submitting to CRAN. Use find in page to locate “malicious or anti-social” to see the details.\n\n\nAdditionally, you want your examples to send the user on a short walk, not a long hike. Examples need to execute relatively quickly so users can quickly see the results, it doesn’t take ages to build your website, automated checks happen quickly, and it doesn’t take up computing resources when submitting to CRAN.\n\n\n\n\n\n\nSubmitting to CRAN\n\n\n\nAll examples must run in under 10 minutes.\n\n\n\n17.5.3 Errors\nYour examples cannot throw any errors, so don’t include flaky code that can fail for reasons beyond your control. In particular, it’s best to avoid accessing websites, because R CMD check will fail whenever the website is down.\nWhat can you do if you want to include code that causes an error for the purposes of teaching? There are two basic options:\n\n\nYou can wrap the code in try() so that the error is shown, but doesn’t stop execution of the examples. For example, dplyr::bind_cols() uses try() to show you what happens if you attempt to column-bind two data frames with different numbers of rows:\n\n#' @examples\n#' ...\n#' # Row sizes must be compatible when column-binding\n#' try(bind_cols(tibble(x = 1:3), tibble(y = 1:2)))\n\n\n\nYou can wrap the code in \\dontrun{}12, so it is never run by example(). The example above would look like this if you used \\dontrun{} instead of try().\n\n#' # Row sizes must be compatible when column-binding\n#' \\dontrun{\n#' bind_cols(tibble(x = 1:3), tibble(y = 1:2)))\n#' }\n\n\n\nWe generally recommend using try() so that the reader can see an example of the error in action.\n\n\n\n\n\n\nSubmitting to CRAN\n\n\n\nFor the initial CRAN submission of your package, all functions must have at least one example and the example code can’t all be wrapped inside \\dontrun{}. If the code can only be run under specific conditions, use the techniques below to express those pre-conditions.\n\n\n\n17.5.4 Dependencies and conditional execution\nAn additional source of errors in examples is the use of external dependencies: you can only use packages in your examples that your package formally depends on (i.e. that appear in Imports or Suggests). Furthermore, example code is run in the user’s environment, not the package environment, so you’ll have to either explicitly attach the dependency with library() or refer to each function with ::. For example, dbplyr is a dplyr extension package, so all of its examples start with library(dplyr):\n\n#' @examples\n#' library(dplyr)\n#' df <- data.frame(x = 1, y = 2)\n#'\n#' df_sqlite <- tbl_lazy(df, con = simulate_sqlite())\n#' df_sqlite %>% summarise(x = sd(x, na.rm = TRUE)) %>% show_query()\n\nIn the past, we recommended only using code from suggested packages inside a block like this:\n\n#' @examples\n#' if (requireNamespace(\"suggestedpackage\", quietly = TRUE)) { \n#'   # some example code\n#' }\n\nWe no longer believe that approach is a good idea, because:\n\nOur policy is to expect that suggested packages are installed when running R CMD check13 and this informs what we do in examples, tests, and vignettes.\nThe cost of putting example code inside { ... } is high: you can no longer see intermediate results, such as when the examples are rendered in the package’s website. The cost of a package not being installed is low: users can usually recognize the associated error and resolve it themselves, i.e. by installing the missing package.\n\nIn other cases, your example code may depend on something other than a package. For example, if your examples talk to a web API, you probably only want to run them for an authenticated user, and never want such code to run on CRAN. In this case, you really do need conditional execution. The entry-level solution is to express this explicitly:\n\n#' @examples\n#' if (some_condition()) {\n#'   # some example code\n#' }\n\nThe condition could be quite general, such as interactive(), or very specific, such as a custom predicate function provided by your package. But this use of if() still suffers from the downside highlighted above, where the rendered examples don’t clearly show what’s going on inside the { … } block.\nThe @examplesIf tag is a great alternative to @examples in this case:\n\n#' @examplesIf some_condition()\n#' some_other_function()\n#' some_more_functions()\n\nThis looks almost like the snippet just above, but has several advantages:\n\nUsers won’t actually see the if() { … } machinery when they are reading your documentation from within R or on a pkgdown website. Users only see realistic code.\nThe example code renders fully in pkgdown.\nThe example code runs when it should and does not run when it should not.\nThis doesn’t run afoul of CRAN’s prohibition of putting all your example code inside \\dontrun{}.\n\nFor example, googledrive uses @examplesIf in almost every function, guarded by googledrive::drive_has_token(). Here’s how the examples for googledrive::drive_publish() begin:\n\n#' @examplesIf drive_has_token()\n#' # Create a file to publish\n#' file <- drive_example_remote(\"chicken_sheet\") %>%\n#'   drive_cp()\n#'\n#' # Publish file\n#' file <- drive_publish(file)\n#' ...\n\nThe example code doesn’t run on CRAN, because there’s no token. It does run when the pkgdown site is built, because we can set up a token securely. And, if a normal user executes this code, they’ll be prompted to sign in to Google, if they haven’t already.\n\n17.5.5 Intermixing examples and text\nAn alternative to examples is to use R Markdown code blocks elsewhere in your roxygen comments, either ```R if you just want to show some code, or ```{r} if you want the code to be run. These can be effective techniques but there are downsides to each:\n\nThe code in ```R blocks is never run; this means it’s easy to accidentally introduce syntax errors or to forget to update it when your package changes.\nThe code in ```{r} blocks is run every time you document the package. This has the nice advantage of including the output in the documentation (unlike examples), but the code can’t take very long to run or your iterative documentation workflow will become quite painful."
  },
  {
    "objectID": "man.html#re-using-documentation",
    "href": "man.html#re-using-documentation",
    "title": "17  Function documentation",
    "section": "\n17.6 Re-using documentation",
    "text": "17.6 Re-using documentation\nroxygen2 provides a number of features that allow you to reuse documentation across topics. They are documented in vignette(\"reuse\", package = \"roxygen2\"), so here we’ll focus on the three most important:\n\nDocumenting multiple functions in one topic.\nInheriting documentation from another topic.\nUsing child documents to share prose between topics, or to share between documentation topics and vignettes.\n\n\n17.6.1 Multiple functions in one topic\nBy default, each function gets its own documentation topic, but if two functions are very closely connected, you can combine the documentation for multiple functions into a single topic. For example, take str_length() and str_width(), which provide two different ways of computing the size of a string. As you can see from the description, both functions are documented together, because this makes it easier to see how they differ:\n\n#' The length/width of a string\n#'\n#' @description\n#' `str_length()` returns the number of codepoints in a string. These are\n#' the individual elements (which are often, but not always letters) that\n#' can be extracted with [str_sub()].\n#'\n#' `str_width()` returns how much space the string will occupy when printed\n#' in a fixed width font (i.e. when printed in the console).\n#'\n#' ...\nstr_length <- function(string) {\n  ...\n}\n\nTo merge the two topics, str_width() uses @rdname str_length to add its documentation to an existing topic:\n\n#' @rdname str_length\nstr_width <- function(string) {\n  ...\n}\n\nThis technique works best for functions that have a lot in common, i.e. similar return values and examples, in addition to similar arguments.\n\n17.6.2 Inheriting documentation\nIn other cases, functions in a package might share many related behaviors, but aren’t closely enough connected that you want to document them together. We’ve discussed @inheritParams above, but there are three variations that allow you to inherit other things:\n\n@inherit source_function will inherit all supported components from source_function().\n@inheritSection source_function Section title will inherit the single section with title “Section title” from source_function().\n@inheritDotParams automatically generates parameter documentation for ... for the common case where you pass ... on to another function.\n\nSee https://roxygen2.r-lib.org/articles/reuse.html#inheriting-documentation for more details.\n\n17.6.3 Child documents\nFinally, you can reuse the same .Rmd or .md document in the function documentation, README.Rmd, and vignettes by using R Markdown child documents. The syntax looks like this:\n\n#' ```{r child = \"man/rmd/filename.Rmd\"}\n#' ```\n\nThis is a feature we use very sparingly in the tidyverse, but one place we do use it is in dplyr, because a number of functions use the same syntax as select() and we want to provide all the info in one place:\n\n#' # Overview of selection features\n#'\n#' ```{r, child = \"man/rmd/overview.Rmd\"}\n#' ```\n\nThen man/rmd/overview.Rmd contains the repeated markdown:\nTidyverse selections implement a dialect of R where operators make\nit easy to select variables:\n\n- `:` for selecting a range of consecutive variables.\n- `!` for taking the complement of a set of variables.\n- `&` and `|` for selecting the intersection or the union of two\n  sets of variables.\n- `c()` for combining selections.\n\n...\nIf the Rmd file contains roxygen (Markdown-style) links to other help topics, then some care is needed. See https://roxygen2.r-lib.org/dev/articles/reuse.html#child-documents for details."
  },
  {
    "objectID": "man.html#sec-man-package-doc",
    "href": "man.html#sec-man-package-doc",
    "title": "17  Function documentation",
    "section": "\n17.7 Help topic for the package",
    "text": "17.7 Help topic for the package\nThis chapter focuses on function documentation, but remember you can document other things, as detailed in vignette(\"rd-other\", package = \"roxygen2\"). In particular, you can create a help topic for the package itself by documenting the special sentinel \"_PACKAGE\". The resulting .Rd file automatically pulls in information parsed from the DESCRIPTION, including title, description, list of authors, and useful URLs. This help topic appears alongside all your other topics and can also be accessed with package?pkgname, e.g. package?usethis, or even just ?usethis.\nWe recommend calling usethis::use_package_doc() to set up this package-level documentation in a dummy file R/{pkgname}-package.R, whose contents will look something like this:\n\n#' @keywords internal \n\"_PACKAGE\"\n\nThe R/{pkgname}-package.R file is the main reason we wanted to mention use_package_doc() and package-level documentation here. It turns out there are a few other package-wide housekeeping tasks for which this file is a very natural home. For example, it’s a sensible, central location for import directives, i.e. for importing individual functions from your dependencies or even entire namespaces. In Section 12.4.1, we recommend importing specific functions via usethis::use_import_from() and this function is designed to write the associated roxygen tags into the R/{pkgname}-package.R file created by use_package_doc(). So, putting it all together, this is a minimal example of how the R/{pkgname}-package.R file might look:\n\n#' @keywords internal \n\"_PACKAGE\"\n#> [1] \"_PACKAGE\"\n\n# The following block is used by usethis to automatically manage\n# roxygen namespace tags. Modify with care!\n## usethis namespace: start\n#' @importFrom glue glue_collapse\n## usethis namespace: end\nNULL\n#> NULL"
  },
  {
    "objectID": "vignettes.html#sec-vignettes-workflow-writing",
    "href": "vignettes.html#sec-vignettes-workflow-writing",
    "title": "18  Vignettes",
    "section": "\n18.1 Workflow for writing a vignette",
    "text": "18.1 Workflow for writing a vignette\nTo create your first vignette, run:\n\nusethis::use_vignette(\"my-vignette\")\n\nThis does the following:\n\nCreates a vignettes/ directory.\nAdds the necessary dependencies to DESCRIPTION, i.e. adds knitr to the VignetteBuilder field and adds both knitr and rmarkdown to Suggests.\nDrafts a vignette, vignettes/my-vignette.Rmd.\nAdds some patterns to .gitignore to ensure that files created as a side effect of previewing your vignettes are kept out of source control (we’ll say more about this later).\n\nThis draft document has the the key elements of an R Markdown vignette and leaves you in a position to add your content. You also call use_vignette() to create your second and all subsequent vignettes; it will just skip any setup that’s already been done.\nOnce you have the draft vignette, the workflow is straightforward:\n\nStart adding prose and code chunks to the vignette. Use devtools::load_all() as needed and use your usual interactive workflow for developing the code chunks.\n\nRender the entire vignette periodically.\nThis requires some intention, because unlike tests, by default, a vignette is rendered using the currently installed version of your package, not with the current source package, thanks to the initial call to library(yourpackage).\nOne option is to properly install your current source package with devtools::install() or, in RStudio, Ctrl/Cmd + Shift + B. Then use your usual workflow for rendering an .Rmd file. For example, press Ctrl/Cmd + Shift + K or click .\nOr you could properly install your package and request that vignettes be built, with install(build_vignettes = TRUE), then use browseVignettes().\nAnother option is to use devtools::build_rmd(\"vignettes/my-vignette.Rmd\") to render the vignette. This builds your vignette against a (temporarily installed) development version of your package.\nIt’s very easy to overlook this issue and be puzzled when your vignette preview doesn’t seem to reflect recent developments in the package. Double check that you’re building against the current version!\n\nRinse and repeat until the vignette looks the way you want.\n\nIf you’re regularly checking your entire package (Section 5.5), which we strongly recommend, this will help to keep your vignettes in good working order. In particular, this will alert you if a vignette makes use of a package that’s not a formal dependency. We will come back to these package-level workflow issues below in Section 18.5."
  },
  {
    "objectID": "vignettes.html#metadata",
    "href": "vignettes.html#metadata",
    "title": "18  Vignettes",
    "section": "\n18.2 Metadata",
    "text": "18.2 Metadata\nThe first few lines of the vignette contain important metadata. The default template contains the following information:\n---\ntitle: \"Vignette Title\"\noutput: rmarkdown::html_vignette\nvignette: >\n  %\\VignetteIndexEntry{Vignette Title}\n  %\\VignetteEngine{knitr::rmarkdown}\n  %\\VignetteEncoding{UTF-8}\n---\nThis metadata is written in YAML, a format designed to be both human and computer readable. YAML frontmatter is a common feature of R Markdown files. The syntax is much like that of the DESCRIPTION file, where each line consists of a field name, a colon, then the value of the field. The one special YAML feature we’re using here is >. It indicates that the following lines of text are plain text and shouldn’t use any special YAML features.\nThe default vignette template uses these fields:\n\ntitle: this is the title that appears in the vignette. If you change it, make sure to make the same change to VignetteIndexEntry{}. They should be the same, but unfortunately that’s not automatic.\noutput: this specifies the output format. There are many options that are useful for regular reports (including html, pdf, slideshows, etc.), but rmarkdown::html_vignette has been specifically designed for this exact purpose. See ?rmarkdown::html_vignette for more details.\nvignette: this is a block of special metadata needed by R. Here, you can see the legacy of LaTeX vignettes: the metadata looks like LaTeX comments. The only entry you might need to modify is the \\VignetteIndexEntry{}. This is how the vignette appears in the vignette index and it should match the title. Leave the other two lines alone. They tell R to use knitr to process the file and that the file is encoded in UTF-8 (the only encoding you should ever use for a vignette).\n\nWe generally don’t use these fields, but you will see them in other packages:\n\nauthor: we don’t use this unless the vignette is written by someone not already credited as a package author.\ndate: we think this usually does more harm than good, since it’s not clear what the date is meant to convey. Is it the last time the vignette source was updated? In that case you’ll have to manage it manually and it’s easy to forget to update it. If you manage date programmatically with Sys.date(), the date reflects when the vignette was built, i.e. when the package bundle was created, which has nothing to do with when the vignette or package was last modified. We’ve decided it’s best to omit the date.\n\nThe draft vignette also includes two R chunks. The first one configures our preferred way of displaying code output and looks like this:\n```{r, include = FALSE}\nknitr::opts_chunk$set(\n  collapse = TRUE,\n  comment = \"#>\"\n)\n```\nThe second chunk just attaches the package the vignette belongs to.\n```{r setup}\nlibrary(yourpackage)\n```\nYou might be tempted to (temporarily) replace this library() call with load_all(), but we advise that you don’t. Instead, use the techniques given in Section 18.1 to exercise your vignette code with the current source package."
  },
  {
    "objectID": "vignettes.html#advice-on-writing-vignettes",
    "href": "vignettes.html#advice-on-writing-vignettes",
    "title": "18  Vignettes",
    "section": "\n18.3 Advice on writing vignettes",
    "text": "18.3 Advice on writing vignettes\n\nIf you’re thinking without writing, you only think you’re thinking. — Leslie Lamport\n\nWhen writing a vignette, you’re teaching someone how to use your package. You need to put yourself in the reader’s shoes, and adopt a “beginner’s mind”. This can be difficult because it’s hard to forget all of the knowledge that you’ve already internalized. For this reason, we find in-person teaching to be a really useful way to get feedback. You’re immediately confronted with what you’ve forgotten that only you know.\nA useful side effect of this approach is that it helps you improve your code. It forces you to re-see the initial on-boarding process and to appreciate the parts that are hard. Our experience is that explaining how code works often reveals some problems that need fixing.\nIn fact, a key part of the tidyverse package release process is writing a blog post: we now do that before submitting to CRAN, because of the number of times it’s revealed some subtle problem that requires a fix. It’s also fair to say that the tidyverse and its supporting packages would benefit from more “how-to” guides, so that’s an area where we are constantly trying to improve.\nWriting a vignette also makes a nice break from coding. Writing seems to use a different part of the brain from programming, so if you’re sick of programming, try writing for a bit.\nHere are some resources we’ve found helpful:\n\nLiterally anything written by Kathy Sierra. She is not actively writing at the moment, but her content is mostly timeless and is full of advice about programming, teaching, and how to create valuable tools. See her original blog, Creating passionate users, or the site that came after, Serious Pony.\n“Style: Lessons in Clarity and Grace” by Joseph M. Williams and Joseph Bizup. This book helps you understand the structure of writing so that you’ll be better able to recognise and fix bad writing.\n\n\n18.3.1 Diagrams\n\n\n\n\n\n\nSubmitting to CRAN\n\n\n\nYou’ll need to watch the file size. If you include a lot of graphics, it’s easy to create a very large file. Be on the look out for a NOTE that complains about an overly large directory. You might need to take explicit measures, such as lowering the resolution, reducing the number of figures, or switching from a vignette to an article (Section 18.4.1).\n\n\n\n18.3.2 Links\nThere is no official way to link to help topics from vignettes or vice versa or from one vignette to another.\nThis is a concrete example of why we think pkgdown sites are a great way to present package documentation, because pkgdown makes it easy (literally zero effort, in many cases) to get these hyperlinked cross-references. This is documented in vignette(\"linking\", package = \"pkgdown\"). If you’re reading this book online, the inline call to vignette() in the previous sentence should be hyperlinked to the corresponding vignette in pkgdown4, using the same toolchain that will create automatic links in your pkgdown websites! We discussed this syntax previously in Section 17.1.3, in the context of function documentation.\nAutomatic links are generated for functions in the host package, namespace-qualified functions in another package, vignettes, and more. Here are the two most important examples of automatically linked text:\n\n`some_function()`: Autolinked to the documentation of some_function(), within the pkgdown site of its host package. Note the use of backticks and the trailing parentheses.\n`vignette(\"fascinating-topic\")`: Autolinked to the “fascinating-topic” article within the pkgdown site of its host package. Note the use of backticks.\n\n18.3.3 Filepaths\nSometimes it is necessary to refer to another file from a vignette. The best way to do this depends on the application:\n\nA figure created by code evaluated in the vignette: By default, in the .Rmd workflow that we recommend, this takes care of itself. Such figures are automatically embedded into the .html using data URIs. You don’t need to do anything. Example: vignette(\"extending-ggplot2\", package = \"ggplot2\") generates a few figures in evaluated code chunks.\n\nAn external file that could be useful to users or elsewhere in the package (not just in vignettes): Put such a file in inst/ (Section 9.2), perhaps in inst/extdata/ (Section 8.3), and refer to it with system.file() or fs::path_package() (Section 8.3.1). Example from vignette(\"sf2\", package = \"sf\"):\n\n````{r}\nlibrary(sf)\nfname <- system.file(\"shape/nc.shp\", package=\"sf\")\nfname\nnc <- st_read(fname)\n```\n\n\n\nAn external file whose utility is limited to your vignettes: put it alongside the vignette source files in vignettes/ and refer to it with a filepath that is relative to vignettes/.\nExample: The source of vignette(\"tidy-data\", package = \"tidyr\") is found at vignettes/tidy-data.Rmd and it includes a chunk that reads a file located at vignettes/weather.csv like so:\n\n```{r}\nweather <- as_tibble(read.csv(\"weather.csv\", stringsAsFactors = FALSE))\nweather\n```\n\n\n\nAn external graphics file: put it in vignettes/, refer to it with a filepath that is relative to vignettes/ and use knitr::include_graphics() inside a code chunk. Example from vignette(\"sheet-geometry\", package = \"readxl\"):\n\n```{r out.width = '70%', echo = FALSE}\nknitr::include_graphics(\"img/geometry.png\")\n```\n\n\n\n18.3.4 How many vignettes?\nFor simpler packages, one vignette is often sufficient. If your package is named “somepackage”, call this vignette somepackage.Rmd. This takes advantage of a pkgdown convention, where the vignette that’s named after the package gets an automatic “Getting Started” link in the top navigation bar.\nMore complicated packages probably need more than one vignette. It can be helpful to think of vignettes like chapters of a book – they should be self-contained, but still link together into a cohesive whole.\n\n18.3.5 Scientific publication\nVignettes can also be useful if you want to explain the details of your package. For example, if you have implemented a complex statistical algorithm, you might want to describe all the details in a vignette so that users of your package can understand what’s going on under the hood, and be confident that you’ve implemented the algorithm correctly. In this case, you might also consider submitting your vignette to the Journal of Statistical Software or The R Journal. Both journals are electronic only and peer-reviewed. Comments from reviewers can be very helpful for improving your package and vignette.\nIf you just want to provide something very lightweight so folks can easily cite your package, consider the Journal of Open Source Software. This journal has a particularly speedy submission and review process, and is where we published “Welcome to the Tidyverse”, a paper we wrote so that folks could have a single paper to cite and all the tidyverse authors would get some academic credit."
  },
  {
    "objectID": "vignettes.html#sec-vignettes-eval-option",
    "href": "vignettes.html#sec-vignettes-eval-option",
    "title": "18  Vignettes",
    "section": "\n18.4 Special considerations for vignette code",
    "text": "18.4 Special considerations for vignette code\nA recurring theme is that the R code inside a package needs to be written differently from the code in your analysis scripts and reports. This is true for your functions (Section 7.4), tests (Section 15.2), and examples (Section 17.5), and it’s also true for vignettes. In terms of what you can and cannot do, vignettes are fairly similar to examples, although some of the mechanics differ.\nAny package used in a vignette must be a formal dependency, i.e. it must be listed in Imports or Suggests in DESCRIPTION. Similar to our stance in tests (Section 12.5.2), our policy is to write vignettes under the assumption that suggested packages will be installed in any context where the vignette is being built (Section 12.5.3). We generally use suggested packages unconditionally in vignettes. But, as with tests, if a package is particularly hard to install, we might make an exception and take extra measures to guard its use.\nThere are many other reasons why it might not be possible to evaluate all of the code in a vignette in certain contexts, such as on CRAN’s machines or in CI/CD. These include all the usual suspects: lack of authentication credentials, long-running code, or code that is vulnerable to intermittent failure.\nThe main method for controlling evaluation in an .Rmd document is the eval code chunk option, which can be TRUE (the default) or FALSE. Importantly, the value of eval can be the result of evaluating an expression. Here are some relevant examples:\n\neval = requireNamespace(\"somedependency\")\neval = !identical(Sys.getenv(\"SOME_THING_YOU_NEED\"), \"\")\neval = file.exists(\"credentials-you-need\")\n\nThe eval option can be set for an individual chunk, but in a vignette it’s likely that you’ll want to evaluate most or all of the chunks or practically none of them. In the latter case, you’ll want to use knitr::opts_chunk$set(eval = FALSE) in an early, hidden chunk to make eval = FALSE the default for the remainder of the vignette. You can still override with eval = TRUE in individual chunks.\nIn vignettes, we use the eval option in a similar way as @examplesIf in examples (Section 17.5.4). If the code can only be run under specific conditions, you must find a way to to check for those pre-conditions programmatically at runtime and use the result to set the eval option.\nHere are the first few chunks in a vignette from googlesheets4, which wraps the Google Sheets API. The vignette code can only be run if we are able to decrypt a token that allows us to authenticate with the API. That fact is recorded in can_decrypt, which is then set as the vignette-wide default for eval.\n```{r setup, include = FALSE}\ncan_decrypt <- gargle:::secret_can_decrypt(\"googlesheets4\")\nknitr::opts_chunk$set(\n  collapse = TRUE,\n  comment = \"#>\",\n  error = TRUE,\n  eval = can_decrypt\n)\n```\n\n```{r eval = !can_decrypt, echo = FALSE, comment = NA}\nmessage(\"No token available. Code chunks will not be evaluated.\")\n```\n\n```{r index-auth, include = FALSE}\ngooglesheets4:::gs4_auth_docs()\n```\n\n```{r}\nlibrary(googlesheets4)\n```\nNotice the second chunk uses eval = !can_decrypt, which prints an explanatory message for anyone who builds the vignette without the necessary credentials.\nThe example above shows a few more handy chunk options. Use include = FALSE for chunks that should be evaluated but not seen in the rendered vignette. The echo option controls whether code is printed, in addition to output. Finally, error = TRUE is what allows you to purposefully execute code that could throw an error. The error will appear in the vignette, just as it would for your user, but it won’t prevent the execution of the rest of your vignette’s code, nor will it cause R CMD check to fail. This is something that works much better in a vignette than in an example.\nMany other options are described at https://yihui.name/knitr/options.\n\n18.4.1 Article instead of vignette\nThere is one last technique, if you don’t want any of your code to execute on CRAN. Instead of a vignette, you can create an article, which is a term used by pkgdown for a vignette-like .Rmd document that is not shipped with the package, but that appears only in the website. An article will be less accessible than a vignette, for certain users, such as those with limited internet access, because it is not present in the local installation. But that might be an acceptable compromise, for example, for a package that wraps a web API.\nYou can draft a new article with usethis::use_article(), which ensures the article will be .Rbuildignored. A great reason to use an article instead of a vignette is to show your package working in concert with other packages that you don’t want to depend on formally. Another compelling use case is when an article really demands lots of graphics. This is problematic for a vignette, because the large size of the package causes problems with R CMD check (and, therefore, CRAN) and is also burdensome for everyone who installs it, especially those with limited internet."
  },
  {
    "objectID": "vignettes.html#sec-vignettes-how-built-checked",
    "href": "vignettes.html#sec-vignettes-how-built-checked",
    "title": "18  Vignettes",
    "section": "\n18.5 How vignettes are built and checked",
    "text": "18.5 How vignettes are built and checked\nWe close this chapter by returning to a few workflow issues we didn’t cover in Section 18.1: How do the .Rmd files get turned into the vignettes consumed by users of an installed package? What does R CMD check do with vignettes? What are the implications for maintaining your vignettes?\nIt can be helpful to appreciate the big difference between the workflow for function documentation and vignettes. The source of function documentation is stored in roxygen comments in .R files below R/. We use devtools::document() to generate .Rd files below man/. These man/*.Rd files are part of the source package. The official R machinery cares only about the .Rd files.\nVignettes are very different because the .Rmd source is considered part of the source package and the official machinery (R CMD build and check) interacts with vignette source and built vignettes in many ways. The result is that the vignette workflow feels more constrained, since the official tooling basically treats vignettes somewhat like tests, instead of documentation.\n\n18.5.1 R CMD build and vignettes\nFirst, it’s important to realize that the vignettes/*.Rmd source files exist only when a package is in source (Section 4.2) or bundled form (Section 4.3). Vignettes are rendered when a source package is converted to a bundle via R CMD build or a convenience wrapper such as devtools::build(). The rendered products (.html) are placed in inst/doc/, along with their source (.Rmd) and extracted R code (.R; discussed in Section 18.5.2). Finally, when a package binary is made (Section 4.4), the inst/doc/ directory is promoted to a top-level doc/ directory, as happens with everything below inst/.\n\nThe key takeaway from the above is that it is awkward to keep rendered vignettes in a source package and this has implications for the vignette development workflow. It is tempting to fight this (and many have tried), but based on years of experience and discussion, the devtools philosophy is to accept this reality.\nAssuming that you don’t try to keep built vignettes around persistently in your source package, here are our recommendations for various scenarios:\n\nActive, iterative work on your vignettes: Use your usual interactive .Rmd workflow (such as the  button) or devtools::build_rmd(\"vignettes/my-vignette.Rmd\") to render a vignette to .html in the vignettes/ directory. Regard the .html as a disposable preview. (If you initiate vignettes with use_vignette(), this .html will already be gitignored.)\n\nMake the current state of vignettes in a development version available to the world:\n\nOffer a pkgdown website, preferably with automated “build and deploy”, such as using GitHub Actions to deploy to GitHub Pages. Here are tidyr’s vignettes in the development version (note the “dev” in the URL): https://tidyr.tidyverse.org/dev/articles/index.html.\nBe aware that anyone who installs directly from GitHub will need to explicitly request vignettes, e.g. with devtools::install_github(dependencies = TRUE, build_vignettes = TRUE).\n\n\n\nMake the current state of vignettes in a development version available locally:\n\nInstall your package locally and request that vignettes be built and installed, e.g. with devtools::install(dependencies = TRUE, build_vignettes = TRUE).\n\n\nPrepare built vignettes for a CRAN submission: Don’t try to do this by hand or in advance. Allow vignette (re-)building to happen as part of devtools::submit_cran() or devtools::release(), both of which build the package.\n\nIf you really do want to build vignettes in the official manner on an ad hoc basis, devtools::build_vignettes() will do this. But we’ve seen this lead to developer frustration, because it leaves the package in a peculiar form that is a mishmash of a source package and an unpacked package bundle. This nonstandard situation can then lead to even more confusion. For example, it’s not clear how these not-actually-installed vignettes are meant to be accessed. Most developers should avoid using build_vignettes() and, instead, pick one of the approaches outlined above.\n\n\n\n\n\n\nPre-built vignettes (or other documentation)\n\n\n\nWe highly recommend treating inst/doc/ as a strictly machine-writable directory for vignettes. We recommend that you do not take advantage of the fact that you can place arbitrary pre-built documentation in inst/doc/. This opinion permeates the devtools ecosystem which, by default, cleans out inst/doc/ during various development tasks, to combat the problem of stale documentation.\nHowever, we acknowledge that there are exceptions to every rule. In some domains, it might be impractical to rebuild vignettes as often our recommended workflow implies. Here are a few tips:\n\nYou can prevent the cleaning of inst/doc/ with pkgbuild::build(clean_doc =). You can put Config/build/clean-inst-doc: FALSE in DESCRIPTION to prevent pkgbuild and rcmdcheck from cleaning inst/doc/.\nThe rOpenSci tech note How to precompute package vignettes or pkgdown articles describes a clever, lightweight technique for keeping a manually-updated vignette in vignettes/.\nThe R.rsp package offers explicit support for static vignettes.\n\n\n\n\n18.5.2 R CMD check and vignettes\nWe conclude with a discussion of how vignettes are treated by R CMD check. This official checker expects a package bundle created by R CMD build, as described above. In the devtools workflow, we usually rely on devtools::check(), which automatically does this build step for us, before checking the package. R CMD check has various command line options and also consults many environment variables. We’re taking a maximalist approach here, i.e. we describe all the checks that could happen.\nR CMD check does some static analysis of vignette code and scrutinizes the existence, size, and modification times of various vignette-related files. If your vignettes use packages that don’t appear in DESCRIPTION, that is caught here. If files that should exist don’t exist or vice versa, that is caught here. This should not happen if you use the standard vignette workflow outlined in this chapter and is usually the result of some experiment that you’ve done, intentionally or not.\nThe vignette code is then extracted into a .R file, using the “tangle” feature of the relevant vignette engine (knitr, in our case), and run. The code originating from chunks marked as eval = FALSE will be commented out in this file and, therefore, is not executed. Then the vignettes are rebuilt from source, using the “weave” feature of the vignette engine (knitr, for us). This executes all the vignette code yet again, except for chunks marked eval = FALSE.\n\n\n\n\n\n\nSubmitting to CRAN\n\n\n\nCRAN’s incoming and ongoing checks use R CMD check which, as described above, exercises vignette code up to two times. Therefore, it is important to conditionally suppress the execution of code that is doomed to fail on CRAN.\nHowever, it’s important to note that the package bundle and binaries distributed by CRAN actually use the built vignettes included in your submission. Yes, CRAN will attempt to rebuild your vignettes regularly, but this is for quality control purposes. CRAN distributes the vignettes you built."
  },
  {
    "objectID": "other-markdown.html#sec-readme",
    "href": "other-markdown.html#sec-readme",
    "title": "19  Other markdown files",
    "section": "\n19.1 README\n",
    "text": "19.1 README\n\nFirst, we’ll talk about the role of the README file and we leave off the file extension, until we’re ready to talk about mechanics.\nThe goal of the README is to answer the following questions about your package:\n\nWhy should I use it?\nHow do I use it?\nHow do I get it?\n\nThe README file is a long-established convention in software, going back decades. Some of its traditional content is found elsewhere in an R package, for example, we use the DESCRIPTION file to document authorship and licensing.\nWhen you write your README, try to put yourself in the shoes of someone who’s come across your package and is trying to figure out if it solves a problem they have. If they decide that your package looks promising, the README should also show them how to install it and how to do one or two basic tasks. Here’s a good template for README:\n\nA paragraph that describes the high-level purpose of the package.\nAn example that shows how to use the package to solve a simple problem.\nInstallation instructions, giving code that can be copied and pasted into R.\nAn overview that describes the main components of the package. For more complex packages, this will point to vignettes for more details. This is also a good place to describe how your package fits into the ecosystem of its target domain.\n\n\n19.1.1 README.Rmd and README.md\n\nAs mentioned above, we prefer to write README in Markdown, i.e. to have README.md. This will be rendered as HTML and displayed in several important contexts:\n\n\nThe repository home page, if you maintain your package on GitHub (or a similar host).\n\nhttps://github.com/tidyverse/dplyr\n\n\n\nOn CRAN, if you release your package there.\n\n\nhttps://cran.r-project.org/web/packages/dplyr/index.html\nNotice the hyperlinked “README” under “Materials”.\n\n\n\n\nAs the home page of your pkgdown site, if you have one.\n\nhttps://dplyr.tidyverse.org\n\n\n\nGiven that it’s best to include a couple of examples in README.md, ideally you would generate it with R Markdown. That is, it works well to have README.Rmd as the main source file, which you then render to README.md.\nThe easiest way to get started is to use usethis::use_readme_rmd().1 This creates a template README.Rmd and adds it to .Rbuildignore, since only README.md should be included in the package bundle. The template looks like this:\n---\noutput: github_document\n---\n\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n```{r, include = FALSE}\nknitr::opts_chunk$set(\n  collapse = TRUE,\n  comment = \"#>\",\n  fig.path = \"man/figures/README-\",\n  out.width = \"100%\"\n)\n```\n\n\n# somepackage\n\n<!-- badges: start -->\n\n<!-- badges: end -->\n\nThe goal of somepackage is to ...\n\n## Installation\n\nYou can install the development version of somepackage from [GitHub](https://github.com/) with:\n\n``` r\n# install.packages(\"devtools\")\ndevtools::install_github(\"jane/somepackage\")\n```\n\n## Example\n\nThis is a basic example which shows you how to solve a common problem:\n\n```{r example}\nlibrary(somepackage)\n## basic example code\n```\n\n\nWhat is special about using `README.Rmd` instead of just `README.md`?\nYou can include R chunks like so:\n\n```{r cars}\nsummary(cars)\n```\n\n\nYou'll still need to render `README.Rmd` regularly, to keep `README.md` up-to-date.\n`devtools::build_readme()` is handy for this.\n\nYou can also embed plots, for example:\n\n```{r pressure, echo = FALSE}\nplot(pressure)\n```\n\n\nIn that case, don't forget to commit and push the resulting figure files, so they display on GitHub and CRAN.\nA few things to note about this starter README.Rmd:\n\nIt renders to GitHub Flavored Markdown.\nIt includes a comment to remind you to edit README.Rmd, not README.md.\nIt sets up our recommended knitr options, including saving images to man/figures/README- which ensures that they’re included in your built package. This is important so that your README works when it’s displayed by CRAN.\n\nIt sets up a place for future badges, such as results from automatic continuous integration checks (Section 21.2). Examples of functions that insert development badges:\n\nusethis::use_cran_badge() reports the current version of your package on CRAN.\nusethis::use_coverage() reports test coverage.\nuse_github_actions() and friends report the R CMD check status of your development package.\n\n\nIt includes placeholders where you should provide code for package installation and for some basic usage.\nIt reminds you of key facts about maintaining your README.\n\nYou’ll need to remember to re-render README.Rmd periodically and, most especially, before release. The best function to use for this is devtools::build_readme(), because it is guaranteed to render README.Rmd against the current source code of your package.\nThe devtools ecosystem tries to help you keep README.Rmd up-to-date in two ways:\n\n\nIf your package is also a Git repo, use_readme_rmd() automatically adds the following pre-commit hook:\n#!/bin/bash\nif [[ README.Rmd -nt README.md ]]; then\n  echo \"README.md is out of date; please re-knit README.Rmd\"\n  exit 1\nfi \nThis prevents a git commit if README.Rmd is more recently modified than README.md. If the hook is preventing a commit you really want to make, you can override it with git commit --no-verify. Note that Git commit hooks are not stored in the repository, so this hook needs to be added to any fresh clone. For example, you could re-run usethis::use_readme_rmd() and discard the changes to README.Rmd.\n\nThe release checklist placed by usethis::use_release_issue() includes a reminder to call devtools::build_readme()."
  },
  {
    "objectID": "other-markdown.html#sec-news",
    "href": "other-markdown.html#sec-news",
    "title": "19  Other markdown files",
    "section": "\n19.2 NEWS\n",
    "text": "19.2 NEWS\n\nThe README is aimed at new users, whereas the NEWS file is aimed at existing users: it should list all the changes in each release that a user might notice or want to learn more about. As with README, it’s a well-established convention for open source software to have a NEWS file, which is also sometimes called a changelog.\nAs with README, base R tooling does not require that NEWS be a Markdown file, but it does allow for that and it’s our strong preference. A NEWS.md file is pleasant to read on GitHub, on your pkgdown site, and is reachable from your package’s CRAN landing page. We demonstrate this again with dplyr:\n\n\nNEWS.md in dplyr’s GitHub repo:\n\nhttps://github.com/tidyverse/dplyr/blob/main/NEWS.md\n\n\n\nOn CRAN, if you release your package there.\n\n\nhttps://cran.r-project.org/web/packages/dplyr/index.html\nNotice the hyperlinked “NEWS” under “Materials”.\n\n\n\n\nOn your package site, available as the “Changelog” from the “News” dropdown menu in the main navbar:\n\nhttps://dplyr.tidyverse.org/news/index.html\n\n\n\nYou can use usethis::use_news_md() to initiate the NEWS.md file; many other lifecycle- and release-related functions in the devtools ecosystem will make appropriate changes to NEWS.md as your package evolves.\nHere’s a hypothetical NEWS.md file:\n# foofy (development version)\n\n* Better error message when grooving an invalid grobble (#206).\n\n# foofy 1.0.0\n\n## Major changes\n\n* Can now work with all grooveable grobbles!\n\n## Minor improvements and bug fixes\n\n* Printing scrobbles no longer errors (@githubusername, #100).\n\n* Wibbles are now 55% less jibbly (#200).\nThe example above demonstrates some organizing principles for NEWS.md:\n\nUse a top-level heading for each version: e.g. # somepackage 1.0.0. The most recent version should go at the top. Typically the top-most entry in NEWS.md of your source package will read # somepackage (development version).2\n\nEach change should be part of a bulleted list. If you have a lot of changes, you might want to break them up using subheadings, ## Major changes, ## Bug fixes, etc.\nWe usually stick with a simple list until we’re close to a release, at which point we organize into sections and refine the text. It’s hard to know in advance exactly what sections you’ll need. The release checklist placed by usethis::use_release_issue() includes a reminder to polish the NEWS.md file. In that phase, it can be helpful to remember that NEWS.md is a user-facing record of change, in contrast to, e.g., commit messages, which are developer-facing.\n\nIf an item is related to an issue in GitHub, include the issue number in parentheses, e.g. (#​10). If an item is related to a pull request, include the pull request number and the author, e.g. (#​101, @hadley). This helps an interested reader to find relevant context on GitHub and, in your pkgdown site, these issue and pull request numbers and usernames will be hyperlinks. We generally omit the username if the contributor is already recorded in DESCRIPTION.\n\nThe main challenge with NEWS.md is getting into the habit of noting any user-visible change when you make it. It’s especially easy to forget this when accepting external contributions. Before release, it can be useful to use your version control tooling to compare the source of the release candidate to the previous release. This often surfaces missing NEWS items."
  },
  {
    "objectID": "website.html#initiate-a-site",
    "href": "website.html#initiate-a-site",
    "title": "20  Website",
    "section": "\n20.1 Initiate a site",
    "text": "20.1 Initiate a site\nAssuming your package has a valid structure, pkgdown should be able to make a website for it. Obviously that website will be more substantial if your package has more of the documentation elements listed above. But something reasonable should happen for any valid R package.\n\n\n\n\n\n\nTip\n\n\n\nWe hear that some folks put off “learning pkgdown”, because they think it’s going to be a lot of work. But then they eventually execute the two commands we show next and have a decent website in less than five minutes!\n\n\nusethis::use_pkgdown() is a function you run once and it does the initial, minimal setup necessary to start using pkgdown:\n\nusethis::use_pkgdown\n\n\n#> ✔ Setting active project to '/tmp/RtmpwjeCDw/mypackage'\n#> ✔ Adding '^_pkgdown\\\\.yml$', '^docs$', '^pkgdown$' to '.Rbuildignore'\n#> ✔ Adding 'docs' to '.gitignore'\n#> ✔ Writing '_pkgdown.yml'\n#> • Edit '_pkgdown.yml'\n#> ✔ Setting active project to '<no active project>'\n\nHere’s what use_pkgdown() does:\n\nCreates _pkgdown.yml, which is the main configuration file for pkgdown. In an interactive session, _pkgdown.yml will be opened for inspection and editing. But there’s no immediate need to change or add anything here.\nAdds various patterns to .Rbuildignore, to keep pkgdown-specific files and directories from being included in your package bundle.\nAdds docs, the default destination for a rendered site, to .gitignore. This is harmless for those who don’t use Git. For those who do, this opts you in to our recommended lifestyle, where the definitive source for your pkgdown site is built and deployed elsewhere (probably via GitHub Actions and Pages; more on this soon). This means the rendered website at docs/ just serves as a local preview.\n\npkgdown::build_site() is a function you’ll call repeatedly, to re-render your site locally. In an extremely barebones package, you’ll see something like this:\n\npkgdown::build_site()\n\n\n#> ✔ Setting active project to '/tmp/RtmpwjeCDw/mypackage'\n#> -- Installing package into temporary library -----------------------\n#> == Building pkgdown site =======================================================\n#> Reading from: '/tmp/RtmpwjeCDw/mypackage'\n#> Writing to:   '/tmp/RtmpwjeCDw/mypackage/docs'\n#> -- Initialising site -----------------------------------------------------------\n#> Copying '../../../home/runner/work/_temp/Library/pkgdown/BS5/assets/link.svg' to 'link.svg'\n#> Copying '../../../home/runner/work/_temp/Library/pkgdown/BS5/assets/pkgdown.js' to 'pkgdown.js'\n#> -- Building home ---------------------------------------------------------------\n#> Writing 'authors.html'\n#> Writing '404.html'\n#> -- Building function reference -------------------------------------------------\n#> Writing 'reference/index.html'\n#> Writing 'sitemap.xml'\n#> -- Building search index -------------------------------------------------------\n#> == DONE ========================================================================\n#> ✔ Setting active project to '<no active project>'\n\nIn an interactive session your newly rendered site should appear in your default web browser.\n\n\n\n\n\n\nRStudio\n\n\n\nAnother nice gesture to build your site is via Addins > pkgdown > Build pkgdown.\n\n\nYou can look in the local docs/ directory to see the files that constitute your package’s website. To manually browse the site, open docs/index.html in your preferred browser.\nThis is almost all you truly need to know about pkgdown. It’s certainly a great start and, as your package and ambitions grow, the best place to learn more is the pkgdown-made website for the pkgdown package itself: https://pkgdown.r-lib.org."
  },
  {
    "objectID": "website.html#sec-website-deployment",
    "href": "website.html#sec-website-deployment",
    "title": "20  Website",
    "section": "\n20.2 Deployment",
    "text": "20.2 Deployment\nYour next task is to deploy your pkgdown site somewhere on the web, so that your users can visit it. The path of least resistance looks like this:\n\nUse Git and host your package on GitHub. The reasons to do this go well beyond offering a package website, but this will be one of the major benefits to adopting Git and GitHub, if you’re on the fence.\nUse GitHub Actions (GHA) to build your website, i.e. to run pkgdown::build_site(). GHA is a platform where you can configure certain actions to happen automatically when some event happens. We’ll use it to rebuild your website every time you push to GitHub.\nUse GitHub Pages to serve your website, i.e. the files you see below docs/ locally. GitHub Pages is a static website hosting service that creates a site from files found in a GitHub repo.\n\nThe advice to use GitHub Action and Pages are implemented for you in the function usethis::use_pkgdown_github_pages(). It’s not an especially difficult task, but there are several steps and it would be easy to miss or flub one. The output of use_pkgdown_github_pages() should look something like this:\n\nusethis::use_pkgdown_github_pages()\n#> ✔ Initializing empty, orphan 'gh-pages' branch in GitHub repo 'jane/mypackage'\n#> ✔ GitHub Pages is publishing from:\n#> • URL: 'https://jane.github.io/mypackage/'\n#> • Branch: 'gh-pages'\n#> • Path: '/'\n#> ✔ Creating '.github/'\n#> ✔ Adding '^\\\\.github$' to '.Rbuildignore'\n#> ✔ Adding '*.html' to '.github/.gitignore'\n#> ✔ Creating '.github/workflows/'\n#> ✔ Saving 'r-lib/actions/examples/pkgdown.yaml@v2' to '.github/workflows/pkgdown.yaml'\n#> • Learn more at <https://github.com/r-lib/actions/blob/v2/examples/README.md>.\n#> ✔ Recording 'https://jane.github.io/mypackage/' as site's url in '_pkgdown.yml'\n#> ✔ Adding 'https://jane.github.io/mypackage/' to URL field in DESCRIPTION\n#> ✔ Setting 'https:/jane.github.io/mypackage/' as homepage of GitHub repo 'jane/mypackage'\n\nLike use_pkgdown(), this is a function you basically call once, when setting up a new site. In fact, the first thing it does is to call use_pkgdown() (it’s OK if you’ve already called use_pkgdown()), so we usually skip straight to use_pkgdown_github_pages() when setting up a new site.\nLet’s walk through what use_pkgdown_github_pages() actually does:\n\nInitializes an empty, “orphan” branch in your GitHub repo, named gh-pages (for “GitHub Pages”). The gh-pages branch will only live on GitHub (there’s no reason to fetch it to your local computer) and it represents a separate, parallel universe from your actual package source. The only files tracked in gh-pages are those that constitute your package’s website (the files that you see locally below docs/).\nTurns on GitHub Pages for your repo and tells it to serve a website from the files found in the gh-pages branch.\nCopies the configuration file for a GHA workflow that does pkgdown “build and deploy”. The file shows up in your package as .github/workflows/pkgdown.yaml. If necessary, some related additions are made to .gitignore and .Rbuildignore.\nAdds the URL for your site as the homepage for your GitHub repo.\nAdds the URL for your site to DESCRIPTION and _pkgdown.yml. The autolinking behaviour we’ve touted elsewhere relies on your package listing its URL in these two places, so this is a high-value piece of configuration.\n\nAfter successful execution of use_pkgdown_github_pages(), you should be able to visit your new site at the URL displayed in the output above.1 By default the URL has this general form: https://USERNAME.github.io/REPONAME/."
  },
  {
    "objectID": "website.html#now-what",
    "href": "website.html#now-what",
    "title": "20  Website",
    "section": "\n20.3 Now what?",
    "text": "20.3 Now what?\nFor a typical package, you could stop here — after creating a basic pkgdown site and arranging for it to be re-built and deployed regularly — and people using (or considering using) your package would benefit greatly. Everything beyond this point is a “nice to have”.\nOverall, we recommend vignette(\"pkgdown\", package = \"pkgdown\") as a good place to start, if you think you want to go beyond the basic defaults.\nIn the sections below, we highlight a few areas that are connected to other topics in the book or customizations that are particularly rewarding."
  },
  {
    "objectID": "website.html#logo",
    "href": "website.html#logo",
    "title": "20  Website",
    "section": "\n20.4 Logo",
    "text": "20.4 Logo\nIt’s fun to have a package logo! In the R community, we have a strong tradition of hex stickers, so it can be nice to join in with a hex logo of your own. Keen R user Amelia McNamara made herself a dress out of custom hex logo fabric and useR! 2018 featured a spectacular hex photo wall.\nHere are some resources to guide your logo efforts:\n\nThe convention is to orient the logo with a vertex at the top and bottom, with flat vertical sides.\n\nIf you think you might print stickers, make sure to comply with the de facto standard for sticker size. hexb.in is a reliable source for the dimensions and also provides a list of potential vendors for printed stickers.\n\n\n\n\nFigure 20.1: Standard dimensions of a hex sticker.\n\n\n\n\n\nThe hexSticker package helps you make your logo from within the comfort of R.\n\nOnce you have your logo, the usethis::use_logo() function places an appropriately scaled copy of the image file at man/figures/logo.png and also provides a copy-paste-able markdown snippet to include your logo in your README. pkgdown will also discover a logo placed in the standard location and incorporate it into your site."
  },
  {
    "objectID": "website.html#reference-index",
    "href": "website.html#reference-index",
    "title": "20  Website",
    "section": "\n20.5 Reference index",
    "text": "20.5 Reference index\npkgdown creates a function reference in reference/ that includes one page for each .Rd help topic in man/. This is one of the first pages you should admire in your new site. As you look around, there are a few things to contemplate, which we review below.\n\n20.5.1 Rendered examples\npkgdown executes all your examples (Section 17.5) and inserts the rendered results. We find this is a fantastic improvement over just showing the source code. This view of your examples can be eye-opening and often you’ll notice things you want to add, omit, or change. If you’re not satisfied with how your examples appear, this is a good time to review techniques for including code that is expected to error (Section 17.5.3) or that can only be executed under certain conditions (Section 17.5.4).\n\n20.5.2 Linking\nThese help topics will be linked to from many locations within and, potentially, beyond your pkgdown site. This is what we are talking about in Section 17.1.3 when we recommend putting functions inside square brackets when mentioning them in a roxygen comment:\n\n#' I am a big fan of [thisfunction()] in my package. I\n#' also have something to say about [otherpkg::otherfunction()]\n#' in somebody else's package.\n\nOn pkgdown sites, those square-bracketed functions become hyperlinks to the relevant pages in your pkgdown site. This is automatic within your package. But inbound links from other people’s packages (and websites, etc.) require two things2:\n\n\nThe URL field of your DESCRIPTION file must include the URL of your pkgdown site (preferably followed by the URL of your GitHub repo):\nURL: https://dplyr.tidyverse.org, https://github.com/tidyverse/dplyr\n\n\nYour _pkgdown.yml file must include the URL for your site:\nurl: https://dplyr.tidyverse.org\n\n\ndevtools takes every chance it gets to do this sort of configuration for you. But if you elect to do things manually, this is something you might overlook. A general resource on auto-linking in pkgdown is vignette(\"linking\", package = \"pkgdown\").\n\n20.5.3 Index organization\nBy default, the reference index is just an alphabetically-ordered list of functions. For packages with more than a handful of functions, it’s often worthwhile to curate the index and organize the functions into groups. For example, dplyr uses this technique: https://dplyr.tidyverse.org/reference/index.html.\nYou achieve this by providing a reference field in _pkgdown.yml. Here’s a redacted excerpt from dplyr’s _pkgdown.yml file that gives you a sense of what’s involved:\nreference:\n- title: Data frame verbs\n\n- subtitle: Rows\n  desc: >\n    Verbs that principally operate on rows.\n  contents:\n  - arrange\n  - distinct\n  ...\n\n- subtitle: Columns\n  desc: >\n    Verbs that principally operate on columns.\n  contents:\n  - glimpse\n  - mutate\n  ...\n\n- title: Vector functions\n  desc: >\n    Unlike other dplyr functions, these functions work on individual vectors,\n    not data frames.\n  contents:\n  - between\n  - case_match\n  ...\n\n- title: Built in datasets\n  contents:\n  - band_members\n  - starwars\n  - storms\n  ...\n\n- title: Superseded\n  desc: >\n    Superseded functions have been replaced by new approaches that we believe\n    to be superior, but we don't want to force you to change until you're\n    ready, so the existing functions will stay around for several years.\n  contents:\n  - sample_frac\n  - top_n\n  ...\nTo learn more, see ?pkgdown::build_reference."
  },
  {
    "objectID": "website.html#vignettes-and-articles",
    "href": "website.html#vignettes-and-articles",
    "title": "20  Website",
    "section": "\n20.6 Vignettes and articles",
    "text": "20.6 Vignettes and articles\nChapter 18 deals with vignettes, which are long-form guides for a package. They afford various opportunities beyond what’s possible in function documentation. For example, you have much more control over the integration of prose and code and over the presentation of code itself, e.g. code can be executed but not seen, seen but not executed, and so on. It’s much easier to create the reading experience that best prepares your users for authentic usage of your package.\nA package’s vignettes appear, in rendered form, in its website, in the Articles dropdown menu. “Vignette” feels like a technical term that we might not expect all R users to know, which is why pkgdown uses the term “articles” here. To be clear, the Articles menu lists your package’s official vignettes (the ones that are included in your package bundle) and, optionally, other non-vignette articles (Section 18.4.1), which are only available on the website.\n\n20.6.1 Linking\nLike function documentation, vignettes can also be the target of automatic inbound links from within your package and, potentially, beyond. We’ve talked about this elsewhere in the book. In Section 17.1.3, we introduced the idea of referring to a vignette with an inline call like vignette(\"some-topic\"). The rationale behind this syntax is because the code can literally be copied, pasted, and executed for local vignette viewing. So it “works” in any context, even without automatic links. But, in contexts where the auto-linking machinery is available, it knows to look for this exact syntax and turn it into a hyperlink to the associated vignette, within a pkgdown site.\nThe need to specify the host package depends on the context:\n\nvignette(\"some-topic\"): Use this form in your own roxygen comments, vignettes, and articles, to refer to a vignette in your package. The host package is implied.\nvignette(\"some-topic\", package = \"somepackage\"): Use this form to refer to a vignette in some other package. The host package must be explicit.\n\nNote that this shorthand does not work for linking to non-vignette articles. Since the syntax leans so heavily on the vignette() function, it would be too confusing, i.e. evaluating the code in the console would fail because R won’t be able to find such a vignette. Non-vignette articles must be linked like any other URL.\nWhen you refer to a function in your package, in your vignettes and articles, make sure to put it inside backticks and to include parentheses. Qualify functions from other packages with their namespace. Here’s an example of prose in one of your own vignettes or articles:\nI am a big fan of `thisfunction()` in my package. I also have something to\nsay about `otherpkg::otherfunction()` in somebody else's package.\nRemember that automatic inbound links from other people’s packages (and websites, etc.) require that your package advertises the URL of its website in DESCRIPTION and _pkgdown.yaml, as configured by usethis:: use_pkgdown_github_pages() and as described in Section 20.5.2.\n\n20.6.2 Index organization\nAs with the reference index, the default listing of the articles (broadly defined) in a package is alphabetical. But if your package has several articles, it can be worthwhile to provide additional organization. For example, you might feature the articles aimed at the typical user and tuck those meant for advanced users or developers behind “More articles …”. You can learn more about this in ?pkgdown::build_articles.\n\n20.6.3 Non-vignette articles\nIn general, Chapter 18 is our main source of advice on how to approach vignettes and that also includes some coverage of non-vignette articles (Section 18.4.1). Here we review some reasons to use a non-vignette article and give some examples.\nAn article is morally like a vignette (e.g. it tells a story that involves multiple functions and is written with R markdown), except it does not ship with the package bundle. usethis::use_article() is the easiest way to create an article. The main reason to use an article is when you want to show code that is impossible or very painful to include in a vignette or official example. Possible root causes of this pain:\n\n\nUse of a package you don’t want to formally depend on. In vignettes and examples, it’s forbidden to show your package working with a package that you don’t list in DESCRIPTION, e.g. in Imports or Suggests.\nThere is a detailed example of this in Section 12.7.2, featuring a readxl article that uses the tidyverse meta-package. The key idea is to list such a dependency in the Config/Needs/website field of DESCRIPTION. This keeps tidyverse out of readxl’s dependencies, but ensures it’s installed when the website is built.\n\n\nCode that requires authentication or access to specific assets, tools, or secrets that are not available on CRAN.\nThe googledrive package has no true vignettes, only non-vignette articles, because it’s essentially impossible to demonstrate usage without authentication. It is possible to access secure environment variables on GitHub Actions, where the pkgdown site is built and deployed, but this is impossible to do on CRAN.\n\n\nContent that involves a lot of figures, which cause your package to bump up against CRAN’s size constraints.\nThe ggplot2 package presents several FAQs as articles for this reason."
  },
  {
    "objectID": "website.html#development-mode",
    "href": "website.html#development-mode",
    "title": "20  Website",
    "section": "\n20.7 Development mode",
    "text": "20.7 Development mode\nEvery pkgdown site has a so-called development mode, which can be specified via the development field in _pkgdown.yml. If unspecified, the default is mode: release, which results in a single pkgdown site. Despite the name, this single site reflects the state of the current source package, which could be either a released state or a development state. The diagram below shows the evolution of a hypothetical package that is on CRAN and that has a pkgdown site in “release” mode.\n...\n |\n V\nTweaks before release     v0.1.9000\n |\n V\nIncrement version number  v0.2.0     <-- install.packages() gets this\n |\n V\nIncrement version number  v0.2.9000  \n |\n V\nImprove error message     v0.2.9000  <-- site documents this\n |\n V\n...\nUsers who install from CRAN get version 0.2.0. But the pkgdown site is built from the development version of the package.\nThis creates the possibility that users will read about some new feature on the website that is not present in the package version that they have installed with install.packages(). We find that the simplicity of this setup outweighs the downsides, until a package has a broad user base, i.e. lots of users of varying levels of sophistication. It’s probably safe to stay in “release” mode until you actually hear from a confused user.\nPackages with a substantial user base should use “auto” development mode:\ndevelopment:\n  mode: auto\nThis directs pkgdown to generate a top-level site from the released version and to document the development version in a dev/ subdirectory. We revisit the same hypothetical package as above, but assuming the pkdown site is in “auto” mode.\n...\n |\n V\nTweaks before release     v0.1.9000\n |\n V\nIncrement version number  v0.2.0     <-- install.packages() gets this\n |                                       main site documents this\n V\nIncrement version number  v0.2.9000  \n |\n V\nImprove error message     v0.2.9000  <-- dev/ site documents this\n |\n V\n...\nAll of the core tidyverse packages use “auto” mode. For example, consider the website of the readr package:\n\nreadr.tidyverse.org documents the released version, i.e. what install.packages(\"readr\") delivers.\nreadr.tidyverse.org/dev/ documents the dev version, i.e. what install_github(\"tidyverse/readr\") delivers.\n\nAutomatic development mode is recommended for packages with a broad user base, because it maximizes the chance that a user will read web-based documentation that reflects the package version that is locally installed."
  },
  {
    "objectID": "software-development-practices.html#sec-sw-dev-practices-git-github",
    "href": "software-development-practices.html#sec-sw-dev-practices-git-github",
    "title": "21  Software development practices",
    "section": "\n21.1 Git and GitHub",
    "text": "21.1 Git and GitHub\nGit is a version control system that was originally built to coordinate the work of a global group of developers working on the Linux kernel. Git manages the evolution of a set of files — called a repository — in a highly structured way and we recommend that every R package should also be a Git repository (and also, probably, an RStudio Project; Section 5.2).\nA solo developer, working on a single computer, will benefit from adopting version control. But, for most of us, that benefit is not nearly large enough to make up for the pain of installing and using Git. In our opinion, for most folks, the pros of Git only outweigh the cons once you take the additional step of hooking your local repository up to a remote host like GitHub. The joint use of Git and GitHub offers many benefits that more than justify the learning curve.\n\n21.1.1 Standard practice\nThis recommendation is well aligned with the current, general practices in software development. Here are a few relevant facts from the 2022 Stack Overflow developer survey, which is based on about 70K responses.\n\n94% report using Git. The second-most used version control system was SVN, used by 5% of respondents.\nFor personal projects, 87% of respondents report using GitHub, followed by GitLab (21%) and Bitbucket (11%). The ranking is the same albeit less skewed for professional work: GitHub still dominates with 56%, followed by GitLab (29%) and Bitbucket (18%).\n\nWe can even learn a bit about the habits of R package developers, based on the URLs found in the DESCRIPTION files of CRAN packages. As of March 2023, there are about 19K packages on CRAN, of which about 55% have a non-empty URL field (over 10K). Of those, 80% have a GitHub URL (over 8K), followed by GitLab (just over 1%) and Bitbucket (around 0.5%).\nThe prevalence of Git/GitHub, both within the R community and beyond, should help you feel confident that adoption will have tangible benefits. Furthermore, the sheer popularity of these tools means there are lots of resources available for learning how to use Git and GitHub and for getting unstuck1.\nTwo specific resources that address the intersection of Git/GitHub and the R world are the website Happy Git and GitHub for the useR and the article “Excuse me, do you have a moment to talk about version control?” (Bryan 2018).\nWe conclude this section with a few examples of why Git/GitHub can be valuable specifically for R package development:\n\nCommunication with users: GitHub Issues are well-suited for taking bug reports and feature requests. Unlike email sent to the maintainer, these conversations are accessible to others and searchable.\nCollaboration: GitHub pull requests are a very low-friction way for outside contributors to help fix bugs and add features.\nDistribution: Functions like devtools::install_github(\"r-lib/devtools\") and pak::pak(\"r-lib/devtools\") allow people to easily install the development version of your package, based on a source repository. More generally, anyone can install your package from any valid Git ref, such as a branch, specific SHA, pull request, or tag.\nWebsite: GitHub Pages is one of the easiest ways to offer a website for your package (Section 20.2).\nContinuous integration: This is actually the topic of the next section, so read on for more."
  },
  {
    "objectID": "software-development-practices.html#sec-sw-dev-practices-ci",
    "href": "software-development-practices.html#sec-sw-dev-practices-ci",
    "title": "21  Software development practices",
    "section": "\n21.2 Continuous integration",
    "text": "21.2 Continuous integration\nAs we said in the introduction, continuous integration and deployment is commonly abbreviated as CI/CD or just CI. For R package development, what this means in practice is:\n\nYou host your source package on a platform like GitHub. The key point is that the hosted repository provides the formal structure for integrating the work of multiple contributors. Sometimes multiple developers have permission to push (this is how tidyverse and r-lib packages are managed). In other cases, only the primary maintainer has push permission. In either model, external contributors can propose changes via a pull request.\nYou configure one or more development tasks to execute automatically when certain events happen in the hosted repository, such as a push or a pull request. For example, for an R package, it’s extremely valuable to configure an automatic run of R CMD check. This helps you discover breakage quickly, when it’s easier to diagnose and fix, and is a tremendous help for evaluating whether to accept an external contribution.\n\nOverall, the use of hosted version control and continuous integration can make development move more smoothly and quickly.\nEven for a solo developer, having R CMD check run remotely, possibly on a couple of different operating systems, is a mighty weapon against the dreaded “works on my machine” problem. Especially for packages destined for CRAN, the use of CI decreases the chance of nasty surprises right before release.\n\n21.2.1 GitHub Actions\nThe easiest way to start using CI is to host your package on GitHub and use its companion service, GitHub Actions (GHA). Then you can use various functions from usethis to configure so-called GHA workflows. usethis copies workflow configuration files from r-lib/actions, which is where the tidyverse team maintains GHA infrastructure useful to the R community.\n\n21.2.2 R CMD check via GHA\nIf you only use CI for one thing, it should be to run R CMD check. If you call usethis::use_github_action() with no arguments, you can choose from a few of the most useful workflows. Here’s what that menu looks like at the time of writing:\n\n> use_github_action()\nWhich action do you want to add? (0 to exit)\n(See <https://github.com/r-lib/actions/tree/v2/examples> for other options) \n\n1: check-standard: Run `R CMD check` on Linux, macOS, and Windows\n2: test-coverage: Compute test coverage and report to https://about.codecov.io\n3: pr-commands: Add /document and /style commands for pull requests\n\nSelection: \n\ncheck-standard is highly recommended, especially for any package that is (or aspires to be) on CRAN. It runs R CMD check across a few combinations of operating system and R version. This increases your chances of quickly detecting code that relies on the idiosyncrasies of a specific platform, while it’s still easy to make the code more portable.\nAfter making that selection, you will see some messages along these lines:\n#> ✔ Creating '.github/'\n#> ✔ Adding '*.html' to '.github/.gitignore'\n#> ✔ Creating '.github/workflows/'\n#> ✔ Saving 'r-lib/actions/examples/check-standard.yaml@v2' to .github/workflows/R-CMD-check.yaml'\n#> • Learn more at <https://github.com/r-lib/actions/blob/v2/examples/README.md>.\n#> ✔ Adding R-CMD-check badge to 'README.md'\nThe key things that happen here are:\n\nA new GHA workflow file is written to .github/workflows/R-CMD-check.yaml. GHA workflows are specified via YAML files. The message reveals the source of the YAML and gives a link to learn more.\nSome helpful additions may be made to various “ignore” files.\nA badge reporting the R CMD check result is added to your README, if it has been created with usethis and has an identifiable badge “parking area”. Otherwise, you’ll be given some text you can copy and paste.\n\nCommit these file changes and push to GitHub. If you visit the “Actions” section of your repository, you should see that a GHA workflow run has been launched. In due course, its success (or failure) will be reported there, in your README badge, and in your GitHub notifications (depending on your personal settings).\nCongratulations! Your package will now benefit from even more regular checks.\n\n21.2.3 Other uses for GHA\nAs suggested by the interactive menu, usethis::use_github_action() gives you access to pre-made workflows other than R CMD check. In addition to the featured choices, you can use it to configure any of the example workflows in r-lib/actions by passing the workflow’s name. For example:\n\n\nuse_github_action(\"test-coverage\") configures a workflow to track the test coverage of your package, as described in Section 15.1.1.\n\nSince GHA allows you to run arbitrary code, there are many other things that you can use it for:\n\nBuilding your package’s website and deploying the rendered site to GitHub Pages, as described in Section 20.2. See also ?usethis::use_pkgdown_github_pages().\nRe-publishing a book website every time you make a change to the source. (Like we do for this book!).\n\nIf the example workflows don’t cover your exact use case, you can also develop your own workflow. Even in this case, the example workflows are often useful as inspiration. The r-lib/actions repository also contains important lower-level building blocks, such as actions to install R or to install all of the dependencies indicated in a DESCRIPTION file.\n\n\n\n\nBryan, Jennifer. 2018. “Excuse Me, Do You Have a Moment to Talk about Version Control?” The American Statistician 72 (1): 20–27. https://doi.org/10.1080/00031305.2017.1399928."
  },
  {
    "objectID": "lifecycle.html#sec-lifecycle-evolution",
    "href": "lifecycle.html#sec-lifecycle-evolution",
    "title": "22  Lifecycle",
    "section": "\n22.1 Package evolution",
    "text": "22.1 Package evolution\nFirst we should establish a working definition of what it means for your package to change. Technically, you could say that the package has changed every time any file in its source changes. This level of pedantry isn’t terribly useful, though. The smallest increment of change that’s meaningful is probably a Git commit. This represents a specific state of the source package that can be talked about, installed from, compared to, subjected to R CMD check, reverted to, and so on. This level of granularity is really only of interest to developers. But the package states accessible via the Git history are genuinely useful for the maintainer, so if you needed any encouragement to be more intentional with your commits, let this be it.\nThe primary signal of meaningful change is to increment the package version number and release it, for some definition of release, such as releasing on CRAN (Chapter 23). Recall that this important piece of metadata lives in the Version field of the DESCRIPTION file:\nPackage: usethis\nTitle: Automate Package and Project Setup\nVersion: 2.1.6\n...\nIf you visit the CRAN landing page for usethis, you can access its history via Downloads > Old sources > usethis archive. That links to a folder of package bundles (Section 4.3), reflecting usethis’s source for each version released on CRAN, presented in Table 22.1:\n\n\n\n\nTable 22.1: Releases of the usethis package.\n\nVersion\nDate\n\n\n\n1.0.0\n2017-10-22 17:36:29 UTC\n\n\n1.1.0\n2017-11-17 22:52:07 UTC\n\n\n1.2.0\n2018-01-19 18:23:54 UTC\n\n\n1.3.0\n2018-02-24 21:53:51 UTC\n\n\n1.4.0\n2018-08-14 12:10:02 UTC\n\n\n1.5.0\n2019-04-07 10:50:44 UTC\n\n\n1.5.1\n2019-07-04 11:00:05 UTC\n\n\n1.6.0\n2020-04-09 04:50:02 UTC\n\n\n1.6.1\n2020-04-29 05:50:02 UTC\n\n\n1.6.3\n2020-09-17 17:00:03 UTC\n\n\n2.0.0\n2020-12-10 09:00:02 UTC\n\n\n2.0.1\n2021-02-10 10:40:06 UTC\n\n\n2.1.0\n2021-10-16 23:30:02 UTC\n\n\n2.1.2\n2021-10-25 07:30:02 UTC\n\n\n2.1.3\n2021-10-27 15:00:02 UTC\n\n\n2.1.5\n2021-12-09 23:00:02 UTC\n\n\n2.1.6\n2022-05-25 20:50:02 UTC\n\n\n\n\n\n\nThis is the type of package evolution we’re going to address in this chapter. In section Section 22.2, we’ll delve into the world of software version numbers, which is a richer topic than you might expect. R also has some specific rules and tools around package version numbers. Finally, we’ll explain the conventions we use for the version numbers of tidyverse packages (Section 22.3).\nBut first, this is a good time to revisit a resource we first pointed out in Section 4.2, when introducing the different states of an R package. Recall that the (unofficial) cran organization on GitHub provides a read-only history of all CRAN packages. For example, you can get a different view of usethis’s released versions at https://github.com/cran/usethis/.\nThe archive provided by CRAN itself allows you to download older versions of usethis as .tar.gz files, which is useful if you truly want to get your hands on the source of an older version. However, if you just want to quickly check something about a version or compare two versions of usethis, the read-only GitHub mirror is much more useful. Each commit in this repo’s history represents a CRAN release, which makes it easy to see exactly what changed: https://github.com/cran/usethis/commits/HEAD. Furthermore, you can browse the state of all the package’s source files at any specific version, such as usethis’s initial release at version 1.0.01.\nThis information is technically available from the repository where usethis is actually developed (https://github.com/r-lib/usethis). But you have to work much harder to zoom out to the level of CRAN releases, amid the clutter of the small incremental steps in which development actually unfolds. These three different views of usethis’s evolution are all useful for different purposes:\n\nhttps://cran.r-project.org/src/contrib/Archive/usethis/: the official CRAN package bundles.\nhttps://github.com/cran/usethis/commits/HEAD: the unofficial read-only CRAN mirror, obtained by unpacking CRAN’s bundles.\nhttps://github.com/r-lib/usethis/commits/HEAD: the official development home for usethis."
  },
  {
    "objectID": "lifecycle.html#sec-lifecycle-version-number",
    "href": "lifecycle.html#sec-lifecycle-version-number",
    "title": "22  Lifecycle",
    "section": "\n22.2 Package version number",
    "text": "22.2 Package version number\nFormally, an R package version is a sequence of at least two integers separated by either . or -. For example, 1.0 and 0.9.1-10 are valid versions, but 1 and 1.0-devel are not. Base R offers the utils::package_version()2 function to parse a package version string into a proper S3 class by the same name. This class makes it easier to do things like compare versions.\n\npackage_version(c(\"1.0\", \"0.9.1-10\"))\n#> [1] '1.0'      '0.9.1.10'\nclass(package_version(\"1.0\"))\n#> [1] \"package_version\" \"numeric_version\"\n\n# these versions are not allowed for an R package\npackage_version(\"1\")\n#> Error: invalid version specification '1'\npackage_version(\"1.0-devel\")\n#> Error: invalid version specification '1.0-devel'\n\n# comparing package versions\npackage_version(\"1.9\") == package_version(\"1.9.0\")\n#> [1] TRUE\npackage_version(\"1.9\") < package_version(\"1.9.2\")\n#> [1] TRUE\npackage_version(c(\"1.9\", \"1.9.2\")) < package_version(\"1.10\")\n#> [1] TRUE TRUE\n\nThe last examples above make it clear that R considers version 1.9 to be equal to 1.9.0 and to be less than 1.9.2. And both 1.9 and 1.9.2 are less than 1.10, which you should think of as version “one point ten”, not “one point one zero”.\nIf you’re skeptical that the package_version class is really necessary, check out this example:\n\n\"2.0\" > \"10.0\"\n#> [1] TRUE\npackage_version(\"2.0\") > package_version(\"10.0\")\n#> [1] FALSE\n\nThe string 2.0 is considered to be greater than the string 10.0, because the character 2 comes after the character 1. By parsing version strings into proper package_version objects, we get the correct comparison, i.e. that version 2.0 is less than version 10.0.\nR offers this support for working with package versions, because it’s necessary, for example, to determine whether package dependencies are satisfied (Section 10.6.1). Under-the-hood, this tooling is used to enforce minimum versions recorded like this in DESCRIPTION:\nImports:\n    dplyr (>= 1.0.0),\n    tidyr (>= 1.1.0)\nIn your own code, if you need to determine which version of a package is installed, use utils::packageVersion()3:\n\npackageVersion(\"usethis\")\n#> [1] '2.1.6'\nstr(packageVersion(\"usethis\"))\n#> Classes 'package_version', 'numeric_version'  hidden list of 1\n#>  $ : int [1:3] 2 1 6\n\npackageVersion(\"usethis\") > package_version(\"10.0\")\n#> [1] FALSE\npackageVersion(\"usethis\") > \"10.0\"\n#> [1] FALSE\n\nThe return value of packageVersion() has the package_version class and is therefore ready for comparison to other version numbers. Note the last example where we seem to be comparing a version number to a string. How can we get the correct result without explicitly converting 10.0 to a package version? It turns out this conversion is automatic as long as one of the comparators has the package_version class."
  },
  {
    "objectID": "lifecycle.html#sec-lifecycle-version-number-tidyverse",
    "href": "lifecycle.html#sec-lifecycle-version-number-tidyverse",
    "title": "22  Lifecycle",
    "section": "\n22.3 Tidyverse package version conventions",
    "text": "22.3 Tidyverse package version conventions\nR considers 0.9.1-10 to be a valid package version, but you’ll never see a version number like that for a tidyverse package. Here is our recommended framework for managing the package version number:\n\nAlways use . as the separator, never -.\nA released version number consists of three numbers, <major>.<minor>.<patch>. For version number 1.9.2, 1 is the major number, 9 is the minor number, and 2 is the patch number. Never use versions like 1.0. Always spell out the three components, 1.0.0.\n\nAn in-development package has a fourth component: the development version. This should start at 9000. The number 9000 is arbitrary, but provides a clear signal that there’s something different about this version number. There are two reasons for this practice: First, the presence of a fourth component makes it easy to tell if you’re dealing with a released or in-development version. Also, the use of the fourth place means that you’re not limited to what the next released version will be. 0.0.1, 0.1.0, and 1.0.0 are all greater than 0.0.0.9000.\nIncrement the development version, e.g. from 9000 to 9001, if you’ve added an important feature and you (or others) need to be able to detect or require the presence of this feature. For example, this can happen when two packages are developing in tandem. This is generally the only reason that we bother to increment the development version. This makes in-development versions special and, in some sense, degenerate. Since we don’t increment the development component with each Git commit, the same package version number is associated with many different states of the package source, in between releases.\n\n\nThe advice above is inspired in part by Semantic Versioning and by the X.Org versioning schemes. Read them if you’d like to understand more about the standards of versioning used by many open source projects. But we should underscore that our practices are inspired by these schemes and are somewhat less regimented. Finally, know that other maintainers follow different philosophies on how to manage the package version number."
  },
  {
    "objectID": "lifecycle.html#sec-lifecycle-breaking-change-definition",
    "href": "lifecycle.html#sec-lifecycle-breaking-change-definition",
    "title": "22  Lifecycle",
    "section": "\n22.4 Backward compatibility and breaking change",
    "text": "22.4 Backward compatibility and breaking change\nThe version number of your package is always increasing, but it’s more than just an incrementing counter – the way the number changes with each release can convey information about the nature of the changes. The transition from 0.3.1 to 0.3.2, which is a patch release, has a very different vibe from the transition from 0.3.2 to 1.0.0, which is a major release. A package version number can also convey information about where the package is in its lifecycle. For example, the version 1.0.0 often signals that the public interface of a package is considered stable.\nHow do you decide which type of release to make, i.e. which component(s) of the version should you increment? A key concept is whether the associated changes are backward compatible, meaning that pre-existing code will still “work” with the new version. We put “work” in quotes, because this designation is open to a certain amount of interpretation. A hardliner might take this to mean “the code works in exactly the same way, in all contexts, for all inputs”. A more pragmatic interpretation is that “the code still works, but could produce a different result in some edge cases”. A change that is not backward compatible is often described as a breaking change. Here we’re going to talk about how to assess whether a change is breaking. In Section 22.6 we’ll talk about how to decide if a breaking change is worth it.\nIn practice, backward compatibility is not a clear-cut distinction. It is typical to assess the impact of a change from a few angles:\n\nDegree of change in behaviour. The most extreme is to make something that used to be possible into an error, i.e. impossible.\nHow the changes fit into the design of the package. A change to low-level infrastructure, such as a utility that gets called in all user-facing functions, is more fraught than a change that only affects one parameter of a single function.\nHow much existing usage is affected. This is a combination of how many of your users will perceive the change and how many existing users there are to begin with.\n\nHere are some concrete examples of breaking change:\n\nRemoving a function\nRemoving an argument\nNarrowing the set of valid inputs to a function\n\nConversely, these are usually not considered breaking:\n\nAdding a function. Caveat: there’s a small chance this could introduce a conflict in user code.\nAdding an argument. Caveat: this could be breaking for some usage, e.g. if a user is relying on position-based argument matching. This also requires some care in a function that accepts ….\nIncreasing the set of valid inputs.\nChanging the text of a print method or error. Caveat: This can be breaking if other packages depend on yours in fragile ways, such as building logic or a test that relies on an error message from your package.\nFixing a bug. Caveat: It really can happen that users write code that “depends” on a bug. Sometimes such code was flawed from the beginning, but the problem went undetected until you fixed your bug. Other times this surfaces code that uses your package in an unexpected way, i.e. it’s not necessarily wrong, but neither is it right.\n\nIf reasoning about code was a reliable way to assess how it will work in real life, the world wouldn’t have so much buggy software. The best way to gauge the consequences of a change in your package is to try it and see what happens. In addition to running your own tests, you can also run the tests of your reverse dependencies and see if your proposed change breaks anything. The tidyverse team has a fairly extensive set of tools for running so-called reverse dependency checks (Section 23.5), where we run R CMD check on all the packages that depend on ours. Sometimes we use this infrastructure to study the impact of a potential change, i.e. reverse dependency checks can be used to guide development, not only as a last-minute, pre-release check. This leads to yet another, deeply pragmatic definition of a breaking change:\n\nA change is breaking if it causes a CRAN package that was previously passing R CMD check to now fail AND the package’s original usage and behavior is correct.\n\nThis is obviously a narrow and incomplete definition of breaking change, but at least it’s relatively easy to get solid data.\nHopefully we’ve made the point that backward compatibility is not always a clearcut distinction. But hopefully we’ve also provided plenty of concrete criteria to consider when thinking about whether a change could break someone else’s code."
  },
  {
    "objectID": "lifecycle.html#sec-lifecycle-release-type",
    "href": "lifecycle.html#sec-lifecycle-release-type",
    "title": "22  Lifecycle",
    "section": "\n22.5 Major vs minor vs patch release",
    "text": "22.5 Major vs minor vs patch release\nRecall that a version number will have one of these forms, if you’re following the conventions described in Section 22.3:\n<major>.<minor>.<patch>        # released version\n<major>.<minor>.<patch>.<dev>  # in-development version\nIf the current package version is 0.8.1.9000, here’s our advice on how to pick the version number for the next release:\n\nIncrement patch, e.g. 0.8.2 for a patch release: you’ve fixed bugs, but you haven’t added any significant new features and there are no breaking changes. For example, if we discover a show-stopping bug shortly after a release, we would make a quick patch release with the fix. Most releases will have a patch number of 0.\nIncrement minor, e.g. 0.9.0, for a minor release. A minor release can include bug fixes, new features, and changes that are backward compatible4. This is the most common type of release. It’s perfectly fine to have so many minor releases that you need to use two (or even three!) digits, e.g. 1.17.0.\nIncrement major, e.g. 1.0.0, for a major release. This is the most appropriate time to make changes that are not backward compatible and that are likely to affect many users. The 1.0.0 release has special significance and typically indicates that your package is feature complete with a stable API.\n\nThe trickiest decision you are likely to face is whether a change is “breaking” enough to deserve a major release. For example, if you make an API-incompatible change to a rarely-used part of your code, it may not make sense to increase the major number. But if you fix a bug that many people depend on (it happens!), it will feel like a breaking change to those folks. It’s conceivable that such a bug fix could merit a major release.\nWe’re mostly dwelling on breaking change, but let’s not forget that sometimes you also add exciting new features to your package. From a marketing perspective, you probably want to save these for a major release, because your users are more likely to learn about the new goodies, from reading a blog post or NEWS.\nHere are a few tidyverse blog posts that have accompanied different types of package releases:\n\nMajor release: dplyr 1.0.0, purrr 1.0.0, pkgdown 2.0.0, readr 2.0.0\nMinor release: stringr 1.5.0, ggplot2 3.4.0\nPatch release: These are usually not considered worthy of a blog post.\n\n\n22.5.1 Package version mechanics\nYour package should start with version number 0.0.0.9000. usethis::create_package() starts with this version, by default.\nFrom that point on, you can use usethis::use_version() to increment the package version. When called interactively, with no argument, it presents a helpful menu:\n\nusethis::use_version()\n#> Current version is 0.1.\n#> What should the new version be? (0 to exit) \n#> \n#> 1: major --> 1.0\n#> 2: minor --> 0.2\n#> 3: patch --> 0.1.1\n#> 4:   dev --> 0.1.0.9000\n#> \n#> Selection: \n\nIn addition to incrementing Version in DESCRIPTION (Chapter 10), use_version() also adds a new heading in NEWS.md (Section 19.2)."
  },
  {
    "objectID": "lifecycle.html#sec-lifecycle-breaking-change-pros-cons",
    "href": "lifecycle.html#sec-lifecycle-breaking-change-pros-cons",
    "title": "22  Lifecycle",
    "section": "\n22.6 Pros and cons of breaking change",
    "text": "22.6 Pros and cons of breaking change\nThe big difference between major and minor releases is whether or not the code is backward compatible. In the general software world, the idea is that a major release signals to users that it may contain breaking changes and they should only upgrade when they have the capacity to deal with any issues that emerge.\nReality is a bit different in the R community, because of the way most users manage package installation. If we’re being honest, most R users don’t manage package versions in a very intentional way. Given the way update.packages() and install.packages() work, it’s quite easy to upgrade a package to a new major version without really meaning to, especially for dependencies of the target package. This, in turn, can lead to unexpected exposure to breaking changes in code that previously worked. This unpleasantness has implications both for users and for maintainers.\nIf it’s important to protect a data product against change in its R package dependencies, we recommend the use of a project-specific package library. In particular, we like to implement this approach using the renv package. This supports a lifestyle where a user’s default package library is managed in the usual, somewhat haphazard way. But any project that has a specific, higher requirement for reproducibility is managed with renv. This keeps package updates triggered by work in project A from breaking the code in project B and also helps with collaboration and deployment.\nWe suspect that package-specific libraries and tools like renv are currently under-utilized in the R world. That is, lots of R users still use just one package library. Therefore, package maintainers still need to exercise considerable caution and care when they introduce breaking changes, regardless of what’s happening with the version number. In section Section 22.7, we describe how tidyverse packages approach this, supported by tools in the lifecycle package.\nAs with dependencies (Section 11.1), we find that extremism isn’t a very productive stance. Extreme resistance to breaking change puts a significant drag on ongoing development and maintenance. Backward compatible code tends to be harder to work with because of the need to maintain multiple paths to support functionality from previous versions. The harder you strive to maintain backward compatibility, the harder it is to develop new features or fix old mistakes. This, in turn, can discourage adoption by new users and can make it harder to recruit new contributors. On the other hand, if you constantly make breaking changes, users will become very frustrated with your package and will decide they’re better off without it. Find a happy medium. Be concerned about backward compatibility, but don’t let it paralyze you.\nThe importance of backward compatibility is directly proportional to the number of people using your package: you are trading your time and pain for that of your users. There are good reasons to make backward incompatible changes. Once you’ve decided it’s necessary, your main priority is to use a humane process that is respectful of your users."
  },
  {
    "objectID": "lifecycle.html#sec-lifecycle-stages-and-package",
    "href": "lifecycle.html#sec-lifecycle-stages-and-package",
    "title": "22  Lifecycle",
    "section": "\n22.7 Lifecycle stages and supporting tools",
    "text": "22.7 Lifecycle stages and supporting tools\nThe tidyverse team’s approach to package evolution has become more structured and deliberate over the years. The associated tooling and documentation lives in the lifecycle package (lifecycle.r-lib.org). The approach relies on two major components:\n\nLifecycle stages, which can be applied at different levels, i.e. to an individual argument or function or to an entire package.\nConventions and functions to use when transitioning a function from one lifecycle stage to another. The deprecation process is the one that demands the most care.\n\nWe won’t duplicate too much of the lifecycle documentation here. Instead, we highlight the general principles of lifecycle management and present specific examples of successful lifecycle “moves”.\n\n22.7.1 Lifecycle stages and badges\n\n\n\n\nFigure 22.1: The four primary stages of the tidyverse lifecycle: stable, deprecated, superseded, and experimental.\n\n\n\n\nThe four lifecycle stages are:\n\nStable. This is the default stage and signals that users should feel comfortable relying on a function or package. Breaking changes should be rare and should happen gradually, giving users sufficient time and guidance to adapt their usage.\nExperimental. This is appropriate when a function is first introduced and the maintainer reserves the right to change it without much of a deprecation process. This is the implied stage for any package with a major version of 0, i.e. that hasn’t had a 1.0.0 release yet.\nDeprecated. This applies to functionality that is slated for removal. Initially, it still works, but triggers a deprecation warning with information about preferred alternatives. After a suitable amount of time and with an appropriate version change, such functions are typically removed.\nSuperseded. This is a softer version of deprecated, where legacy functionality is preserved as if in a time capsule. Superseded functions receive only minimal maintenance, such as critical bug fixes.\n\nYou can get much more detail in vignette(\"stages\", package = \"lifecycle\").\nThe lifecycle stage is often communicated through a badge. If you’d like to use lifecycle badges, call usethis::use_lifecycle() to do some one-time setup:\n\nusethis::use_lifecycle()\n#> ✔ Adding 'lifecycle' to Imports field in DESCRIPTION\n#> • Refer to functions with `lifecycle::fun()`\n#> ✔ Adding '@importFrom lifecycle deprecated' to 'R/somepackage-package.R'\n#> ✔ Writing 'NAMESPACE'\n#> ✔ Creating 'man/figures/'\n#> ✔ Copied SVG badges to 'man/figures/'\n#> • Add badges in documentation topics by inserting one of:\n#>   #' `r lifecycle::badge('experimental')`\n#>   #' `r lifecycle::badge('superseded')`\n#>   #' `r lifecycle::badge('deprecated')`\n\nThis leaves you in a position to use lifecycle badges in help topics and to use lifecycle functions, as described in the remainder of this section.\nFor a function, include the badge in its @description block. Here’s how we indicate that dplyr::top_n() is superseded:\n\n#' Select top (or bottom) n rows (by value)\n#'\n#' @description\n#' `r lifecycle::badge(\"superseded\")`\n#' `top_n()` has been superseded in favour of ...\n\nFor a function argument, include the badge in the @param tag. Here’s how the deprecation of readr::write_file(path =) is documented:\n\n#' @param path `r lifecycle::badge(\"deprecated\")` Use the `file` argument\n#'   instead.\n\nCall usethis::use_lifecycle_badge() if you want to use a badge in README to indicate the lifecycle of an entire package (Section 19.1).\nIf the lifecycle of a package is stable, it’s not really necessary to use a badge, since that is the assumed default stage. Similarly, we typically only use a badge for a function if its stage differs from that of the associated package and likewise for an argument and the associated function.\n\n22.7.2 Deprecating a function\nIf you’re going to remove or make significant changes to a function, it’s usually best to do so in phases. Deprecation is a general term for the situation where something is explicitly discouraged, but it has not yet been removed. Various deprecation scenarios are explored in vignette(\"communicate\", package = \"lifecycle\"); we’re just going to cover the main idea here.\nThe lifecycle::deprecate_warn() function can be used inside a function to inform your user that they’re using a deprecated feature and, ideally, to let them know about the preferred alternative. In this example, the plus3() function is being replaced by add3():\n\n# new function\nadd3 <- function(x, y, z) {\n  x + y + z\n}\n\n# old function\nplus3 <- function(x, y, z) {\n  lifecycle::deprecate_warn(\"1.0.0\", \"plus3()\", \"add3()\")\n  add3(x, y, z)\n}\n\nplus3(1, 2, 3)\n#> Warning: `plus3()` was deprecated in somepackage 1.0.0.\n#> ℹ Please use `add3()` instead.\n#> [1] 6\n\nAt this point, a user who calls plus3() sees a warning explaining that the function has a new name, but we go ahead and call add3() with their inputs. Pre-existing code still “works”. In some future major release, plus3() could be removed entirely.\nlifecycle::deprecate_warn() has a few features that are worth highlighting:\n\nThe warning message is built up from inputs like when, what, with, and details, which gives deprecation warnings a predictable form across different functions, packages, and time. The intent is to reduce the cognitive load for users who may already be somewhat stressed.\nBy default, a specific warning is only issued once every 8 hours, in an effort to cause just the right amount of aggravation. The goal is to be just annoying enough to motivate the user to update their code before the function or argument goes away, but not so annoying that they fling their computer into the sea. Near the end of the deprecation process, the always argument can be set to TRUE to warn on every call.\nA warning is only issued if the person reading it is the one who can actually do something about it, i.e. update the offending code. If a user calls a deprecated function indirectly, i.e. because they are using a package that’s using a deprecated function, by default that user doesn’t get a warning. (But the maintainer of the guilty package will see these warnings in their test results.)\n\nHere’s a hypothetical schedule for removing a function fun():\n\nPackage version 1.5.0: fun() exists. The lifecycle stage of the package is stable, as indicated by its post-1.0.0 version number and, perhaps, a package-level badge. The lifecycle stage of fun() is also stable, by extension, since it hasn’t been specifically marked as experimental.\nPackage version 1.6.0: The deprecation process of fun() begins. We insert in its @description to place a badge in its help topic. In the body of fun(), we add a call to lifecycle::deprecate_warn() to inform users about the situation. Otherwise, fun() still works as it always has.\nPackage version 1.7.0 or 2.0.0: fun() is removed. Whether this happens in a minor or major release will depend on the context, i.e. how widely used this package and function are.\n\nIf you’re using base R only, the .Deprecated() and .Defunct() functions are the closest substitutes for lifecycle::deprecate_warn() and friends.\n\n22.7.3 Deprecating an argument\nlifecycle::deprecate_warn() is also useful when deprecating an argument. In this case, it’s also handy to use lifecycle::deprecate() as the default value for the deprecated argument. Here we continue an example from above, i.e. the switch from path to file in readr::write_file():\n\nwrite_file <- function(x,\n                       file,\n                       append = FALSE,\n                       path = deprecated()) {\n  if (is_present(path)) {\n    lifecycle::deprecate_warn(\"1.4.0\", \"write_file(path)\", \"write_file(file)\")\n    file <- path\n  }\n  ...\n}\n\nHere’s what a user sees if they use the deprecated argument:\n\nreadr::write_file(\"hi\", path = tempfile(\"lifecycle-demo-\"))\n#> Warning: The `path` argument of `write_file()` is deprecated as of readr\n#> 1.4.0.\n#> ℹ Please use the `file` argument instead.\n\nThe use of deprecated() as the default accomplishes two things. First, if the user reads the documentation, this is a strong signal that an argument is deprecated. But deprecated() also has benefits for the package maintainer. Inside the affected function, you can use lifecycle::is_present() to determine if the user has specified the deprecated argument and proceed accordingly, as shown above.\nIf you’re using base R only, the missing() function has substantial overlap with lifecycle::is_present(), although it can be trickier to finesse issues around default values.\n\n22.7.4 Deprecation helpers\nSometimes a deprecation affects code in multiple places and it’s clunky to inline the full logic everywhere. In this case, you might create an internal helper to centralize the deprecation logic.\nThis happened in googledrive, when we changed how to control the package’s verbosity. The original design let the user specify this in every single function, via the verbose = TRUE/FALSE argument. Later, we decided it made more sense to use a global option to control verbosity at the package level. This is a case of (eventually) removing an argument, but it affects practically every single function in the package. Here’s what a typical function looks like after starting the deprecation process:\n\ndrive_publish <- function(file, ..., verbose = deprecated()) {\n  warn_for_verbose(verbose)\n  # rest of the function ...\n}\n\nNote the use of verbose = deprecated(). Here’s a slightly simplified version of warn_for_verbose():\n\nwarn_for_verbose <- function(verbose = TRUE,\n                             env = rlang::caller_env(),\n                             user_env = rlang::caller_env(2)) {\n  # This function is not meant to be called directly, so don't worry about its\n  # default of `verbose = TRUE`.\n  # In authentic, indirect usage of this helper, this picks up on whether\n  # `verbose` was present in the **user's** call to the calling function.\n  if (!lifecycle::is_present(verbose) || isTRUE(verbose)) {\n    return(invisible())\n  }\n\n  lifecycle::deprecate_warn(\n    when = \"2.0.0\",\n    what = I(\"The `verbose` argument\"),\n    details = c(\n      \"Set `options(googledrive_quiet = TRUE)` to suppress all googledrive messages.\",\n      \"For finer control, use `local_drive_quiet()` or `with_drive_quiet()`.\",\n      \"googledrive's `verbose` argument will be removed in the future.\"\n    ),\n    user_env = user_env\n  )\n  # only set the option during authentic, indirect usage\n  if (!identical(env, global_env())) {\n    local_drive_quiet(env = env)\n  }\n  invisible()\n}\n\nThe user calls a function, such as drive_publish(), which then calls warn_for_verbose(). If the user leaves verbose unspecified or if they request verbose = TRUE (default behavior), warn_for_verbose() does nothing. But if they explicitly ask for verbose = FALSE, we throw a warning with advice on the preferred way to suppress googledrive’s messaging. We also go ahead and honor their wishes for the time being, via the call to googledrive::local_drive_quiet(). In the next major release, the verbose argument can be removed everywhere and this helper can be deleted.\n\n22.7.5 Dealing with change in a dependency\nWhat if you want to use functionality in a new version of another package? Or the less happy version: what if changes in another package are going to break your package? There are a few possible scenarios, depending on whether the other package has been released and the experience you want for your users. We’ll start with the simple, happier case of using features newly available in a dependency.\nIf the other package has already been released, you could bump the minimum version you declare for it in DESCRIPTION and use the new functionality unconditionally. This also means that users who update your package will be forced to update the other package, which you should at least contemplate. Note, also, that this only works for a dependency in Imports. While it’s a good idea to record a minimum version for a suggested package, it’s not generally enforced the same as for Imports.\nIf you don’t want to require your users to update this other package, you could make your package work with both new and old versions. This means you’ll check its version at run-time and proceed accordingly. Here is a sketch of how that might look in the context of an existing or new function:\n\nyour_existing_function <- function(..., cool_new_feature = FALSE) {\n  if (isTRUE(cool_new_feature) && packageVersion(\"otherpkg\") < \"1.0.0\") {\n    message(\"otherpkg >= 1.0.0 is needed for cool_new_feature\")\n    cool_new_feature <- FALSE\n  }\n  # the rest of the function\n}\n\nyour_new_function <- function(...) {\n  if (packageVersion(\"otherpkg\") < \"1.0.0\") {\n    stop(\"otherpkg >= 1.0.0 needed for this function.\")\n  }\n  # the rest of the function\n}\n\nAlternatively, this would also be a great place to use rlang::is_installed() and rlang::check_installed() with the version argument (see examples of usage in Section 12.5.1).\nThis approach can also be adapted if you’re responding to not-yet-released changes that are coming soon in one of your dependencies. It’s helpful to have a version of your package that works both before and after the change. This allows you to release your package at any time, even before the other package. Sometimes you can refactor your code to make it work with either version of the other package, in which case you don’t need to condition on the other package’s version at all. But sometimes you might really need different code for the two versions. Consider this example:\n\nyour_function <- function(...) {\n  if (packageVersion(\"otherpkg\") >= \"1.3.9000\") {\n    otherpkg::their_new_function()\n  } else {\n    otherpkg::their_old_function()\n  }\n  # the rest of the function\n}\n\nThe hypothetical minimum version of 1.3.9000 suggests a case where the development version of otherpkg already has the change you’re responding to, which is a new function in this case. Assuming their_new_function() doesn’t exist in the latest release of otherpkg, you’ll get a note from R CMD check stating that their_new_function() doesn’t exist in otherpkg’s namespace. If you’re submitting such a version to CRAN, you can explain that you’re doing this for the sake of backward and forward compatibility with otherpkg and they are likely to be satisfied.\n\n22.7.6 Superseding a function\nThe last lifecycle stage that we’ll talk about is superseded. This is appropriate when you feel like a function is no longer the preferred solution to a problem, but it has enough usage and history that you don’t want to initiate the process of removing it. Good examples of this are tidyr::spread() and tidyr::gather(). Those functions have been superseded by tidyr::pivot_wider() and tidyr::pivot_longer(). But some users still prefer the older functions and it’s likely that they’ve been used a lot in projects that are not under active development. Thus spread() and gather() are marked as superseded, they don’t receive any new innovations, but they aren’t at risk of removal.\nA related phenomenon is when you want to change some aspect of a package, but you also want to give existing users a way to opt-in to the legacy behaviour. The idea is to provide users a band-aid they can apply to get old code working quickly, until they have the bandwidth to do a more thorough update (which might not ever happen, in some cases). Here are some examples where legacy behaviour was preserved for users who opt-in:\n\nIn tidyr 1.0.0, the interface of tidyr::nest() and tidyr::unnest() changed. Most authentic usage can be translated to the new syntax, which tidyr does automatically, along with conveying the preferred modern syntax via a warning. But the old interface remains available via tidyr::nest_legacy() and tidyr::unnest_legacy(), which were marked superseded upon creation.\ndplyr 1.1.0 takes advantage of a much faster algorithm for computing groups. But this speedier method also sorts the groups with respect to the C locale, whereas previously the system locale was used. The global option dplyr.legacy_locale allows a user to explicitly request the legacy behaviour.5\nThe tidyverse packages have been standardizing on a common approach to name repair, which is implemented in vctrs::vec_as_names(). The vctrs package also offers vctrs::vec_as_names_legacy(), which makes it easier to get names repaired with older strategies previously used in packages like tibble, tidyr, and readxl.\nreadr 2.0.0 introduced a so-called second edition, marking the switch to a backend provided by the vroom package. Functions like readr::with_edition(1, ...) and readr::local_edition(1) make it easier for a user to request first edition behaviour for a specific bit of code or for a specific script."
  },
  {
    "objectID": "release.html#decide-the-release-type",
    "href": "release.html#decide-the-release-type",
    "title": "23  Releasing to CRAN",
    "section": "\n23.1 Decide the release type",
    "text": "23.1 Decide the release type\nWhen you call use_release_issue(), you’ll be asked which type of release you intend to make.\n\n> use_release_issue()\n✔ Setting active project to '/Users/jenny/rrr/usethis'\nCurrent version is 2.1.6.9000.\nWhat should the release version be? (0 to exit) \n\n1: major --> 3.0.0\n2: minor --> 2.2.0\n3: patch --> 2.1.7\n\nSelection: \n\nThe immediate question feels quite mechanical: which component of the version number do you want to increment? But remember that we discussed the substantive differences in release types in Section 22.5.\nIn our workflow, this planned version number is recorded in the GitHub issue that holds the release checklist, but we don’t actually increment the version in DESCRIPTION until later in the process (Section 23.7). However, it’s important to declare the release type up front, because the process (and, therefore, the checklist) looks different for, e.g., a patch release versus a major release."
  },
  {
    "objectID": "release.html#sec-release-initial",
    "href": "release.html#sec-release-initial",
    "title": "23  Releasing to CRAN",
    "section": "\n23.2 Initial CRAN release: Special considerations",
    "text": "23.2 Initial CRAN release: Special considerations\nEvery new package receives a higher level of scrutiny from CRAN. In addition to the usual automated checks, new packages are also reviewed by a human, which inevitably introduces a certain amount of subjectivity and randomness. There are many packages on CRAN that would not be accepted in their current form, if submitted today as a completely new package. This isn’t meant to discourage you. But you should be aware that, just because you see some practice in an established package (or even in base R), that doesn’t mean you can do the same in your new package.\nLuckily, the community maintains lists of common “gotchas” for new packages. If your package is not yet on CRAN, the checklist begins with a special section that reflects this recent collective wisdom. Attending to these checklist items has dramatically improved our team’s success rate for initial submissions.\nFirst release\n\n\nusethis::use_news_md()\n\n\nusethis::use_cran_comments()\n\n\nUpdate (aspirational) install instructions in README\n\n\nProofread Title: and Description:\n\n\nCheck that all exported functions have @returns and @examples\n\n\nCheck that Authors@R: includes a copyright holder (role ‘cph’)\n\nCheck licensing of included files\n\nReview https://github.com/DavisVaughan/extrachecks\n\n\nIf you don’t already have a NEWS.md file, you are encouraged to create one now with usethis::use_news_md(). You’ll want this file eventually and this anticipates the fact that the description of your eventual GitHub release (Section 23.9) is drawn from NEWS.md.\nusethis::use_cran_comments() initiates a file to hold submission comments for your package. It’s very barebones at first, e.g.:\n## R CMD check results\n\n0 errors | 0 warnings | 1 note\n\n* This is a new release.\nIn subsequent releases, this file becomes less pointless; for example, it is where we report the results of reverse dependency checks. This is not a place to wax on with long explanations about your submission. In general, you should eliminate the need for such explanations, especially for an initial submission.\nWe highly recommend that your package have a README file (Section 19.1). If it does, this is a good time to check the installation instructions provided there. You may need to switch from instructions to install it from GitHub, in favor of installing from CRAN, in anticipation of your package’s acceptance.\nThe Title and Description fields of DESCRIPTION are real hotspots for nitpicking during CRAN’s human review. Carefully review the advice given in Section 10.2. Also check that Authors@R includes a copyright holder, indicated by the ‘cph’ role. The two most common scenarios are that you add ‘cph’ to your other roles (probably ‘cre’ and ‘aut’) or that you add your employer to Authors@R: with the ‘cph’ and, perhaps, ‘fnd’ role. (When you credit a funder via the ‘fnd’ role, they are acknowledged in the footer of your pkgdown website.) This is also a good time to ensure that the maintainer’s e-mail address is appropriate. This is the only way that CRAN can correspond with you. If there are problems and they can’t get in touch with you, they will remove your package from CRAN. Make sure this email address is likely to be around for a while and that it’s not heavily filtered.\nDouble check that each of your exported functions documents its return value (with the @returns tag, Section 17.4) and has an @examples section (Section 17.5). If you have examples that cannot be run on CRAN, you absolutely must use the techniques in Section 17.5.4 to express the relevant pre-conditions properly. Do not take shortcuts, such as having no examples, commenting out your examples, or putting all of your examples inside \\dontrun{}.\nIf you have embedded third party code in your package, check that you are correctly abiding by and declaring its license (Section 13.4).\nFinally, take advantage of any list of ad hoc checks that other package developers have recently experienced with CRAN. At the time of writing, https://github.com/DavisVaughan/extrachecks is a good place to find such first-hand reports. Reading such a list and preemptively modifying your package can often make the difference between a smooth acceptance and a frustrating process requiring multiple attempts.\n\n23.2.1 CRAN policies\nWe alert you to specific CRAN policies throughout this book and, especially, through this chapter. However, this is something of a moving target, so it pays off to make some effort to keep yourself informed about future changes to CRAN policy.\nThe official home of CRAN policy is https://cran.r-project.org/web/packages/policies.html. However, it’s not very practical to read this document, e.g., once a week and simply hope that you’ll notice any changes. The GitHub repository eddelbuettel/crp monitors the CRAN Repository Policy by tracking the evolution of the underlying files in the source of the CRAN website. Therefore the commit history of that repository makes policy changes much easier to navigate. You may also want to follow the CRAN Policy Watch Twitter account, which tweets whenever a change is detected.\nThe R-package-devel mailing list is another good resource for learning more about package development. You could subscribe to it to keep tabs on what other maintainers are talking about. Even if you don’t subscribe, it can be useful to search this list, when you’re researching a specific topic."
  },
  {
    "objectID": "release.html#keeping-up-with-change",
    "href": "release.html#keeping-up-with-change",
    "title": "23  Releasing to CRAN",
    "section": "\n23.3 Keeping up with change",
    "text": "23.3 Keeping up with change\nNow we move into the main checklist items for a minor or major release of a package that is already on CRAN. Many of these items also appear in the checklist for a patch or initial release.\n\n\nCheck current CRAN check results\n\nCheck if any deprecation processes should be advanced, as described in Gradual deprecation\n\n\nPolish NEWS\n\n\nurlchecker::url_check()\n\n\ndevtools::build_readme()\n\n\nThese first few items confirm that your package is keeping up with its surroundings and with itself. The first item, “Check current CRAN check results”, will be a hyperlink to the CRAN check results for the version of the package that is currently on CRAN. If there are any WARNINGs or ERRORs or NOTEs there, you should investigate and determine what’s going on. Occasionally there can be an intermittent hiccup at CRAN, but generally speaking, any result other than “OK” is something you should address with the release you are preparing. You may discover your package is in a dysfunctional state due to changes in base R, CRAN policies, CRAN tooling, or packages you depend on.\nIf you are in the process of deprecating a function or an argument, a minor or major release is a good time to consider moving that process along as described in Section 22.7. This is also a good time to look at all the NEWS bullets that have accumulated since the last release (“Polish NEWS”). Even if you’ve been diligent about jotting down all the news-worthy changes, chances are these bullets will benefit from some re-organization and editing for consistency and clarity (Section 19.2).\nAnother very important check is to run urlchecker::url_check(). CRAN’s URL checks are described at https://cran.r-project.org/web/packages/URL_checks.html and are implemented by code that ships with R itself. However, these checks are not exposed in a very usable way. The urlchecker package was created to address this and exposes CRAN’s URL-checking logic in the url_check() function. The main problems that surface tend to be URLs that don’t work anymore or URLs that use redirection. Obviously, you should update or remove any URL that no longer exists. Redirection, however, is trickier. If the status code is “301 Moved Permanently”, CRAN’s view is that your package should use the redirected URL. The problem is that many folks don’t follow RFC7231 to the letter and use this sort of redirect even when they have a different intent, i.e. their intent is to provide a stable, user-friendly URL that then redirects to something less user-friendly or more volatile. If a legitimate URL you want to use runs afoul of CRAN’s checks, you’ll have to choose between a couple of less-than-appealing options. You could try to explain the situation to CRAN, but this requires human review, and thus is not recommended. Or you can convert such URLs into non-hyperlinked, verbatim text. Note also that even though urlchecker is using the same code as CRAN, your local results may still differ from CRAN’s, due to differences in other ambient conditions, such as environment variables and system capabilities.\nIf you have a README.Rmd file, you will also want to re-build the static README.md file with the current version of your package. The best function to use for this is devtools::build_readme(), because it is guaranteed to render README.Rmd against the current source code of your package."
  },
  {
    "objectID": "release.html#double-r-cmd-checking",
    "href": "release.html#double-r-cmd-checking",
    "title": "23  Releasing to CRAN",
    "section": "\n23.4 Double R CMD checking",
    "text": "23.4 Double R CMD checking\nNext come a couple of items related to R CMD check. Remember that this should not be the first time you’ve run R CMD check since the previous release! Hopefully, you are running R CMD check often during local development and are using a continuous integration service, like GitHub Actions. This is meant to be a last-minute, final reminder to double-check that all is still well:\n\ndevtools::check(remote = TRUE, manual = TRUE). This happens on your primary development machine, presumably with the current version of R, and with some extra checks that are usually turned off to make day-to-day development faster.\ndevtools::check_win_devel(). This sends your package off to be checked with CRAN’s win-builder service, against the latest development version of R (a.k.a. r-devel). You should receive an e-mail within about 30 minutes with a link to the check results. It’s a good idea to check your package with r-devel, because base R and R CMD check are constantly evolving. Checking with r-devel is required by CRAN policy and it will be done as part of CRAN’s incoming checks. There is no point in skipping this step and hoping for the best.\n\nNote that the brevity of this list implicitly reflects that tidyverse packages are checked after every push via GitHub Actions, across multiple operating systems and versions of R (including the development version), and that most of the tidyverse team develops primarily on macOS. CRAN expects you to “make all reasonable efforts” to get your package working across all of the major R platforms and packages that don’t work on at least two will typically not be accepted.\nThe next subsection (Section 23.4.1) is optional reading with more details on the all the platforms that CRAN cares about and how you can access them. If your ongoing checks are more limited than ours, you may want to make up for that with more extensive pre-submission checks. You may also need this knowledge to troubleshoot a concrete problem that surfaces in CRAN’s checks, either for an incoming submission or for a package that’s already on CRAN.\nWhen running R CMD check for a CRAN submission, you have to address any problems that show up:\n\nYou must fix all ERRORs and WARNINGs. A package that contains any errors or warnings will not be accepted by CRAN.\nEliminate as many NOTEs as possible. Each NOTE requires human oversight, which creates friction for both you and CRAN. If there are notes that you do not believe are important, it is almost always easier to fix them (even if the fix is a bit of a hack) than to persuade CRAN that they’re OK. See our online-only guide to R CMD check for details on how to fix individual problems.\n\nIf you can’t eliminate a NOTE, list it in cran-comments.md and explain why you think it is spurious. We discuss this file further in Section 23.6.\nNote that there will always be one NOTE when you first submit your package. This reminds CRAN that this is a new submission and that they’ll need to do some extra checks. You can’t eliminate this NOTE, so just mention in cran-comments.md that this is your first submission.\n\n\n\n23.4.1 CRAN check flavors and related services\nCRAN runs R CMD check on all contributed packages upon submission and on a regular basis, on multiple platforms or what they call “flavors”. You can see CRAN’s current check flavors here: https://cran.r-project.org/web/checks/check_flavors.html. There are various combinations of:\n\nOperating system and CPU: Windows, macOS (x86_64, arm64), Linux (various distributions)\nR version: r-devel, r-release, r-oldrel\nC, C++, FORTRAN compilers\nLocale, in the sense of the LC_CTYPE environment variable (this is about which human language is in use and character encoding)\n\nCRAN’s check flavors almost certainly include platforms other than your preferred development environment(s), so you will eventually need to make an explicit effort to check and, perhaps, troubleshoot your package on these other flavors.\nIt would be impractical for individual package developers to personally maintain all of these testing platforms. Instead, we turn to various community- and CRAN-maintained resources for this. Here is a selection, in order of how central they are to our current practices:\n\nGitHub Actions (GHA) is our primary means of testing packages on multiple flavors, as covered in Section 21.2.1.\n\nR-hub builder (R-hub). This is a service supported by the R Consortium where package developers can submit their package for checks that replicate various CRAN check flavors.\nYou can use R-hub via a web interface (https://builder.r-hub.io) or, as we recommend, through the rhub R package.\nrhub::check_for_cran() is a good option for a typical CRAN package and is morally similar to the GHA workflow configured by usethis::use_github_action(\"check-standard\"). However, unlike GHA, R-hub currently does not cover macOS, only Windows and Linux.\nrhub also helps you access some of the more exotic check flavors and offers specialized checks relevant to packages with compiled code, such as rhub::check_with_sanitizers().\n\n\nmacOS builder is a service maintained by the CRAN personnel who build the macOS binaries for CRAN packages. This is a relatively new addition to the list and checks packages with “the same setup and available packages as the CRAN M1 build machine”.\nYou can submit your package using the web interface (https://mac.r-project.org/macbuilder/submit.html) or with devtools::check_mac_release()."
  },
  {
    "objectID": "release.html#sec-release-revdep-checks",
    "href": "release.html#sec-release-revdep-checks",
    "title": "23  Releasing to CRAN",
    "section": "\n23.5 Reverse dependency checks",
    "text": "23.5 Reverse dependency checks\n\n\nrevdepcheck::revdep_check(num_workers = 4)\n\n\nThis innocuous checklist item can actually represent a considerable amount of effort. At a high-level, checking your reverse dependencies (“revdeps”) breaks down into:\n\nForm a list of your reverse dependencies. These are CRAN packages that list your package in their Depends, Imports, Suggests or LinkingTo fields.\nRun R CMD check on each one.\nMake sure you haven’t broken someone else’s package with the planned changes in your package.\n\nEach of these steps can require considerable work and judgment. So, if you have no reverse dependencies, you should rejoice that you can skip this step. If you only have a couple of reverse dependencies, you can probably do this “by hand”, i.e. download each package’s source and run R CMD check.\nHere we explain ways to do reverse dependency checks at scale, which is the problem we face. Some of the packages maintained by our team have thousands of reverse dependencies and even some of the lower-level packages have hundreds. We have to approach this in an automated fashion and this section will be most useful to other maintainers in the same boat.\nAll of our reverse dependency tooling is concentrated in the revdepcheck package (https://revdepcheck.r-lib.org/). Note that, at least at the time of writing, the revdepcheck package is not on CRAN. You can install it from Github via devtools::install_github(\"r-lib/revdepcheck\") or pak::pak(\"r-lib/revdepcheck\").\nDo this when you’re ready to do revdep checks for the first time:\n\nusethis::use_revdep()\n\nThis does some one-time setup in your package’s .gitignore and .Rbuildignore files. Revdep checking will create some rather large folders below revdep/, so you definitely want to configure these ignore files. You will also see this reminder to actually perform revdep checks like so, as the checklist item suggests:\n\nrevdepcheck::revdep_check(num_workers = 4)\n\nThis runs ⁠R CMD check⁠ on all of your reverse dependencies, with our recommendation to use 4 parallel workers to speed things along. The output looks something like this:\n> revdepcheck::revdep_check(num_workers = 4)\n── INIT ───────────────────────────────────── Computing revdeps ──\n── INSTALL ───────────────────────────────────────── 2 versions ──\nInstalling CRAN version of cellranger\nalso installing the dependencies 'cli', 'glue', 'utf8', 'fansi', 'lifecycle', 'magrittr', 'pillar', 'pkgconfig', 'rlang', 'vctrs', 'rematch', 'tibble'\n\nInstalling DEV version of cellranger\nInstalling 13 packages: rlang, lifecycle, glue, cli, vctrs, utf8, fansi, pkgconfig, pillar, magrittr, tibble, rematch2, rematch\n── CHECK ─────────────────────────────────────────── 8 packages ──\n✔ AOV1R 0.1.0                     ── E: 0     | W: 0     | N: 0\n✔ mschart 0.4.0                   ── E: 0     | W: 0     | N: 0\n✔ googlesheets4 1.0.1             ── E: 0     | W: 0     | N: 1\n✔ readODS 1.8.0                   ── E: 0     | W: 0     | N: 0\n✔ readxl 1.4.2                    ── E: 0     | W: 0     | N: 0\n✔ readxlsb 0.1.6                  ── E: 0     | W: 0     | N: 0\n✔ unpivotr 0.6.3                  ── E: 0     | W: 0     | N: 0\n✔ tidyxl 1.0.8                    ── E: 0     | W: 0     | N: 0                  \nOK: 8                                                                                 \nBROKEN: 0\nTotal time: 6 min\n── REPORT ────────────────────────────────────────────────────────\nWriting summary to 'revdep/README.md'\nWriting problems to 'revdep/problems.md'\nWriting failures to 'revdep/failures.md'\nWriting CRAN report to 'revdep/cran.md'\nTo minimize false positives, revdep_check() runs ⁠R CMD check⁠ twice per revdep: once with the released version of your package currently on CRAN and again with the local development version, i.e. with your release candidate. Why two checks? Because sometimes the revdep is already failing R CMD check and it would be incorrect to blame your planned release for the breakage. revdep_check() reports the packages that can’t be checked and, most importantly, those where there are so-called “changes to the worse”, i.e. where your release candidate is associated with new problems. Note also that revdep_check() always works with a temporary, self-contained package library, i.e. it won’t modify your default user or system library.\n\n\n\n\n\n\ntidyverse team\n\n\n\nWe actually use a different function for our reverse dependency checks: revdepcheck::cloud_check(). This runs the checks in the cloud, massively in parallel, making it possible to run revdep checks for packages like testthat (with >10,000 revdeps) in just a few hours!\ncloud_check() has been a gamechanger for us, allowing us to run revdep checks more often. For example, we even do this now when assessing the impact of a potential change to a package (Section 22.4), instead of only right before a release.\nAt the time of writing, cloud_check() is only available for package maintainers at Posit, but we hope to offer this service for the broader R community in the future.\n\n\nIn addition to some interactive messages, the revdep check results are written to the revdep/ folder:\n\n\nrevdep/README.md: This is a high-level summary aimed at maintainers. The filename and Markdown format are very intentional, in order to create a nice landing page for the revdep/ folder on GitHub.\n\nrevdep/problems.md: This lists the revdeps that appear to be broken by your release candidate.\n\nrevdep/failures.md: This lists the revdeps that could not be checked, usually because of an installation failure, either of the revdep itself or one of its dependencies.\n\nrevdep/cran.md: This is a high-level summary aimed at CRAN. You should copy and paste this into cran-comments.md (Section 23.6).\nOther files and folders, such as checks.noindex, data.sqlite, and library.noindex. These are for revdepcheck’s internal use and we won’t discuss them further.\n\nThe easiest way to get a feel for these different files is to look around at the latest revdep results for some tidyverse packages, such as dplyr or tidyr.\nThe revdep check results – local, cloud, or CRAN – are not perfect, because this is not a simple task. There are various reasons why a result might be missing, incorrect, or contradictory in different runs.\n\nFalse positives: sometimes revdepcheck reports a package has been broken, but things are actually fine (or, at least, no worse than before). This most commonly happens because of flaky tests that fail randomly (Section 16.4.1), such as HTTP requests. This can also happen because the instance runs out of disk space or other resources, so the first check using the CRAN version succeeds and the second check using the dev version fails. Sometimes it’s obvious that the problem is not related to your package.\nFalse negatives: sometimes a package has been broken, but you don’t detect that. For us, this usually happens when cloud_check() can’t check a revdep because it can’t be installed, typically because of a missing system requirement (e.g. Java). These are separately reported as “failed to test” but are still included in problems.md, because this could still be direct breakage caused by your package. For example, if you remove an exported function that’s used by another package, installation will fail.\n\nGenerally these differences are less of a worry now that CRAN’s own revdep checks are well automated, so new failures typically don’t involve a human.\n\n23.5.1 Revdeps and breaking changes\nIf the revdep check reveals breakages, you need to examine each failure and determine if it’s:\n\nA false positive.\nA non-breaking change, i.e. a failure caused by off-label usage of your package.\nA bug in your package that you need to fix.\nA deliberate breaking change.\n\nIf your update will break another package (regardless of why), you need to inform the maintainer, so they hear it first from you, rather than CRAN. The nicest way to do this is with a patch that updates their package to play nicely with yours, perhaps in the form of a pull request. This can be a decent amount of work and is certainly not feasible for all maintainers. But working through a few of these can be a good way to confront the pain that breaking change causes and to reconsider whether the benefits outweigh the costs. In most cases, a change that affects revdeps is likely to also break less visible code that lives outside of CRAN packages, such as scripts, reports, and Shiny apps.\nIf you decide to proceed, functions such as revdepcheck::revdep_maintainers() and revdepcheck::revdep_email() can help you notify revdep maintainers en masse. Make sure the email includes a link to documentation that describes the most common breaking changes and how to fix them. You should let the maintainers know when you plan to submit to CRAN (we recommend giving at least two weeks notice), so they can submit their updated version before that. When your release date rolls around, re-run your checks to see how many problems have been resolved. Explain any remaining failures in cran-comments.md as demonstrated in Section 23.6. The two most common cases are that you are unable to check a package because you aren’t able to install it locally or a legitimate change in the API which the maintainer hasn’t addressed yet. As long as you have given sufficient advanced notice, CRAN will accept your update, even if it breaks some other packages.\n\n\n\n\n\n\ntidyverse team\n\n\n\nLately the tidyverse team is trying to meet revdep maintainers more than halfway in terms of dealing with breaking changes. For example, in GitHub issue tidyverse/dplyr#6262, the dplyr maintainers tracked hundreds of pull requests in the build-up to the release of dplyr v1.1.0. As the PRs are created, it’s helpful to add links to those as well. As the revdep maintainers merge the PRs, they can be checked off as resolved. If some PRs are still in-flight when the announced submission date rolls around, the situation can be summarized in cran-comments.md, as was true in the case of dplyr v1.1.0."
  },
  {
    "objectID": "release.html#sec-release-cran-comments",
    "href": "release.html#sec-release-cran-comments",
    "title": "23  Releasing to CRAN",
    "section": "\n23.6 Update comments for CRAN",
    "text": "23.6 Update comments for CRAN\n\n\nUpdate cran-comments.md\n\n\nWe use the cran-comments.md file to record comments about a submission, mainly just the results from R CMD check and revdep checks. If you are making a specific change at CRAN’s request, possibly under a deadline, that would also make sense to mention. We like to track this file in Git, so we can see how it changes over time. It should also be listed in .Rbuildignore, since it should not appear in your package bundle. When you’re ready to submit, devtools::submit_cran() (Section 23.7) incorporates the contents of cran-comments.md when it uploads your submission.\nThe target audience for these comments is the CRAN personnel, although there is no guarantee that they will read the comments (or when in the submission process they read them). For example, if your package breaks other packages, you will likely receive an automated email about that, even if you’ve explained that in the comments. Sometimes a human at CRAN then reads the comments, is satisfied, and accepts your package anyway, without further action from you. At other times, your package may be stuck in the queue until you copy cran-comments.md and paste it into an email exchange to move things along. In either case, it’s worth keeping these comments in their own, version-controlled file.\nHere is a fairly typical cran-comments.md from a recent release of forcats. Note that the R CMD check results are clean, i.e. there is nothing that needs to be explained or justified, and there is a concise summary of the revdep process.\n## R CMD check results\n\n0 errors | 0 warnings | 0 notes\n\n## revdepcheck results\n\nWe checked 231 reverse dependencies (228 from CRAN + 3 from Bioconductor), comparing R CMD check results across CRAN and dev versions of this package.\n\nWe saw 2 new problems:\n\n* epikit\n* stevemisc\n\nBoth maintainers were notified on Jan 12 (~2 week ago) and supplied with patches.\n\nWe failed to check 3 packages\n\n* genekitr     (NA)\n* OlinkAnalyze (NA)\n* SCpubr       (NA)\nThis layout is designed to be easy to skim, and easy to match up to the R CMD check results seen by CRAN maintainers. It includes two sections:\n\n\nCheck results: We always state that there were no errors or warnings (and we make sure that’s true!). Ideally we can also say there were no notes. But if not, any NOTEs are presented in a bulleted list. For each NOTE, we include the message from R CMD check and a brief description of why we think it’s OK.\nHere is how a NOTE is explained for the nycflights13 data package:\n## R CMD check results\n\n0 errors | 0 warnings | 1 note\n\n* Checking installed package size:\n  installed size is  6.9Mb\n  sub-directories of 1Mb or more:\n    data   6.9Mb\n\n  This is a data package that will be rarely updated.\n\nReverse dependencies: If there are revdeps, this is where we paste the contents of revdep/cran.md (Section 23.5). If there are no revdeps, we recommend that you keep this section, but say something like: “There are currently no downstream dependencies for this package”."
  },
  {
    "objectID": "release.html#sec-release-process",
    "href": "release.html#sec-release-process",
    "title": "23  Releasing to CRAN",
    "section": "\n23.7 The submission process",
    "text": "23.7 The submission process\n\n\nusethis::use_version('minor') (or ‘patch’ or ‘major’)\n\ndevtools::submit_cran()\n\n\nApprove email\n\nWhen you’re truly ready to submit, it’s time to actually bump the version number in DESCRIPTION. This checklist item will reflect the type of release declared at the start of this process (patch, minor, or major), in the initial call to use_release_issue().\nWe recommend that you submit your package to CRAN by calling devtools::submit_cran(). This convenience function wraps up a few steps:\n\nCreates the package bundle (Section 4.3) with pkgbuild::build(manual = TRUE), which ultimately calls R CMD build.\nPosts the resulting *.tar.gz file to CRAN’s official submission form (https://cran.r-project.org/submit.html), populating your name and email from DESCRIPTION and your submission comments from cran-comments.md.\nConfirms that the submission was successful and reminds you to check your email for the confirmation link.\nWrites submission details to a local CRAN-SUBMISSION file, which records the package version, SHA, and time of submission. This information is used later by usethis::use_github_release() to create a GitHub release once your package has been accepted. CRAN-SUBMISSION will be added to .Rbuildignore. We generally do not gitignore this file, but neither do we commit it. It’s an ephemeral note that exists during the interval between submission and (hopefully) acceptance.\n\nAfter a successful upload, you should receive an email from CRAN within a few minutes. This email notifies you, as maintainer, of the submission and provides a confirmation link. Part of what this does is confirm that the maintainer’s email address is correct. At the confirmation link, you are required to re-confirm that you’ve followed CRAN’s policies and that you want to submit the package. If you fail to complete this step, your package is not actually submitted to CRAN!\nOnce your package enters CRAN’s system it is automatically checked on Windows and Linux, probably against both the released and development versions of R. You will get another email with links to these check results, usually within a matter of hours. An initial submission (Section 23.2) will receive additional scrutiny from CRAN personnel. The process is potentially fully automated when updating a package that is already on CRAN. If a package update passes its initial checks, CRAN will then run reverse dependency checks."
  },
  {
    "objectID": "release.html#failure-modes",
    "href": "release.html#failure-modes",
    "title": "23  Releasing to CRAN",
    "section": "\n23.8 Failure modes",
    "text": "23.8 Failure modes\nThere are at least three ways for your CRAN submission to fail:\n\nIt does not pass R CMD check. This is an automated result.\nHuman review finds the package to be in violation of CRAN policies. This applies mostly to initial submissions, but sometimes CRAN personnel decides to engage in ad hoc review of updates to existing packages that fail any automated checks.\nReverse dependency checks suggest there are “changes to the worse”. This is an automated result.\n\nFailures are frustrating and the feedback may be curt and may feel downright insulting. Take comfort in the fact that this a widely shared experience across the R community. It happens to us on a regular basis. Don’t rush to respond, especially if you are feeling defensive.\nWait until you are able to focus your attention on the technical issues that have been raised. Read any check results or emails carefully and investigate the findings. Unless you feel extremely strongly that discussion is merited, don’t respond to the e-mail. Instead:\n\nFix the identified problems and make recommended changes. Re-run devtools::check() on any relevant platforms to make sure you didn’t accidentally introduce any new problems.\nIncrease the patch version of your package. Yes, this means that there might be gaps in your released version numbers. This is not a big deal.\n\nAdd a “Resubmission” section at the top of cran-comments.md. This should clearly identify that the package is a resubmission, and list the changes that you made.\n## Resubmission\nThis is a resubmission. In this version I have:\n\n* Converted the DESCRIPTION title to title case.\n\n* More clearly identified the copyright holders in the DESCRIPTION\n  and LICENSE files.\n\nIf necessary, update the check results and revdep sections.\nRun devtools::submit_cran() to re-submit the package.\n\nIf your analysis indicates that the initial failure was a false positive, reply to CRAN’s email with a concise explanation. For us, this scenario mostly comes up with respect to revdep checks. It’s extremely rare for us to see failure for CRAN’s initial R CMD check runs and, when it happens, it’s often legitimate. On the other hand, for packages with a large number of revdeps, it’s inevitable that a subset of these packages have some flaky tests or brittle examples. Therefore it’s quite common to see revdep failures that have nothing to do with the proposed package update. In this case, it is appropriate to send a reply email to CRAN explaining why you think these are false positives."
  },
  {
    "objectID": "release.html#sec-release-post-acceptance",
    "href": "release.html#sec-release-post-acceptance",
    "title": "23  Releasing to CRAN",
    "section": "\n23.9 Celebrating success",
    "text": "23.9 Celebrating success\nNow we move into the happiest section of the check list.\n\n\nAccepted 🎉\n\ngit push\n\n\nusethis::use_github_release()\n\n\nusethis::use_dev_version()\n\n\ngit push\n\n\nFinish blog post, share on social media, etc.\n\nAdd link to blog post in pkgdown news menu\n\nCRAN will notify you by email once your package is accepted. This is when we first push to GitHub with the new version number, i.e. we wait until it’s certain that this version will actually be released on CRAN. Next we create a GitHub release corresponding to this CRAN release, using usethis::use_github_release(). A GitHub release is basically a glorified Git tag. The only aspect of GitHub releases that we regularly take advantage of is the release notes. usethis::use_github_release() creates release notes from the NEWS bullets relevant to the current release. Note that usethis::use_github_release() depends crucially on the CRAN-SUBMISSION file that was written by devtools::submit_cran(): that’s how it knows which SHA to tag. After the successful creation of the GitHub release, use_github_release() deletes this temporary file.\nNow we prepare for the next release by incrementing the version number yet again, this time to a development version using usethis::use_dev_version(). It makes sense to immediately push this state to GitHub so that, for example, any new branches or pull requests clearly have a development version as their base.\nAfter the package has been accepted by CRAN, binaries are built for macOS and Windows. It will also be checked across the panel of CRAN check flavors. These processes unfold over a few days, post-acceptance, and sometimes they uncover errors that weren’t detected by the less comprehensive incoming checks. It’s a good idea to visit your package’s CRAN landing page a few days after release and just make sure that all still seems to be well. Figure 23.1 highlights where these results are linked from a CRAN landing page.\n\n\n\n\n\nFigure 23.1: Link to CRAN check results.\n\n\n\n\nIf there is a problem, prepare a patch release to address it and submit using the same process as before. If this means you are making a second submission less than a week after the previous, explain the situation in cran-comments.md. Getting a package established on CRAN can take a couple of rounds, although the guidance in this chapter is intended to maximize the chance of success on the first try. Future releases, initiated from your end, should be spaced at least one or two months apart, according to CRAN policy.\nOnce your package’s binaries are built and it has passed checks across CRAN’s flavors, it’s time for the fun part: publicizing your package. This takes different forms, depending on the type of release. If this is your initial release (or, at least, the first release for which you really want to attract users), it’s especially important to spread the word. No one will use your helpful new package if they don’t know that it exists. There are a number of places to announce your package, such as Twitter, Mastodon, LinkedIn, Slack communities, etc. Make sure to use any relevant tags, such as the #rstats hashtag. If you have a blog, it’s a great idea to write a post about your release.\nWhen introducing a package, the vibe should be fairly similar to writing your README or a “Getting Started” vignette. Make sure to describe what the package does, so that people who haven’t used it before can understand why they should even care. For existing packages, we tend to write blog posts for minor and major releases, but not for a patch release. In all cases, we find that these blog posts are most effective when they include lots of examples, i.e. “show, don’t tell”. For package updates, remember that the existence of a comprehensive NEWS file frees you from the need to list every last change in your blog post. Instead, you can focus on the most important changes and link to the full release notes, for those who want the gory details.\nIf you do blog about your package, it’s good to capture this as yet another piece of documentation in your pkgdown website. A typical pkgdown site has a “News” item in the top navbar, linking to a “Changelog” which is built from NEWS.md. This drop-down menu is a common place to insert links to any blog posts about the package. You can accomplish this by having YAML like this in your _pkgdown.yml configuration file:\nnews:\n  releases:\n  - text: \"Renaming the default branch (usethis >= 2.1.2)\"\n    href: https://www.tidyverse.org/blog/2021/10/renaming-default-branch/\n  - text: \"usethis 2.0.0\"\n    href: https://www.tidyverse.org/blog/2020/12/usethis-2-0-0/\n  - text: \"usethis 1.6.0\"\n    href: https://www.tidyverse.org/blog/2020/04/usethis-1-6-0/\nCongratulations! You have released your first package to CRAN and made it to the end of the book!"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "24  References",
    "section": "",
    "text": "Bryan, Jennifer. 2018. “Excuse Me, Do You Have a Moment to Talk\nabout Version Control?” The American Statistician 72\n(1): 20–27. https://doi.org/10.1080/00031305.2017.1399928.\n\n\nMarwick, Ben, Carl Boettiger, and Lincoln Mullen. 2018a.\n“Packaging Data Analytical Work Reproducibly Using r (and\nFriends).” The American Statistician 72 (1): 80–88. https://doi.org/10.1080/00031305.2017.1375986.\n\n\n———. 2018b. “Packaging Data Analytical Work Reproducibly Using r\n(and Friends).” PeerJ Preprints 6 (March): e3192v2. https://doi.org/10.7287/peerj.preprints.3192v2.\n\n\nMüller, Kirill, and Lorenz Walthert. 2018. Styler: Non-Invasive\nPretty Printing of R Code. http://styler.r-lib.org.\n\n\nSilge, Julia, John C. Nash, and Spencer Graves. 2018. “Navigating the R Package Universe.”\nThe R Journal 10 (2): 558–63. https://doi.org/10.32614/RJ-2018-058."
  },
  {
    "objectID": "R-CMD-check.html#check-metadata",
    "href": "R-CMD-check.html#check-metadata",
    "title": "Appendix A — R CMD check",
    "section": "\nA.1 Check metadata",
    "text": "A.1 Check metadata\nR CMD check always starts by describing your current environment.\n\n\nUsing log directory ‘/some/tmp/path/googledrive.Rcheck’\n\nUsing R version 4.2.2 (2022-10-31)\n\nUsing platform: x86_64-apple-darwin17.0 (64-bit)\n\nUsing session charset: UTF-8\n\nNext the DESCRIPTION file is parsed and the package version and encoding is printed.\n\n\n\nChecking for file ‘googledrive/DESCRIPTION’\n\nThis is package ‘googledrive’ version ‘2.1.0.9000’\n\nPackage encoding: UTF-8"
  },
  {
    "objectID": "R-CMD-check.html#package-structure",
    "href": "R-CMD-check.html#package-structure",
    "title": "Appendix A — R CMD check",
    "section": "\nA.2 Package structure",
    "text": "A.2 Package structure\n\n\n\nChecking package directory. The directory you’re checking must exist - devtools::check() protects you against this problem.\n\n\n\nChecking if this is a source package. You must check a source package, not a binary or installed package. This should never fail if you use devtools::check().\n\n\n\nChecking for executable files. You must not have executable files in your package: they’re not portable, they’re not open source, and they are a security risk. Delete any executable files from your package. (If you’re not submitting to CRAN, you can silence this warning by listing each executable file in the BinaryFiles field in your DESCRIPTION.)\n\n\n\nChecking for hidden files and directories. On Linux and macOS, files with a name starting with . are hidden by default, and you’ve probably included them in your package by mistake. Either delete them, or if they are important, use .Rbuildignore to remove them from the package bundle. R automatically removes some common directories like .git and .svn.\n\n\n\nChecking for portable file names. R packages must work on Windows, Linux and macOS, so you can only use file names that work on all platforms. The easiest way to do this is to stick to letters, numbers, underscores and dashes. Avoid non-English letters and spaces. Fix this check by renaming the listed files.\n\n\n\nChecking for sufficient/correct file permissions. If you can’t read a file, you can’t check it. This check detects the unlikely occurrence that you have files in the package that you don’t have permission to read. Fix this problem by fixing the file permissions.\n\n\n\nChecking whether package ‘XYZ’ can be installed. R CMD check runs R CMD INSTALL to make sure that it’s possible to install your package. If this fails, you should run devtools::install() or the equivalent from RStudio’s menus and debug any problems before continuing.\n\n\n\nChecking installed package size. It’s easy to accidentally include large files that blow up the size of your package. This check ensures that the whole package is less than 5 MB and each subdirectory is less than 1 MB. If you see this message, check that you haven’t accidentally included a large file.\nIf submitting to CRAN, you’ll need to justify the size of your package. First, make sure the package is as small as it possibly can be: try recompressing the data, Section 8.1.1; and minimising vignettes, Chapter 18. If it’s still too large, consider moving data into its own package.\n\n\n\n\nChecking top-level files. Only specified files and directories are allowed at the top level of the package (e.g. DESCRIPTION, R/, src/). To include other files, you have two choices:\n\nIf they don’t need to be installed (i.e. they’re only used for development tasks), add them to .Rbuildignore with usethis::use_build_ignore().\nIf they need to be installed: move them into inst/. They’ll be moved back to the top-level package directory when installed. Learn more in Section 9.2.\n\n\n\n\n\nChecking package subdirectories.\n\nDon’t include any empty directories. These are usually removed automatically by R CMD build so you shouldn’t see this error. If you do, just delete the empty directory.\nThe case of files and directories is important. All sub-directories should be lower-case, except for R/. A citation file, if present, should be in inst/CITATION. Rename as needed.\nThe contents of inst/ shouldn’t clash with top-level contents of the package, such as data/ or R/. If they do, rename your files/directories. Learn more in Section 9.2.\n\n\n\n\n\nChecking for left-over files. Remove any files listed here. They’ve been included in your package by accident."
  },
  {
    "objectID": "R-CMD-check.html#description",
    "href": "R-CMD-check.html#description",
    "title": "Appendix A — R CMD check",
    "section": "\nA.3 DESCRIPTION\n",
    "text": "A.3 DESCRIPTION\n\n\n\n\nChecking DESCRIPTION meta-information.\n\nThe DESCRIPTION must be valid. You are unlikely to see this error, because devtools::load_all() runs the same check each time you re-load the package.\nIf you use any non-ASCII characters in the DESCRIPTION, you must also specify an encoding. There are only three encodings that work on all platforms: latin1, latin2 and UTF-8. We strongly recommend UTF-8: Encoding: UTF-8. Learn more in Section 8.1.3.\nThe License must refer to either a known license (a complete list can be found at https://svn.r-project.org/R/trunk/share/licenses/license.db), or it must use file LICENSE and that file must exist. Errors here are most likely to be typos. Learn more in Chapter 13.\nYou should either provide Authors@R or Authors and Maintainer. You’ll get an error if you’ve specified both, which you can fix by removing the one you didn’t want. Learn more in Section 10.3.\n\n\n\n\n\nChecking package dependencies.\n\nAll packages listed in Depends, Imports and LinkingTo must be installed, and their version requirements must be met, otherwise your package can’t be checked.\nPackages listed in Suggests must be installed, unless you’ve set the environment variable _R_CHECK_FORCE_SUGGESTS_ to a false value (e.g. with check(force_suggests = FALSE)). This is useful if some of the suggested packages are not available on all platforms.\nAn easy way to install any missing or outdated dependencies is to run devtools::install_deps(dependencies = TRUE). See also pak::local_install_deps() and pak::local_install_dev_deps().\nR packages can not have a cycle of dependencies: i.e. if package A requires B, then B can not require A (otherwise which one would you load first?). If you see this error, you’ll need to rethink the design of your package. One easy fix is to move the conflicting package from Imports or Depends to Suggests.\nAny packages used in the NAMESPACE must be listed in one of Imports (most commonly) or Depends (only in special cases).\nEvery package listed in Depends must also be imported in the NAMESPACE or accessed with pkg::foo(). If you don’t do this, your package will work when attached to the search path (with library(mypackage)) but will not work when only loaded (e.g. mypackage::foo())\n\n\n\n\n\nChecking CRAN incoming feasibility. These checks only apply if you’re submitting to CRAN.\n\nIf you’re submitting a new package, you can’t use the same name as an existing package. You’ll need to come up with a new name.\nIf you’re submitting an update, the version number must be higher than the current CRAN version. Update the Version field in DESCRIPTION.\nIf the maintainer of the package has changed (even if it’s just a change in email address), the new maintainer should submit to CRAN, and the old maintainer will receive an email prompting them to confirm the change.\nYou must use a standard open source license, as listed in https://svn.r-project.org/R/trunk/share/licenses/license.db. You can not use a custom license as CRAN does not have the legal resources to review custom agreements.\nThe Title and Description must be free from spelling mistakes. The title of the package must be in title case. Neither title nor description should include either the name of your package or the word “package”. Reword your title and description as needed.\nIf you’re submitting a new package, you’ll always get a NOTE. This reminds the CRAN maintainers to do some extra manual checks.\nAvoid submitting multiple versions of the same package in a short period of time. CRAN prefers at most one submission per month. If you need to fix a major bug, be apologetic."
  },
  {
    "objectID": "R-CMD-check.html#namespace",
    "href": "R-CMD-check.html#namespace",
    "title": "Appendix A — R CMD check",
    "section": "\nA.4 Namespace",
    "text": "A.4 Namespace\n\n\n\nChecking if there is a namespace. You must have a NAMESPACE file. This is handled for your automatically by the devtools workflow.\n\n\n\nChecking package namespace information. The NAMESPACE should be parseable by parseNamespaceFile() and valid. If this check fails, it’s a bug in roxygen2.\n\n\n\nChecking whether the package can be loaded with stated dependencies. Runs library(pkg) with R_DEFAULT_PACKAGES=NULL, so the search path is empty (i.e. stats, graphics, grDevices, utils, datasets and methods are not attached like usual). Failure here typically indicates that you’re missing a dependency on one of those packages.\n\n\n\nChecking whether the namespace can be loaded with stated dependencies. Runs loadNamespace(pkg) with R_DEFAULT_PACKAGES=NULL. Failure usually indicates a problem with the namespace."
  },
  {
    "objectID": "R-CMD-check.html#r-code",
    "href": "R-CMD-check.html#r-code",
    "title": "Appendix A — R CMD check",
    "section": "\nA.5 R code",
    "text": "A.5 R code\n\n\n\nChecking R files for non-ASCII characters. For maximum portability (i.e. so people can use your package on Windows) you should avoid using non-ASCII characters in R files. It’s ok to use them in comments, but object names shouldn’t use them, and in strings you should use unicode escapes. See the CRAN-specific notes in ?sec-code for more details.\n\n\n\nChecking R files for syntax errors. Obviously your R code must be valid. You’re unlikely to see this error if you’ve been regularly using devtools::load_all().\n\n\n\nChecking dependencies in R code. Errors here often indicate that you’ve forgotten to declare a needed package in the DESCRIPTION. Remember that you should never use require() or library() inside a package - see Section 10.6, Chapter 11, and Chapter 12 for more details on best practices.\nAlternatively, you may have accidentally used ::: to access an exported function from a package. Switch to :: instead.\n\n\n\n\nChecking S3 generic/method consistency. S3 methods must have a compatible function signature with their generic. This means that the method must have the same arguments as its generic, with one exception: if the generic includes ... the method can have additional arguments.\nA common cause of this error is defining print methods, because the print() generic contains...:\n\n# BAD\nprint.my_class <- function(x) cat(\"Hi\")\n\n# GOOD\nprint.my_class <- function(x, ...) cat(\"Hi\")\n\n# Also ok\nprint.my_class <- function(x, ..., my_arg = TRUE) cat(\"Hi\")\n\n\n\n\n\nChecking replacement functions. Replacement functions (e.g. functions that are called like foo(x) <- y), must have value as the last argument.\n\n\n\nChecking R code for possible problems. This is a compound check for a wide range of problems:\n\nCalls to library.dynam() (and library.dynam.unload()) should look like library.dynam(\"name\"), not library.dynam(\"name.dll\"). Remove the extension to fix this error.\nPut library.dynam() in .onLoad(), not .onAttach(); put packageStartupMessage() in .onAttach(), not .onLoad(). Put library.dynam.unload() in .onUnload(). If you use any of these functions, make sure they’re in the right place.\nDon’t use unlockBinding() or assignInNamespace() to modify objects that don’t belong to you.\ncodetools::checkUsagePackage() is called to check that your functions don’t use variables that don’t exist. This sometimes raises false positives with functions that use non-standard evaluation (NSE), like subset() or with(). Generally, we think you should avoid NSE in package functions, and hence avoid this NOTE, but if you can not, see ?globalVariables for how to suppress this NOTE.\nYou are not allowed to use .Internal() in a package. Either call the R wrapper function, or write your own C function. (If you copy and paste the C function from base R, make sure to maintain the copyright notice, use a GPL-2 compatible license, and list R-core in the Authors@R field.)\nSimilarly you are not allowed to use ::: to access non-exported functions from other packages. Either ask the package maintainer to export the function you need, or write your own version of it using exported functions. Alternatively, if the licenses are compatible you can copy and paste the exported function into your own package. If you do this, remember to update Authors@R.\nDon’t use assign() to modify objects in the global environment. If you need to maintain state across function calls, create your own environment, as described in Section 8.4.\nDon’t use attach() in your code. Instead refer to variables explicitly.\nDon’t use data() without specifying the envir argument. Otherwise the data will be loaded in the global environment.\nDon’t use deprecated or defunct functions. Update your code to use the latest versions.\nYou must use TRUE and FALSE in your code (and examples), not T and F.\n\n\n\n\n\nChecking whether the package can be loaded. R loads your package with library(). Failure here typically indicates a problem with .onLoad() or .onAttach().\n\n\n\nChecking whether the package can be unloaded cleanly. Loads with library() and then detach()es. If this fails, check .onUnload() and .onDetach().\n\n\n\nChecking whether the namespace can be unloaded cleanly. Runs loadNamespace(\"pkg\"); unloadNamespace(\"pkg\"). Check .onUnload() for problems.\n\n\n\nChecking loading without being on the library search path. Calls library(x, lib.loc = ...). Failure here indicates that you are making a false assumption in .onLoad() or .onAttach()."
  },
  {
    "objectID": "R-CMD-check.html#data",
    "href": "R-CMD-check.html#data",
    "title": "Appendix A — R CMD check",
    "section": "\nA.6 Data",
    "text": "A.6 Data\n\n\n\nChecking contents of ‘data’ directory.\n\nThe data directory can only contain file types described in Section 8.1.\nData files can contain non-ASCII characters only if the encoding is correctly set. This usually shouldn’t be a problem if you’re saving .Rdata files. If you do see this error, look at the Encoding() of each column in the data frame, and ensure none are “unknown”. (You’ll typically need to fix this somewhere in the import process). Learn more in Section 8.1.3.\nIf you’ve compressed a data file with bzip2 or xz you need to declare at least Depends: R (>= 2.10) in your DESCRIPTION.\nIf you’ve used a sub-optimal compression algorithm for your data, re-compress with the suggested algorithm."
  },
  {
    "objectID": "R-CMD-check.html#documentation",
    "href": "R-CMD-check.html#documentation",
    "title": "Appendix A — R CMD check",
    "section": "\nA.7 Documentation",
    "text": "A.7 Documentation\nIf you’re grappling with documentation problems specifically, you may be able to iterate more quickly by using devtools::check_man(), which attempts to run only the relevant subset of checks. It also automatically calls devtools::document() for you.\n\n\n\nChecking Rd files. This checks that all man/*.Rd files use the correct Rd syntax. If this fails, it indicates a bug in roxygen2.\n\n\n\nChecking Rd metadata. Names and aliases must be unique across all documentation files in a package. If you encounter this problem you’ve accidentally used the same @name or @aliases in multiple places; make sure they’re unique.\n\n\n\nChecking Rd line widths. Lines in Rd files must be less than 90 characters wide. This is unlikely to occur if you wrap your R code, and hence roxygen comments, to 80 characters. For very long URLs, use a link-shortening service like bit.ly.\n\n\n\nChecking Rd cross-references. Errors here usually represent typos.\n\n\n\nChecking for missing documentation entries. All exported objects must be documented. See ?tools::undoc for more details.\n\n\n\nChecking for code/documentation mismatches. This check ensures that the documentation matches the code. This should never fail because you’re using roxygen2 which automatically keeps them in sync and check() should usually re-document() your package. In any case, the solution is often to re-run devtools::document().\n\n\n\nChecking Rd \\usage sections. All arguments must be documented, and all @params must document an existing argument. You may have forgotten to document an argument, forgotten to remove the documentation for an argument that you’ve removed, or misspelled an argument name.\nS3 and S4 methods need to use special \\S3method{} and \\S4method{} markup in the Rd file. Roxygen2 will generate this for you automatically.\n\n\n\n\nChecking Rd contents. This checks for auto-generated content made by package.skeleton(). Since you’re not using package.skeleton() you should never have a problem here.\n\n\n\nChecking for unstated dependencies in examples. If you use a package only for an example, make sure it’s listed in the Suggests field. Learn more about how to use different types of dependencies in your examples in Chapter 12.\n\n\n\nChecking examples. Every documentation example must run without errors, and must not take too long. See Section 17.5 for details.\n\n\n\nChecking PDF version of manual. Occasionally you’ll get an error when building the PDF manual. This is usually because the pdf is built by latex and you’ve forgotten to escape something. Debugging this is painful - your best bet is to look up the latex logs and combined tex file and work back from there to .Rd files then back to a roxygen comment. Any such failure is potentially a bug in roxygen2, so open an issue."
  },
  {
    "objectID": "R-CMD-check.html#demos",
    "href": "R-CMD-check.html#demos",
    "title": "Appendix A — R CMD check",
    "section": "\nA.8 Demos",
    "text": "A.8 Demos\n\n\n\nChecking index information. If you’ve written demos, each demo must be listed in demo/00Index. The file should look like:\ndemo-name-without-extension  Demo description\nanother-demo-name            Another description"
  },
  {
    "objectID": "R-CMD-check.html#compiled-code",
    "href": "R-CMD-check.html#compiled-code",
    "title": "Appendix A — R CMD check",
    "section": "\nA.9 Compiled code",
    "text": "A.9 Compiled code\n\n\n\nChecking foreign function calls. .Call(), .C(), .Fortran(), .External() must always be called either with a NativeSymbolInfo object (as created with @useDynLib) or use the .package argument. See ?tools::checkFF for more details.\n\n\n\nChecking line endings in C/C++/Fortran sources/headers. Always use LF as a line ending.\n\n\n\nChecking line endings in Makefiles. As above.\n\n\n\nChecking for portable use of $(BLAS_LIBS) and $(LAPACK_LIBS). Errors here indicate an issue with your use of BLAS and LAPACK.\n\n\n\nChecking compiled code. Checks that you’re not using any C functions that you shouldn’t."
  },
  {
    "objectID": "R-CMD-check.html#tests",
    "href": "R-CMD-check.html#tests",
    "title": "Appendix A — R CMD check",
    "section": "\nA.10 Tests",
    "text": "A.10 Tests\n\n\n\nChecking for unstated dependencies in tests. Every package used by tests must be included in the dependencies.\n\n\n\nChecking tests. Each file in tests/ is run. If you’ve followed the instructions in Chapter 14 you’ll have at least one file: testthat.R. The output from R CMD check is not usually that helpful, so you may need to look at the log file package.Rcheck/tests/testthat.Rout. Fix any failing tests by iterating with devtools::test().\nOccasionally you may have a problem where the tests pass when run interactively with devtools::test(), but fail when in R CMD check. This usually indicates that you’ve made a faulty assumption about the testing environment, and it’s often hard to figure it out."
  },
  {
    "objectID": "R-CMD-check.html#vignettes",
    "href": "R-CMD-check.html#vignettes",
    "title": "Appendix A — R CMD check",
    "section": "\nA.11 Vignettes",
    "text": "A.11 Vignettes\nThis is a tricky enough topic that it also receives substantial coverage in the main body of the book; see Section 18.5.\n\n\n\nChecking ‘build’ directory. build/ is used to track vignette builds. It’s hard to imagine how this check could fail unless you’ve accidentally .Rbuildignored the build/ directory.\n\n\n\nChecking installed files from ‘inst/doc’. Don’t put files in inst/doc - keep your vignettes and the files they need in vignettes/.\n\n\n\nChecking files in ‘vignettes’. Problems here are usually straightforward - you’ve included files that are already included in R (like jss.cls, jss.bst, or Sweave.sty), or you have leftover latex compilation files. Delete these files.\n\n\n\nChecking for sizes of PDF files under ‘inst/doc’. If you’re making PDF vignettes, you can make them as small as possible by running tools::compactPDF().\n\n\n\nChecking for unstated dependencies in vignettes. As with tests, every package that you use in a vignette must be listed in the DESCRIPTION. If a package is used only for a vignette, and not elsewhere, make sure it’s listed in Suggests. If you really want to use a package and you don’t want to list it in DESCRIPTION, write an article instead of a vignette.\n\n\n\nChecking package vignettes in ‘inst/doc’. This checks that every source vignette (i.e. .Rmd) has a built equivalent (i.e. .html) in inst/doc. This shouldn’t fail if you’ve used the standard process outlined in Chapter 18. If there is a problem, start by checking your .Rbuildignore.\n\n\n\nChecking running R code from vignettes. The R code from each vignette is run. If you want to deliberately execute errors (to show the user what failure looks like), make sure the chunk has error = TRUE, purl = FALSE.\n\n\n\nChecking re-building of vignette outputs. Each vignette is re-knit to make sure that the output corresponds to the input. Again, this shouldn’t fail in normal circumstances."
  },
  {
    "objectID": "R-CMD-check.html#sec-r-cmd-check-informational-notes",
    "href": "R-CMD-check.html#sec-r-cmd-check-informational-notes",
    "title": "Appendix A — R CMD check",
    "section": "\nA.12 NOTEs that are informational",
    "text": "A.12 NOTEs that are informational\nOur blanket advice is to eliminate all ERRORs, WARNINGs, and even NOTEs that you see in R CMD check. But there are a few exceptions, i.e. there are a couple of NOTEs that do you not need to fix (and, indeed, probably can not fix).\n\nA.12.1 Initial CRAN submission\nWhen a package first goes to CRAN, there will always be one NOTE that alerts the CRAN maintainers that this is a new submission and that they’ll need to do some extra checks. You can’t eliminate this NOTE.\n* checking CRAN incoming feasibility ... NOTE\nMaintainer: 'Jane Doe <jane@example.com>'\n\nNew submission\n\nA.12.2 Non-ASCII characters in data\nIf your package’s data contains non-ASCII characters, you will get a NOTE like this, but it does not necessarily mean you need to do anything about it.\nCheck: data for non-ASCII characters\nResult: NOTE\n     Note: found 25 marked UTF-8 strings\nAs long as you are aware of the non-ASCII characters and the NOTE mentions your intended and declared encoding (preferably UTF-8), all is well.\n\nA.12.3 Rd cross-references\nIf your roxygen comments contain a cross-reference to a package that is not a formal, direct dependency, you might see a NOTE like this:\nCheck: Rd cross-references\nResult: NOTE\n    Undeclared package ‘jsonlite’ in Rd xrefs\nThis could happen if you want to document something related to a hard indirect dependency: There’s a legitimate reason to link to a topic in the other package and it is basically guaranteed to be installed. Therefore, in practice, often more good than harm comes from the cross-reference.\nIn our experience, this NOTE is only seen on certain CRAN check flavors and not others. Thus far, we have never been directed to address this NOTE by CRAN maintainers."
  }
]